{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Adapted from [Create a TFX pipeline using templates](https://www.tensorflow.org/tfx/tutorials/tfx/template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tfx` and `kfp` tools setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TFX_PKG=\"tfx==0.22.0\"\n",
    "KFP_PKG=\"kfp==0.5.1\"\n",
    "\n",
    "pip freeze | grep $TFX_PKG || pip install -Uq $TFX_PKG\n",
    "pip freeze | grep $KFP_PKG || pip install -Uq $KFP_PKG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to restart the kernel at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skaffold` tool setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "LOCAL_BIN=\"/home/jupyter/.local/bin\"\n",
    "SKAFFOLD_URI=\"https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\"\n",
    "\n",
    "test -d $LOCAL_BIN || mkdir -p $LOCAL_BIN\n",
    "\n",
    "which skaffold || (\n",
    "    curl -Lo skaffold $SKAFFOLD_URI &&\n",
    "    chmod +x skaffold               &&\n",
    "    mv skaffold $LOCAL_BIN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `PATH` environment variable so that `skaffold` is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you shoud see the `skaffold` tool with the command `which`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which skaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment variable setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AI Platform Pipelines, TFX is running in a hosted Kubernetes environment using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/).\n",
    "\n",
    "Let's set some environment variables to use Kubeflow Pipelines.\n",
    "\n",
    "First, get your GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform > Pipeline\" menu.\n",
    "\n",
    "The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, \n",
    "or you can get it from the URL of the Getting Started page where you launched this notebook.\n",
    "\n",
    "Let's create an ENDPOINT environment variable and set it to the KFP cluster endpoint.\n",
    "\n",
    "ENDPOINT should contain only the hostname part of the URL. \n",
    "For example, if the URL of the KFP dashboard is\n",
    "\n",
    "<a href=\"https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start\">https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start</a>, \n",
    "\n",
    "ENDPOINT value becomes 1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = # Enter your ENDPOINT here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the image name as tfx-pipeline under the current GCP project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'\n",
    "CUSTOM_TFX_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Copy the predefined template to your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will create a working pipeline project directory and \n",
    "files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the PIPELINE_NAME below. \n",
    "\n",
    "This will also become the name of the project directory where your files will be put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfx_templated_pipeline\"\n",
    "PROJECT_DIR = os.path.join(os.path.expanduser(\".\"), PIPELINE_NAME)\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX includes the taxi template with the TFX python package. \n",
    "\n",
    "If you are planning to solve a point-wise prediction problem,\n",
    "including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline-name={PIPELINE_NAME} \\\n",
    "  --destination-path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Browse your copied source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code,\n",
    "sample data, and Jupyter Notebooks to analyse the output of the pipeline. \n",
    "\n",
    "The `taxi` template uses the same Chicago Taxi dataset and ML model as \n",
    "the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "Here is brief introduction to each of the Python files:\n",
    "\n",
    "`pipeline` - This directory contains the definition of the pipeline\n",
    "* `configs.py` — defines common constants for pipeline runners\n",
    "* `pipeline.py` — defines TFX components and a pipeline\n",
    "\n",
    "`models` - This directory contains ML model definitions.\n",
    "* `features.py`, `features_test.py` — defines features for the model\n",
    "* `preprocessing.py`, `preprocessing_test.py` — defines preprocessing jobs using tf::Transform\n",
    "\n",
    "`models/estimator` - This directory contains an Estimator based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "\n",
    "`models/keras` - This directory contains a Keras based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "\n",
    "`beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n",
    "\n",
    "\n",
    "**Running the tests:**\n",
    "You might notice that there are some files with `_test.py` in their name. \n",
    "These are unit tests of the pipeline and it is recommended to add more unit \n",
    "tests as you implement your own pipelines. \n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. \n",
    "You can usually get a module name by deleting `.py` extension and replacing `/` with `..`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m models.features_test\n",
    "!python -m models.keras.model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Run your first TFX pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components in the TFX pipeline will generate outputs for each run as\n",
    "[ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere.\n",
    "You can use any storage which the KFP cluster can access, and for this example we\n",
    "will use Google Cloud Storage (GCS). A default GCS bucket should have been created\n",
    "automatically. Its name will be `hostedkfp-default-xxx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_STORE = !gsutil ls | grep hostedkfp-default\n",
    "ARTIFACT_STORE = ARTIFACT_STORE[0]\n",
    "ARTIFACT_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp data/data.csv {ARTIFACT_STORE}tfx-template/data/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a TFX pipeline using the `tfx pipeline create` command.\n",
    "\n",
    "**Note:** When creating a pipeline for KFP, we need a container image which will \n",
    "be used to run our pipeline. And skaffold will build the image for us. Because `skaffold`\n",
    "pulls base images from the docker hub, it will take 5~10 minutes when we build\n",
    "the image for the first time, but it will take much less time from the second build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating a pipeline, `Dockerfile` and `build.yaml` will be generated to build a Docker image.\n",
    "\n",
    "Don't forget to add these files to the source control system (for example, git) along with other source files.\n",
    "\n",
    "A pipeline definition file for [argo](https://argoproj.github.io/argo/) will be generated, too. \n",
    "The name of this file is `${PIPELINE_NAME}.tar.gz.` \n",
    "For example, it will be `tfx_templated_pipeline.tar.gz` if the name of your pipeline is my_pipeline. \n",
    "It is recommended NOT to include this pipeline definition file into source control, because it will be generated from other Python files and will be updated whenever you update the pipeline. For your convenience, this file is already listed in `.gitignore` which is generated automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start an execution run with the newly created pipeline using the `tfx run create` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
