{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training pipeline with KFP and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use KF pre-build components (BiqQuery, CAIP training and predictions)\n",
    "1. Learn how to use KF lightweight python components\n",
    "1. Learn how to build a KF pipeline with these components\n",
    "1. Learn how to compile, upload, and run a KF pipeline with the command line\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a KFP pipeline that orchestrates **BigQuery** and **Cloud AI Platform** services to train, tune, and deploy a **scikit-learn** model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `covertype_training_pipeline.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.components import create_component_from_func_v2\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f'gs://{PROJECT_ID}-vertex'\n",
    "\n",
    "PIPELINE_ROOT = f'{ARTIFACT_STORE}/pipeline'\n",
    "DATA_ROOT = f'{ARTIFACT_STORE}/data'\n",
    "JOB_DIR_ROOT = f'{ARTIFACT_STORE}/jobs'\n",
    "TRAINING_FILE_PATH = f'{DATA_ROOT}/training/dataset.csv'\n",
    "VALIDATION_FILE_PATH = f'{DATA_ROOT}/validation/dataset.csv'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "\n",
    "PIPELINE_NAME = 'covertype_kfp_pipeline'\n",
    "PIPELINE_JSON = f'{PIPELINE_NAME}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest'\n",
    "\n",
    "IMAGE_NAME='trainer_image_covertype_vertex'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE=f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}'\n",
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"covertype_training_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "STAGING_BUCKET = f'{PIPELINE_ROOT}/staging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        container_uri: str,\n",
    "        training_file_path: str,\n",
    "        validation_file_path: str,\n",
    "        staging_bucket: str,\n",
    "        job_dir: str,\n",
    "        max_trial_count: int,\n",
    "        parallel_trial_count: int\n",
    "        \n",
    "    ):\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\n",
    "    \n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_K80\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": container_uri,\n",
    "                \"args\": [\n",
    "                    f\"--job_dir={job_dir}\",\n",
    "                    f\"--training_dataset_path={training_file_path}\",\n",
    "                    f\"--validation_dataset_path={validation_file_path}\",\n",
    "                    \"--hptune\"                    \n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    custom_job = aiplatform.CustomJob(\n",
    "        display_name='covertype_kfp_trial_job',\n",
    "        worker_pool_specs=worker_pool_specs\n",
    "    )\n",
    "    \n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name='covertype_kfp_tuning_job',\n",
    "        custom_job=custom_job,\n",
    "        metric_spec={\n",
    "            'accuracy': 'maximize',\n",
    "        },\n",
    "        parameter_spec={\n",
    "            'alpha': hpt.DoubleParameterSpec(min=1.0e-4, max=1.0e-1, scale='linear'),\n",
    "            'max_iter': hpt.DiscreteParameterSpec(values=[10, 20], scale='linear')\n",
    "        },\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "    \n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in hp_job.trials]\n",
    "    best_trial = hp_job.trials[metrics.index(max(metrics))]\n",
    "    best_alpha = best_trial.parameters[0].value\n",
    "    best_max_iter = best_trial.parameters[1].value\n",
    "    best_accuracy = best_trial.final_measurement.metrics[0].value\n",
    "    \n",
    "    return best_accuracy, best_alpha, best_max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_hyperparameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=TRAINER_IMAGE,\n",
    "        training_file_path=TRAINING_FILE_PATH,\n",
    "        validation_file_path=VALIDATION_FILE_PATH,\n",
    "        staging_bucket=STAGING_BUCKET,\n",
    "        job_dir=JOB_DIR,\n",
    "        max_trial_count=5,\n",
    "        parallel_trial_count=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_deploy(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        container_uri: str,\n",
    "        training_file_path: str,\n",
    "        validation_file_path: str,\n",
    "        staging_bucket: str,\n",
    "        job_dir: str,\n",
    "        alpha: float, \n",
    "        max_iter: int,\n",
    "    ):\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    \n",
    "    SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest'\n",
    "\n",
    "    aiplatform.init(project=project, location=location,\n",
    "                    staging_bucket=staging_bucket)\n",
    "    job = aiplatform.CustomContainerTrainingJob(\n",
    "        display_name='covertype_kfp_training',\n",
    "        container_uri=container_uri,\n",
    "        command=[\n",
    "            \"python\", \n",
    "            \"train.py\",\n",
    "            f\"--job_dir={job_dir}\",\n",
    "            f\"--training_dataset_path={training_file_path}\",\n",
    "            f\"--validation_dataset_path={validation_file_path}\",\n",
    "            f\"--alpha={alpha}\",\n",
    "            f\"--max_iter={max_iter}\",\n",
    "            \"--nohptune\"\n",
    "        ],\n",
    "        staging_bucket=staging_bucket,\n",
    "        model_serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    )\n",
    "    model = job.run(replica_count=1, model_display_name='covertype_kfp_model')\n",
    "    endpoint = model.deploy(\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type='n1-standard-2',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_hpyerparameters_component = create_component_from_func_v2(\n",
    "    tune_hyperparameters,\n",
    "    base_image='python:3.8',\n",
    "    output_component_file='covertype_kfp_tune_hyperparameters.yaml',\n",
    "    packages_to_install=['google-cloud-aiplatform'],\n",
    ")\n",
    "\n",
    "train_and_deploy_component = create_component_from_func_v2(\n",
    "    train_and_deploy,\n",
    "    base_image='python:3.8',\n",
    "    output_component_file='covertype_kfp_train_and_deploy.yaml',\n",
    "    packages_to_install=['google-cloud-aiplatform'],\n",
    "\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"covertype-kfp-pipeline\",\n",
    "    description=\"The pipeline training and deploying the Covertype classifier\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def covertype_train():\n",
    "    \n",
    "    tuning_op = tune_hpyerparameters_component(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=TRAINER_IMAGE,\n",
    "        training_file_path=TRAINING_FILE_PATH,\n",
    "        validation_file_path=VALIDATION_FILE_PATH,\n",
    "        staging_bucket=STAGING_BUCKET,\n",
    "        job_dir=JOB_DIR,\n",
    "        max_trial_count=5,\n",
    "        parallel_trial_count=5,\n",
    "    )\n",
    "    \n",
    "    accuracy = tuning_op.outputs['best_accuracy']\n",
    "    \n",
    "    train_and_deploy_op = train_and_deploy(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=TRAINER_IMAGE,\n",
    "        training_file_path=TRAINING_FILE_PATH,\n",
    "        validation_file_path=VALIDATION_FILE_PATH,\n",
    "        staging_bucket=STAGING_BUCKET,\n",
    "        job_dir=JOB_DIR,\n",
    "        alpha=tuning_op.outputs['best_alpha'], \n",
    "        max_iter=tuning_op.outputs['best_max_iter'],            \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=covertype_train, \n",
    "    package_path=PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=PIPELINE_JSON,\n",
    "    # This argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    "    # pipeline_root=PIPELINE_ROOT  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom components execute in a container image defined in `base_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step in the pipeline employes the AI Platform Training component to schedule a  AI Platform Training job in a custom training container. The custom training image is defined in `trainer_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "Before deploying to AI Platform Pipelines, the pipeline DSL has to be compiled into a pipeline runtime format, also refered to as a pipeline package.  The runtime format is based on [Argo Workflow](https://github.com/argoproj/argo), which is expressed in YAML. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training and Prediction\n",
    "- `ARTIFACT_STORE` - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the `hostedkfp-default-` prefix.\n",
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the base image for custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "You can compile the DSL using an API from the **KFP SDK** or using the **KFP** compiler.\n",
    "\n",
    "To compile the pipeline DSL using the **KFP** compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the pipeline's compile time settings\n",
    "\n",
    "The pipeline can run using a security context of the GKE default node pool's service account or the service account defined in the `user-gcp-sa` secret of the Kubernetes namespace hosting Kubeflow Pipelines. If you want to use the `user-gcp-sa` service account you change the value of `USE_KFP_SA` to `True`.\n",
    "\n",
    "Note that the default AI Platform Pipelines configuration does not define the `user-gcp-sa` secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the `covertype_training_pipeline.yaml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='covertype_continuous_training'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting pipeline runs\n",
    "\n",
    "You can trigger pipeline runs using an API from the KFP SDK or using KFP CLI. To submit the run using KFP CLI, execute the following commands. Notice how the pipeline's parameters are passed to the pipeline run.\n",
    "\n",
    "### List the pipelines in AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a run\n",
    "\n",
    "Find the ID of the `covertype_continuous_training` pipeline you uploaded in the previous step and update the value of `PIPELINE_ID` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='defa3c60-637b-4332-88b9-d8647c2aec84'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- EXPERIMENT_NAME is set to the experiment used to run the pipeline. You can choose any name you want. If the experiment does not exist it will be created by the command\n",
    "- RUN_ID is the name of the run. You can use an arbitrary name\n",
    "- PIPELINE_ID is the id of your pipeline. Use the value retrieved by the   `kfp pipeline list` command\n",
    "- GCS_STAGING_PATH is the URI to the GCS location used by the pipeline to store intermediate files. By default, it is set to the `staging` folder in your artifact store.\n",
    "- REGION is a compute region for AI Platform Training and Prediction. \n",
    "\n",
    "You should be already familiar with these and other parameters passed to the command. If not go back and review the pipeline code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the run\n",
    "\n",
    "You can monitor the run using KFP UI. Follow the instructor who will walk you through the KFP UI and monitoring techniques.\n",
    "\n",
    "To access the KFP UI in your environment use the following URI:\n",
    "\n",
    "https://[ENDPOINT]\n",
    "\n",
    "\n",
    "**NOTE that your pipeline run may fail due to the bug in a BigQuery component that does not handle certain race conditions. If you observe the pipeline failure, retry the run from the KFP UI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
