{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the MLOps environment on GCP\n",
    "\n",
    "This notebook verifies the MLOps environment provisioned on GCP\n",
    "1. Test using the local MLflow server in AI Notebooks instance in log entries to the Cloud SQL\n",
    "2. Test deploying and running an Airflow workflow on Composer that uses MLflow server on GKE to log entries to the Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running a local MLflow experiment\n",
    "We implement a simple Scikit-learn model training routine, and examine the logged entries in Cloud SQL and produced articats in Cloud Storage through MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:80\")\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "mlflow_artifact_uri = mlflow.get_artifact_uri()\n",
    "\n",
    "print(\"MLflow tracking uri: {}\".format(mlflow_tracking_uri)\n",
    "print(\"MLflow articfacts store: {}\".format(mlflow_artifact_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Training a simple Scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = mlflow.set_experiment(\"env-test\")\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment_id):\n",
    "    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1, 1, 0])\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    score = lr.score(X, y)\n",
    "    print(\"Score: %s\" % score)\n",
    "    mlflow.log_metric(\"score\", score)\n",
    "    mlflow.sklearn.log_model(lr, \"model\")\n",
    "    print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Query the Mlfow entries from Cloud SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=3306,\n",
    "    database='mlflow',\n",
    "    user=\"root\",\n",
    "    passwd=\"mlflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. List the artifacts in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {mlflow_artifact_uri}/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submitting a workflow to Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a one-step Airflow workflow that trains a Scikit-learn model, and examine the logged entries in Cloud SQL and produced articats in Cloud Storage through MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMOSER_NAME='ks-composer-dev'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Writing the Airflow workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test-workflow.py\n",
    "\n",
    "import airflow\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from airflow.operators import PythonOperator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model(**kwargs):\n",
    "    print(\"Train lr model step started...\")\n",
    "    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1, 1, 0])\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    score = lr.score(X, y)\n",
    "    print(\"Score: %s\" % score)\n",
    "    print(\"Train lr model step finished.\")\n",
    "    \n",
    "default_args = {\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': airflow.utils.dates.days_ago(0)\n",
    "}\n",
    "\n",
    "with airflow.DAG(\n",
    "    'simple_sklearn_mlflow',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=20)) as dag:\n",
    "    \n",
    "    train_model_op = PythonOperator(\n",
    "        task_id='train_sklearn_model',\n",
    "        provide_context=True,\n",
    "        python_callable=train_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Uploading the Airflow workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage dags import \\\n",
    "  --environment {COMOSER_NAME}  --location {REGION} \\\n",
    "  --source test-workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage dags list \\\n",
    "  --environment {COMOSER_NAME}  --location {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Triggering the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments run {COMOSER_NAME} \\\n",
    "    --location {REGION} trigger_dag -- simple_sklearn_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Query the Mlfow entries from Cloud SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. List the artifacts in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {mlflow_artifact_uri}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
