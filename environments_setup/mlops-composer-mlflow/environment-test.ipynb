{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the MLOps environment on GCP\n",
    "\n",
    "This notebook verifies the MLOps environment provisioned on GCP\n",
    "1. Test using the local MLflow server in AI Notebooks instance in log entries to the Cloud SQL\n",
    "2. Test deploying and running an Airflow workflow on Composer that uses MLflow server on GKE to log entries to the Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running a local MLflow experiment\n",
    "We implement a simple Scikit-learn model training routine, and examine the logged entries in Cloud SQL and produced articats in Cloud Storage through MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql+pymysql://root:mlflow@127.0.0.1:3306/mlflow\n",
      "ksalama-research:us-central1:ks-mlflow-01-sql\n",
      "gs://ks-mlflow-01-artifact-store\n"
     ]
    }
   ],
   "source": [
    "!echo $MLFLOW_SQL_CONNECTION_STR\n",
    "!echo $MLFLOW_SQL_CONNECTION_NAME\n",
    "!echo $MLFLOW_EXPERIMENTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking uri: http://localhost:80\n",
      "MLflow articfacts store: gs://ks-mlflow-01-artifact-store\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:80\")\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "mlflow_artifact_uri = os.environ['MLFLOW_EXPERIMENTS_URI']\n",
    "\n",
    "print(\"MLflow tracking uri: {}\".format(mlflow_tracking_uri))\n",
    "print(\"MLflow articfacts store: {}\".format(mlflow_artifact_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Training a simple Scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'exp-notebooks-test' does not exist. Creating a new experiment\n",
      "Score: 0.6666666666666666\n",
      "Model saved in run 6834ce2dabba4785b2a0a66bbb17cde7\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"exp-notebooks-test\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(nested=True):\n",
    "    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1, 1, 0])\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    score = lr.score(X, y)\n",
    "    print(\"Score: %s\" % score)\n",
    "    mlflow.log_metric(\"score\", score)\n",
    "    mlflow.sklearn.log_model(lr, \"model\")\n",
    "    print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Query the Mlfow entries from Cloud SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=3306,\n",
    "    database='mlflow',\n",
    "    user=\"root\",\n",
    "    passwd=\"mlflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alembic_version\n",
      "experiment_tags\n",
      "experiments\n",
      "latest_metrics\n",
      "metrics\n",
      "model_versions\n",
      "params\n",
      "registered_models\n",
      "runs\n",
      "tags\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()   \n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "for entry in cursor:\n",
    "    print(entry[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 'exp-notebooks-test', 'gs://ks-mlflow-01-artifact-store/5', 'active')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM experiments where name='{}'\".format(experiment_name))\n",
    "for entry in cursor:\n",
    "    print(entry)\n",
    "\n",
    "experiment_id = entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6834ce2dabba4785b2a0a66bbb17cde7', '', 'UNKNOWN', '', '', 'root', 'FINISHED', 1594325224720, 1594325225130, '', 'active', 'gs://ks-mlflow-01-artifact-store/5/6834ce2dabba4785b2a0a66bbb17cde7/artifacts', 5)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM runs where experiment_id={}\".format(experiment_id))\n",
    "for entry in cursor:\n",
    "    print(entry)\n",
    "\n",
    "run_uuid = entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('score', 0.666666666666667, 1594325224758, '6834ce2dabba4785b2a0a66bbb17cde7', 0, 0)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM metrics where run_uuid = '{}'\".format(run_uuid))\n",
    "for entry in cursor:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. List the artifacts in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {mlflow_artifact_uri}/{experiment_id}/{run_id}/artifacts/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submitting a workflow to Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a one-step Airflow workflow that trains a Scikit-learn model, and examine the logged entries in Cloud SQL and produced articats in Cloud Storage through MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMOSER_NAME='ks-mlflow-01-af'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Writing the Airflow workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test-sklearn-mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test-sklearn-mlflow.py\n",
    "\n",
    "import airflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from airflow.operators import PythonOperator\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://34.107.167.99:80\") \n",
    "\n",
    "\n",
    "def train_model(**kwargs):\n",
    "\n",
    "    print(\"Train lr model step started...\")\n",
    "    print(\"MLflow tracking uri: {}\".format(mlflow.get_tracking_uri()))\n",
    "    mlflow.set_experiment(\"exp-airflow-test\")\n",
    "    with mlflow.start_run(nested=True):\n",
    "        X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "        y = np.array([0, 0, 1, 1, 1, 0])\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X, y)\n",
    "        score = lr.score(X, y)\n",
    "        print(\"Score: %s\" % score)\n",
    "        mlflow.log_metric(\"score\", score)\n",
    "        mlflow.sklearn.log_model(lr, \"model\")\n",
    "        print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n",
    "    print(\"Train lr model step finished.\")\n",
    "    \n",
    "default_args = {\n",
    "    'retries': 1,\n",
    "    'start_date': airflow.utils.dates.days_ago(0)\n",
    "}\n",
    "\n",
    "with airflow.DAG(\n",
    "    'test_sklearn_mlflow',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=20)) as dag:\n",
    "    \n",
    "    train_model_op = PythonOperator(\n",
    "        task_id='train_sklearn_model',\n",
    "        provide_context=True,\n",
    "        python_callable=train_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Uploading the Airflow workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage dags import \\\n",
    "  --environment {COMOSER_NAME}  --location {REGION} \\\n",
    "  --source test-sklearn-mlflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME\n",
      "dags/\n",
      "dags/airflow_monitoring.py\n",
      "dags/test-sklearn-mlflow.py\n"
     ]
    }
   ],
   "source": [
    "!gcloud composer environments storage dags list \\\n",
    "  --environment {COMOSER_NAME}  --location {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Triggering the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments run {COMOSER_NAME} \\\n",
    "    --location {REGION} unpause -- test_sklearn_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeconfig entry generated for us-central1-ks-mlflow-01-af-6ea49f13-gke.\n",
      "Executing within the following Kubernetes cluster namespace: composer-1-10-4-airflow-1-10-6-6ea49f13\n",
      "[2020-07-09 20:11:55,387] {settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3497\n",
      "[2020-07-09 20:11:55,751] {app.py:53} WARNING - Using default Composer Environment Variables. Overrides have not been applied.\n",
      "[2020-07-09 20:11:55,754] {configuration.py:593} INFO - Reading the config from /etc/airflow/airflow.cfg\n",
      "[2020-07-09 20:11:55,775] {settings.py:254} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3497\n",
      "[2020-07-09 20:11:56,276] {default_celery.py:90} WARNING - You have configured a result_backend of redis://airflow-redis-service.default.svc.cluster.local:6379/0, it is highly recommended to use an alternative result_backend (i.e. a database).\n",
      "[2020-07-09 20:11:56,280] {__init__.py:51} INFO - Using executor CeleryExecutor\n",
      "[2020-07-09 20:11:56,281] {dagbag.py:407} INFO - Filling up the DagBag from /home/airflow/gcs/dags/test-sklearn-mlflow.py\n",
      "/usr/local/lib/airflow/airflow/utils/helpers.py:427: DeprecationWarning: Importing 'PythonOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "/opt/python3.6/lib/python3.6/site-packages/sqlalchemy/engine/default.py:581: Warning: (1300, \"Invalid utf8 character string: '800495'\")\n",
      "  cursor.execute(statement, parameters)\n",
      "[2020-07-09 20:11:57,522] {cli.py:240} INFO - Created <DagRun test_sklearn_mlflow @ 2020-07-09 20:11:57+00:00: manual__2020-07-09T20:11:57+00:00, externally triggered: True>\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!gcloud composer environments run {COMOSER_NAME} \\\n",
    "    --location {REGION} trigger_dag -- test_sklearn_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Query the MLfow entries from Cloud SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=3306,\n",
    "    database='mlflow',\n",
    "    user=\"root\",\n",
    "    passwd=\"mlflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 'exp-airflow-test', 'gs://ks-mlflow-01-artifact-store/4', 'active')\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"exp-airflow-test\"\n",
    "cursor.execute(\"SELECT * FROM experiments where name='{}'\".format(experiment_name))\n",
    "for entry in cursor:\n",
    "    print(entry)\n",
    "\n",
    "experiment_id = entry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e4f611eb836f4df1892d95daeb3c27c1', '', 'UNKNOWN', '', '', 'airflow', 'FINISHED', 1594324933340, 1594324933832, '', 'active', 'gs://ks-mlflow-01-artifact-store/4/e4f611eb836f4df1892d95daeb3c27c1/artifacts', 4)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM runs where experiment_id={}\".format(experiment_id))\n",
    "for entry in cursor:\n",
    "    print(entry)\n",
    "\n",
    "run_uuid = entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. List the artifacts in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {mlflow_artifact_uri}/{experiment_id}/{run_uuid}/artifacts/model"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}