{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Text, Union, Dict, Iterable,Mapping\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_data_validation.utils import io_util\n",
    "from tensorflow_data_validation import constants\n",
    "from tensorflow_data_validation import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"chicago-taxi\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "MLFLOW_TRACKING_EXTERNAL_URI = os.environ[\"MLFLOW_TRACKING_EXTERNAL_URI\"]\n",
    "\n",
    "REGION=os.environ[\"MLOPS_REGION\"]\n",
    "ML_IMAGE_URI = os.environ[\"ML_IMAGE_URI\"]\n",
    "COMPOSER_NAME = os.environ[\"MLOPS_COMPOSER_NAME\"]\n",
    "MLFLOW_GCS_ROOT_URI = os.environ[\"MLFLOW_GCS_ROOT_URI\"]\n",
    "DATASET_GCS_FOLDER = MLFLOW_GCS_ROOT_URI+\"/data\"\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT\", \"edgeml-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline statistics\n",
    "#TODO: Create BQ -> CSV sample file\n",
    "\n",
    "statopt=tfdv.StatsOptions(num_histogram_buckets=5)\n",
    "baseline_stats = tfdv.generate_statistics_from_csv(\n",
    "    data_location=DATASET_GCS_FOLDER+'/stats_source/ds_bq_data_statistics_*.csv',\n",
    "    stats_options = statopt,\n",
    "    delimiter='|'\n",
    ")\n",
    "reference_schema = tfdv.infer_schema(baseline_stats)\n",
    "schema_text = text_format.MessageToString(reference_schema)\n",
    "io_util.write_string_to_file(DATASET_GCS_FOLDER+'/taxi_schema.pbtxt', schema_text)\n",
    "\n",
    "#tfdv.display_schema(schema=reference_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional, Text, Union, Dict, Iterable, Mapping\n",
    "from tensorflow_data_validation import types\n",
    "from tensorflow_data_validation import constants\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_RAW_DATA_COLUMN = 'raw_data'\n",
    "_INSTANCES_KEY = 'instances'\n",
    "_TIMESTAMP_KEY = 'trip_start_timestamp'\n",
    "\n",
    "_SCHEMA_TO_NUMPY = {\n",
    "    schema_pb2.FeatureType.BYTES:  np.str,\n",
    "    schema_pb2.FeatureType.INT:    np.int64,\n",
    "    schema_pb2.FeatureType.FLOAT:  np.float\n",
    "}\n",
    "\n",
    "\n",
    "@beam.typehints.with_input_types(Dict)\n",
    "@beam.typehints.with_output_types(types.BeamExample)\n",
    "class InstanceCoder(beam.DoFn):\n",
    "    \"\"\"A DoFn which converts an taxi row to types.BeamExample elements.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        schema: schema_pb2, \n",
    "        end_time: datetime=None, \n",
    "        time_window: datetime=None,\n",
    "        slicing_column: str=None):\n",
    "\n",
    "        self._example_size = beam.metrics.Metrics.counter(\n",
    "            constants.METRICS_NAMESPACE, \"example_size\")\n",
    "\n",
    "        self._features = {}\n",
    "        for feature in schema.feature:\n",
    "            if not feature.type in _SCHEMA_TO_NUMPY.keys():\n",
    "                raise ValueError(\n",
    "                    \"Unsupported feature type: {}\".format(feature.type))\n",
    "            if feature.HasField('presence') and feature.presence.min_fraction == 1.0:\n",
    "                self._features[feature.name] = _SCHEMA_TO_NUMPY[feature.type]\n",
    "            else:\n",
    "                # What is the NumPy {optional int64} data type?\n",
    "                self._features[feature.name] = np.object\n",
    "\n",
    "        if end_time and time_window and slicing_column:\n",
    "            self._end_time = end_time\n",
    "            self._time_window = time_window\n",
    "            self._slicing_column = slicing_column \n",
    "        else:\n",
    "            self._slicing_column = None\n",
    "\n",
    "    def _get_time_slice(self, time_stamp: str) -> str:\n",
    "        \"\"\"\n",
    "        Assigns a time stamp to a time slice.\n",
    "\n",
    "        Args:\n",
    "            time_stamp: A date_time string in the ISO YYYY-MM-DDTHH:MM:SS format\n",
    "        Returns:\n",
    "            A time slice as a string in the following format:\n",
    "            YYYY-MM-DDTHH:MM_YYYY-MM-DDTHH:MM\n",
    "        \"\"\"\n",
    "\n",
    "        time_stamp = datetime.strptime(time_stamp, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "        q = (self._end_time - time_stamp) // self._time_window\n",
    "        slice_end = self._end_time - q * self._time_window\n",
    "        slice_begining = self._end_time - (q + 1) * self._time_window\n",
    "\n",
    "        return (slice_begining.strftime('%Y-%m-%dT%H:%M') + '_' +\n",
    "                slice_end.strftime('%Y-%m-%dT%H:%M'))\n",
    "\n",
    "    def _parse_raw_instance(self, raw_instance: Union[list, dict]) -> dict:\n",
    "        if type(raw_instance) is dict:\n",
    "            instance = {name: np.array(value if type(value) == list else [value], dtype=self._features[name])\n",
    "                        for name, value in raw_instance.items()}\n",
    "        elif type(raw_instance) is list:\n",
    "            instance = {name: np.array([value], dtype=self._features[name])\n",
    "                        for name, value in zip(list(self._features.keys()), raw_instance)}\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Unsupported input instance format. Only JSON list or JSON object instances are supported\")\n",
    "\n",
    "        return instance\n",
    "\n",
    "    def process(self, raw_instance: Dict) -> Iterable:\n",
    "        instance = self._parse_raw_instance(raw_instance)\n",
    "        if self._slicing_column:\n",
    "            instance[self._slicing_column] = np.array(\n",
    "                [self._get_time_slice(raw_instance[_TIMESTAMP_KEY])])\n",
    "        yield instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_STATS_FILENAME = 'stats.pb'\n",
    "_ANOMALIES_FILENAME = 'anomalies.pbtxt'\n",
    "_SLICING_COLUMN_NAME = 'time_slice'\n",
    "_SLICING_COLUMN_TYPE = schema_pb2.FeatureType.BYTES\n",
    "\n",
    "def _alert_if_anomalies(anomalies: anomalies_pb2.Anomalies, output_path: str):\n",
    "    \"\"\"\n",
    "    Analyzes an anomaly protobuf and reports the status.\n",
    "    Currently, the function just writes to a default Python logger.\n",
    "    A more comprehensive alerting strategy will be considered in the future.\n",
    "    \"\"\"\n",
    "\n",
    "    if list(anomalies.anomaly_info):\n",
    "        logging.warn(\"Anomalies detected. The anomaly report uploaded to: {}\".format(output_path))\n",
    "    else:\n",
    "        logging.info(\"No anomalies detected.\")\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "def _generate_query(sampling_query_template: str, start_time: str, end_time: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a query that extracts a time series of records between start and end time.\n",
    "    \"\"\"    \n",
    "    query = Template(sampling_query_template).render( \n",
    "        start_time=start_time, \n",
    "        end_time=end_time)\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(\n",
    "    query: str,\n",
    "    output_path: str,\n",
    "    start_time: datetime,\n",
    "    end_time: datetime,\n",
    "    schema: schema_pb2.Schema,\n",
    "    baseline_stats: Optional[statistics_pb2.DatasetFeatureStatisticsList]=None,\n",
    "    time_window: Optional[timedelta]=None,\n",
    "    pipeline_options: Optional[PipelineOptions] = None,\n",
    "): \n",
    "    \n",
    "    end_time = end_time.replace(second=0, microsecond=0)\n",
    "    start_time = start_time.replace(second=0, microsecond=0)\n",
    "    query = _generate_query(\n",
    "        sampling_query_template=query, \n",
    "        start_time=start_time.strftime('%Y-%m-%dT%H:%M:%S'), \n",
    "        end_time=end_time.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "    \n",
    "    # Configure slicing for statistics calculations\n",
    "    stats_options = tfdv.StatsOptions(schema=schema)\n",
    "    slicing_column = None\n",
    "    if time_window:\n",
    "        time_window = timedelta(\n",
    "            days=time_window.days,\n",
    "            seconds=(time_window.seconds // 60) * 60)\n",
    "\n",
    "        if end_time - start_time > time_window:\n",
    "            slice_fn = tfdv.get_feature_value_slicer(features={_SLICING_COLUMN_NAME: None})\n",
    "            stats_options.slice_functions=[slice_fn]\n",
    "            slicing_column = _SLICING_COLUMN_NAME \n",
    "            slicing_feature = schema.feature.add()\n",
    "            slicing_feature.name = _SLICING_COLUMN_NAME\n",
    "            slicing_feature.type = _SLICING_COLUMN_TYPE\n",
    "\n",
    "    # Configure output paths \n",
    "    stats_output_path = os.path.join(output_path, _STATS_FILENAME)\n",
    "    anomalies_output_path = os.path.join(output_path, _ANOMALIES_FILENAME)\n",
    "    \n",
    "    # Define an start the pipeline\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        raw_examples = (p\n",
    "           | 'GetData' >> beam.io.Read(beam.io.ReadFromBigQuery(query=query, gcs_location=output_path,project=PROJECT_ID, use_standard_sql=True)))\n",
    "\n",
    "        examples = (raw_examples\n",
    "           | 'InstancesToBeamExamples' >> beam.ParDo(InstanceCoder(schema, end_time, time_window, slicing_column)))\n",
    "\n",
    "        stats = (examples\n",
    "           | 'BeamExamplesToArrow' >> tfdv.utils.batch_util.BatchExamplesToArrowRecordBatches()\n",
    "           | 'GenerateStatistics' >> tfdv.GenerateStatistics(options=stats_options))\n",
    "        \n",
    "        _ = (stats\n",
    "            | 'WriteStatsOutput' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix=stats_output_path,\n",
    "                shard_name_template='',\n",
    "                coder=beam.coders.ProtoCoder(\n",
    "                    statistics_pb2.DatasetFeatureStatisticsList)))\n",
    "\n",
    "        anomalies = (stats\n",
    "            | 'ValidateStatistics' >> beam.Map(tfdv.validate_statistics, schema=schema, previous_statistics=baseline_stats))\n",
    "\n",
    "        _ = (anomalies\n",
    "            | 'AlertIfAnomalies' >> beam.Map(_alert_if_anomalies, anomalies_output_path)\n",
    "            | 'WriteAnomaliesOutput' >> beam.io.textio.WriteToText(\n",
    "                file_path_prefix=anomalies_output_path,\n",
    "                shard_name_template='',\n",
    "                append_trailing_newlines=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_data_validation import StatsOptions\n",
    "from tensorflow_data_validation import load_statistics\n",
    "from tensorflow_data_validation import load_schema_text\n",
    "\n",
    "_SETUP_FILE = './setup.py'\n",
    "\n",
    "sampling_query_template = \"\"\"\n",
    "    SELECT trip_start_timestamp, \n",
    "        unique_key, taxi_id, trip_end_timestamp, trip_seconds, trip_miles, pickup_census_tract, \n",
    "        dropoff_census_tract, pickup_community_area, dropoff_community_area, fare, tips, tolls, extras, trip_total, \n",
    "        payment_type, company, pickup_latitude, pickup_longitude, pickup_location, dropoff_latitude, dropoff_longitude, dropoff_location\n",
    "    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE trip_start_timestamp BETWEEN '{{ start_time }}' AND '{{ end_time }}'\n",
    "    LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "baseline_stats = None\n",
    "#if known_args.baseline_stats_file:\n",
    "#    baseline_stats = load_statistics(known_args.baseline_stats_file)\n",
    "\n",
    "#schema = tfdv.infer_schema(statistics=train_stats)\n",
    "\n",
    "# Load back the saved reference schema\n",
    "loaded_schema = load_schema_text(f'{DATASET_GCS_FOLDER}/taxi_schema.pbtxt')\n",
    "tfdv.display_schema(schema=loaded_schema)\n",
    "\n",
    "generate_statistics(\n",
    "    query= sampling_query_template,\n",
    "    output_path= f'{DATASET_GCS_FOLDER}/stats',\n",
    "    start_time= datetime.datetime(1990,1,1),\n",
    "    end_time= datetime.datetime.today(),\n",
    "    schema= loaded_schema,\n",
    "    baseline_stats= baseline_stats,\n",
    "    time_window =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
