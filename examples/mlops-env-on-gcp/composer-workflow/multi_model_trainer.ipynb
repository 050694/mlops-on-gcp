{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links\n",
    "# https://github.com/v-loves-avocados/chicago-taxi\n",
    "# Pipeline based NYC taxi: https://github.com/LilHelmer/NYC-Taxi-Fare-Prediction/blob/master/ny_taxi_fare_prediction.ipynb\n",
    "# https://www.kaggle.com/margulisshahar/taxi-fare-prediction-and-tip-classification\n",
    "# NYC Taxi: https://www.kaggle.com/aiswaryaramachandran/eda-and-feature-engineering\n",
    "# Training idea: https://medium.com/analytics-vidhya/machine-learning-to-predict-taxi-fare-part-two-predictive-modelling-f80461a8072e\n",
    "# GCP Scikit guide: https://cloud.google.com/ai-platform/training/docs/training-scikit-learn\n",
    "\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic jinja template to create Python file with variable substitution.\n",
    "# Dictonaries for substituted variables: env[] for OS environment vars and var[] for global variables\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "from jinja2 import Template\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    dirname = os.path.dirname(line)\n",
    "    if len(dirname)>0 and not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(Template(cell).render({'env' : os.environ, 'var' : globals()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the experiment in MLFlow tracking and name in model registry\n",
    "experiment_name = \"chicago-taxi-m1\"\n",
    "number_of_parallel_trainings = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name in MLflow \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "# MLflow public URI\n",
    "MLFLOW_TRACKING_EXTERNAL_URI = os.environ[\"MLFLOW_TRACKING_EXTERNAL_URI\"]\n",
    "\n",
    "REGION=os.environ[\"MLOPS_REGION\"]\n",
    "ML_IMAGE_URI = os.environ[\"ML_IMAGE_URI\"]\n",
    "COMPOSER_NAME = os.environ[\"MLOPS_COMPOSER_NAME\"]\n",
    "MLFLOW_GCS_ROOT_URI = os.environ[\"MLFLOW_GCS_ROOT_URI\"]\n",
    "\n",
    "print(f\"Cloud Composer instance name: {COMPOSER_NAME}\")\n",
    "print(f\"Cloud Composer region: {REGION}\")\n",
    "print(f\"MLflow tracking server URI: {mlflow_tracking_uri}\")\n",
    "print(f\"MLflow GCS root: {MLFLOW_GCS_ROOT_URI}\")\n",
    "\n",
    "experiment_path = MLFLOW_GCS_ROOT_URI.replace(\"gs://\",\"\")\n",
    "display(HTML('<hr>You can check results of this test in MLflow and GCS folder:'))\n",
    "display(HTML(f'<h4><a href=\"{MLFLOW_TRACKING_EXTERNAL_URI}\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLflow UI</a></h4>'))\n",
    "display(HTML(f'<h4><a href=\"https://console.cloud.google.com/storage/browser/{experiment_path}/experiments\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLFlow GCS folder</a></h4>'))\n",
    "\n",
    "!mkdir -p ./package/training\n",
    "!touch ./package/training/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./package/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['mlflow==1.11.0','PyMySQL==0.9.3']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Customer training setup.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate ./package/training/task.py\n",
    "\n",
    "import sys, stat\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # Only for train_test\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from joblib import dump, load\n",
    "from google.cloud import storage\n",
    "\n",
    "csv_delimiter = '|'\n",
    "\n",
    "def copy_local_directory_to_gcs(local_path, gcs_uri):\n",
    "    assert os.path.isdir(local_path)\n",
    "    job_dir =  gcs_uri.replace('gs://', '')\n",
    "    bucket_id = job_dir.split('/')[0]\n",
    "    bucket_path = job_dir.lstrip('{}/'.format(bucket_id))\n",
    "    bucket = storage.Client().bucket(bucket_id)\n",
    "    blob = bucket.blob('{}/{}'.format(bucket_path, local_path))\n",
    "    _upload_local_to_gcs(local_path, bucket, bucket_path)\n",
    "        \n",
    "def _upload_local_to_gcs(local_path, bucket, bucket_path):\n",
    "    for local_file in glob.glob(local_path + '/**'):\n",
    "        if not os.path.isfile(local_file):\n",
    "           _upload_local_to_gcs(local_file, bucket, bucket_path + \"/\" + os.path.basename(local_file))\n",
    "        else:\n",
    "           remote_path = os.path.join(bucket_path, local_file[1 + len(local_path):])\n",
    "           blob = bucket.blob(remote_path)\n",
    "           blob.upload_from_filename(local_file)\n",
    "\n",
    "def feature_engineering(data):\n",
    "    # Add 'N/A' for missing 'Company'\n",
    "    data.fillna(value={'company':'N/A','tolls':0}, inplace=True)\n",
    "    # Drop rows contains null data.\n",
    "    data.dropna(how='any', axis='rows', inplace=True)\n",
    "    # Pickup and dropoff locations distance\n",
    "    data[\"abs_distance\"] = (np.hypot(data[\"dropoff_latitude\"]-data[\"pickup_latitude\"], data[\"dropoff_longitude\"]-data[\"pickup_longitude\"]))*100\n",
    "\n",
    "    # Remove extremes, outliers\n",
    "    possible_outliers_cols = ['trip_seconds', 'trip_miles', 'fare', 'abs_distance']\n",
    "    data=data[(np.abs(stats.zscore(data[possible_outliers_cols])) < 3).all(axis=1)].copy()\n",
    "    # Reduce location accuracy\n",
    "    data=data.round({'pickup_latitude': 3, 'pickup_longitude': 3, 'dropoff_latitude':3, 'dropoff_longitude':3})\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "    #X_train = X_train.drop('fare', axis=1)\n",
    "\n",
    "    # Returns training only features (X) and fare (y)  \n",
    "    return (\n",
    "        data.drop(['fare', 'trip_start_timestamp'], axis=1),\n",
    "        data['fare']\n",
    "    )\n",
    "\n",
    "def build_pipeline(number_of_estimators = 20, max_features = 'auto'):\n",
    "    ct_pipe = ColumnTransformer(transformers=[\n",
    "    ('hourly_cat', OneHotEncoder(categories=[range(0,24)], sparse = False), [\"trip_start_hour\"]),\n",
    "    ('dow', OneHotEncoder(categories=[['Mon', 'Tue', 'Sun', 'Wed', 'Sat', 'Fri', 'Thu']], sparse = False), [\"trip_start_day_of_week\"]),\n",
    "    ('std_scaler', StandardScaler(), [\n",
    "        'trip_start_year',\n",
    "        'abs_distance',\n",
    "        'pickup_longitude',\n",
    "        'pickup_latitude',\n",
    "        'dropoff_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'trip_miles',\n",
    "        'trip_seconds'])\n",
    "    ])\n",
    "    rfr_pipe = Pipeline([\n",
    "        ('ct', ct_pipe),\n",
    "        ('forest_reg', RandomForestRegressor(n_estimators = number_of_estimators, max_features = max_features, n_jobs = -1, random_state = 3))\n",
    "    ])\n",
    "    return rfr_pipe\n",
    "\n",
    "def train_model(args):\n",
    "    print(\"Taxi fare estimation model training step started...\")\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    #mlflow.sklearn.autolog()\n",
    "    with mlflow.start_run(nested=True) as mlflow_run:\n",
    "        mlflow.log_param(\"number_of_estimators\", args.number_of_estimators)\n",
    "        mlflow.set_tag(\"version\", args.version_tag)\n",
    "        mlflow.set_tag(\"job_name\", args.job_name)\n",
    "        mlflow.log_param(\"gcs_train_source\", args.gcs_train_source)\n",
    "        if not args.gcs_train_source:\n",
    "            print(\"Missing GCS training source URI\")\n",
    "            return\n",
    "        mlflow.log_param(\"gcs_eval_source\", args.gcs_eval_source)\n",
    "        if not args.gcs_eval_source:\n",
    "            print(\"Missing GCS evaluation source URI\")\n",
    "            return\n",
    "\n",
    "        df = pd.read_csv(args.gcs_train_source, sep=csv_delimiter)\n",
    "        mlflow.log_param('training_shape', f'{df.shape}')\n",
    "        \n",
    "        X_train, y_train = feature_engineering(df)\n",
    "        rfr_pipe = build_pipeline(number_of_estimators=args.number_of_estimators)\n",
    "        \n",
    "        rfr_score = cross_val_score(rfr_pipe, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=5)\n",
    "        mlflow.log_metric(\"train_cross_valid_score_rmse_mean\", np.sqrt(-rfr_score).mean())\n",
    "        final_model = rfr_pipe.fit(X_train, y_train)\n",
    "#        signature = infer_signature(X_train, rfr_pipe.predict(X_train)) , signature=signature\n",
    "        mlflow.sklearn.log_model(final_model, \"chicago_rnd_forest\")\n",
    "\n",
    "        # Evaluate model to eval set\n",
    "        df = pd.read_csv(args.gcs_eval_source, sep=csv_delimiter)\n",
    "        mlflow.log_param('eval_shape',f'{df.shape}')\n",
    "        X_eval, y_eval = feature_engineering(df)\n",
    "        X_eval['fare_pred'] = final_model.predict(X_eval)\n",
    "        rfr_score = cross_val_score(final_model, X_eval, y_eval, scoring='neg_mean_squared_error', cv=5)\n",
    "        mlflow.log_metric(\"eval_cross_valid_score_rmse_mean\", np.sqrt(-rfr_score).mean())\n",
    "        \n",
    "        # Save model\n",
    "        model_file_name = f'{args.version_tag}.joblib'\n",
    "        mlflow.sklearn.save_model(final_model, model_file_name)\n",
    "        copy_local_directory_to_gcs(model_file_name, args.job_dir)\n",
    "        mlflow.log_param('model_file', args.job_dir+'/'+model_file_name)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Training arguments: \" + \" \".join(sys.argv[1:]))\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--number_of_estimators\", type=int)\n",
    "    parser.add_argument(\"--job-dir\", type=str)\n",
    "    parser.add_argument(\"--local_data\", type=str)\n",
    "    parser.add_argument(\"--gcs_train_source\", type=str)\n",
    "    parser.add_argument(\"--gcs_eval_source\", type=str)\n",
    "    parser.add_argument(\"--experiment_name\", type=str)\n",
    "    parser.add_argument(\"--version_tag\", type=str)\n",
    "    parser.add_argument(\"--job_name\", type=str)\n",
    "    \n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    # CLOUD_ML_JOB conatains other CAIP Training runtime parameters in JSON object\n",
    "    # job = os.environ[\"CLOUD_ML_JOB\"]\n",
    "    \n",
    "    # MLflow locally available\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:80\")\n",
    "\n",
    "    train_model(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer packege\n",
    "!cd package && python ./setup.py sdist\n",
    "\n",
    "# Copy to Composer data folder\n",
    "!gcloud composer environments storage data import \\\n",
    "    --environment {COMPOSER_NAME} \\\n",
    "    --location {REGION} \\\n",
    "    --source ./package/dist \\\n",
    "    --destination multi_model_trainer_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy package files to composer 'data' folder\n",
    "!gcloud composer environments storage data import \\\n",
    "    --environment {COMPOSER_NAME} \\\n",
    "    --location {REGION} \\\n",
    "    --source ./package \\\n",
    "    --destination multi_model_trainer_dag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model trainer Airflow DAG\n",
    "Notice: The entire cell is a template will be written to 'multi_model_trainer_dag.py' file.\n",
    "        'writetemplate' magic uses Jinja templating while Airflow also provides Jinja templating for runtime parameters.\n",
    "        Airflow parameters should be wrapped like this: {{ \"{{ ts_nodash }}\" }} because the template in the template mechanizm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate multi_model_trainer_dag.py\n",
    "\n",
    "# Train multiple models in separate AI Platform Training Jobs (PythonOperator)\n",
    "#  Input: data in GCS\n",
    "#  Output: model1.joblib model2.joblib\n",
    "#  Note: eval metric (one eval split) is stored in MLflow\n",
    "\n",
    "# Evaluate the previous model on the current  eval split\n",
    "#  Input: experiment Id (fetch the last (registered) model)\n",
    "#  Output: eval stored in MLflow for the previous model\n",
    "\n",
    "# Validate the model (PythonOperator)\n",
    "#  Input: Mflow metric\n",
    "#  Output: which model (path) to register\n",
    "\n",
    "# Register the model (PythonOperator) \n",
    "#  Input: Path of the winning model\n",
    "#  Output: Model in specific GCS location\n",
    "#  Registering model to MLFlow\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import (datetime, timedelta)\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "#import tensorflow_data_validation as tfdv\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "# TODO: Change to airflow.providers\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "from airflow.providers.google.cloud.operators.mlengine import MLEngineStartTrainingJobOperator\n",
    "\n",
    "csv_delimiter = '|'\n",
    "experiment_name = \"{{ var['experiment_name'] }}\"\n",
    "ML_IMAGE_URI = \"{{ var['ML_IMAGE_URI'] }}\"\n",
    "job_experiment_root = f\"{{ env['MLFLOW_GCS_ROOT_URI'] }}/experiments/{experiment_name}\"\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT\", default=\"edgeml-demo\")\n",
    "REGION = os.getenv(\"COMPOSER_LOCATION\", default=\"us-central1\")\n",
    "\n",
    "# Postfixes for temporary BQ tables and output CSV files\n",
    "TRAINING_POSTFIX = \"_training\"\n",
    "EVAL_POSTFIX = \"_eval\"\n",
    "VALIDATION_POSTFIX = \"_validation\"\n",
    "\n",
    "BQ_DATASET = \"chicago_taxi_trips\"\n",
    "BQ_TABLE = \"taxi_trips\"\n",
    "BQ_QUERY = \"\"\"\n",
    "with tmp_table as (\n",
    "SELECT trip_seconds, trip_miles, fare, tolls, \n",
    "    company, pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude,\n",
    "    DATETIME(trip_start_timestamp, 'America/Chicago') trip_start_timestamp,\n",
    "    DATETIME(trip_end_timestamp, 'America/Chicago') trip_end_timestamp,\n",
    "    CASE WHEN (pickup_community_area IN (56, 64, 76)) OR (dropoff_community_area IN (56, 64, 76)) THEN 1 else 0 END is_airport,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    dropoff_latitude IS NOT NULL and\n",
    "    dropoff_longitude IS NOT NULL and\n",
    "    pickup_latitude IS NOT NULL and\n",
    "    pickup_longitude IS NOT NULL and\n",
    "    fare > 0 and \n",
    "    trip_miles > 0 and\n",
    "    MOD(ABS(FARM_FINGERPRINT(unique_key)), 100) {}\n",
    "ORDER BY RAND()\n",
    "LIMIT {})\n",
    "SELECT *,\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) trip_start_year,\n",
    "    EXTRACT(MONTH FROM trip_start_timestamp) trip_start_month,\n",
    "    EXTRACT(DAY FROM trip_start_timestamp) trip_start_day,\n",
    "    EXTRACT(HOUR FROM trip_start_timestamp) trip_start_hour,\n",
    "    FORMAT_DATE('%a', DATE(trip_start_timestamp)) trip_start_day_of_week\n",
    "FROM tmp_table\n",
    "\"\"\"\n",
    "\n",
    "BQ_QUERY_FOR_TFDV = \"\"\"\n",
    "SELECT unique_key, taxi_id, trip_start_timestamp, trip_end_timestamp, trip_seconds, trip_miles, pickup_census_tract, \n",
    "    dropoff_census_tract, pickup_community_area, dropoff_community_area, fare, tips, tolls, extras, trip_total, \n",
    "    payment_type, company, pickup_latitude, pickup_longitude, pickup_location, dropoff_latitude, dropoff_longitude, dropoff_location\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
    "\"\"\"\n",
    "\n",
    "def generate_tfdv_statistics(gcs_file_name, **kwargs):\n",
    "    logging.info(\"Processing %s\", gcs_file_name)\n",
    "    # Currently skipped because of pip module versions in Airflow\n",
    "    #train_stats = tfdv.generate_statistics_from_csv(gcs_file_name)\n",
    "    #tfdv.WriteStatisticsToTFRecord(output_path = gcs_file_name + \".tfrecord\")\n",
    "    return None\n",
    "\n",
    "def joiner_func(training_gcs_file_name, eval_gcs_file_name, **kwargs):\n",
    "    logging.info(\"Joining %s, eval GCS files %s\", training_gcs_file_name, eval_gcs_file_name)\n",
    "    return None\n",
    "\n",
    "def model_trainer(training_gcs_file_name, eval_gcs_file_name, model_file, **kwargs):\n",
    "    logging.info(\"Training %s, eval GCS files %s\", training_gcs_file_name, eval_gcs_file_name)\n",
    "    return None\n",
    "\n",
    "def fake_model_tracking(**kwargs):\n",
    "    job_name = kwargs.get('templates_dict').get('job_name')\n",
    "    print(f\"Fake model tracking: '{job_name}'\")\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(nested=True) as run:\n",
    "        mlflow.log_param(\"number_of_estimators\", 0)\n",
    "        mlflow.set_tag(\"version\", \"fake\")\n",
    "        mlflow.set_tag(\"job_name\", job_name)\n",
    "        mlflow.log_metric(\"train_cross_valid_score_rmse_mean\", 1+random.random())\n",
    "        mlflow.log_metric(\"eval_cross_valid_score_rmse_mean\", 1+random.random())\n",
    "    return None\n",
    "\n",
    "def register_model(run_id, model_name):\n",
    "    model_uri = f'runs:/{run_id}/{model_name}'\n",
    "    registered_model = mlflow.register_model(model_uri, model_name)\n",
    "    print(registered_model)\n",
    "\n",
    "def compare_to_registered_model(model_name, best_run, metric_to_compare):\n",
    "    # Compare the best run with latest registered model\n",
    "    mlflow_client = mlflow.tracking.MlflowClient()\n",
    "    registered_models=mlflow_client.search_registered_models(filter_string=f\"name='{model_name}'\", max_results=1, order_by=['timestamp DESC'])\n",
    "    if len(registered_models)==0:\n",
    "        register_model(best_run.run_id, model_name)\n",
    "    else:\n",
    "        last_version = registered_models[0].latest_versions[0]\n",
    "        run = mlflow_client.get_run(last_version.run_id)\n",
    "        if not run:\n",
    "            print(f'Registered version run missing!')            \n",
    "            return None\n",
    "            \n",
    "        last_registered_metric=run.data.metrics[metric_to_compare]\n",
    "        best_run_metric=best_run['metrics.'+metric_to_compare]\n",
    "        # Smaller value is better\n",
    "        if last_registered_metric>best_run_metric:\n",
    "            print(f'Register better version with metric: {best_run_metric}')\n",
    "            register_model(best_run.run_id, experiment_name)\n",
    "        else:\n",
    "            print(f'Registered version still better. Metric: {last_registered_metric}')    \n",
    "\n",
    "def model_blessing(**kwargs):\n",
    "    job_name = kwargs.get('templates_dict').get('job_name')\n",
    "    print(f\"Model blessing: '{job_name}'\")\n",
    "\n",
    "    # Select the best from current training jobs\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    filter_string = f\"tags.job_name ILIKE '{job_name}_%'\"\n",
    "    df = mlflow.search_runs([experiment.experiment_id], filter_string=filter_string)\n",
    "\n",
    "    # Compare new trained model and select the best.\n",
    "    eval_max = df.loc[df['metrics.eval_cross_valid_score_rmse_mean'].idxmax()]\n",
    "    train_max= df.loc[df['metrics.train_cross_valid_score_rmse_mean'].idxmax()]\n",
    "    \n",
    "    compare_to_registered_model(experiment_name, eval_max, 'eval_cross_valid_score_rmse_mean')\n",
    "\n",
    "tasks = {\n",
    "    \"training\" : {\n",
    "        \"dataset_range\" : \"between 0 and 80\",\n",
    "        \"limit\" : 4000\n",
    "        },\n",
    "    \"eval\":{\n",
    "        \"dataset_range\" : \"between 80 and 100\",\n",
    "        \"limit\" : 1000\n",
    "        }}\n",
    "\n",
    "with DAG(\"multi_model_trainer\",\n",
    "         description = \"Train evaluate and validate multi models on taxi fare dataset. Select the best one and register it to Mlflow v0.86\",\n",
    "         schedule_interval = None, #'*/15 * * * *', #None, -> manual trigger\n",
    "         start_date = datetime(2021, 1, 1),\n",
    "         max_active_runs = 3,\n",
    "         catchup = False,\n",
    "         default_args = { 'provide_context': True}\n",
    "         ) as dag:\n",
    "\n",
    "    # Define task list for preparation\n",
    "    for task_key in tasks.keys():\n",
    "        # Note: fix table names causes race condition in case when DAG triggered before the previous finished.\n",
    "        table_name = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}_{task_key}\"\n",
    "        task = tasks[task_key]\n",
    "        task[\"gcs_file_name\"] = f\"{job_experiment_root}/data/ds_{task_key}.csv\"\n",
    "        \n",
    "        # Deletes previous training temporary tables\n",
    "        task[\"delete_table\"] = BigQueryTableDeleteOperator(\n",
    "            task_id = \"delete_table_\" + task_key,\n",
    "            deletion_dataset_table = table_name,\n",
    "            ignore_if_missing = True)\n",
    "\n",
    "        # Splits and copy source BQ table to 'dataset_range' sized segments\n",
    "        task[\"split_table\"] = BigQueryOperator(\n",
    "            task_id = \"split_table_\" + task_key,\n",
    "            use_legacy_sql=False,\n",
    "            destination_dataset_table = table_name,\n",
    "            sql = BQ_QUERY.format(task[\"dataset_range\"],task[\"limit\"]),\n",
    "            location = REGION)\n",
    "        \n",
    "        # Extract split tables to CSV files in GCS\n",
    "        task[\"extract_to_gcs\"] = BigQueryToCloudStorageOperator(\n",
    "            task_id = \"extract_to_gcs_\" + task_key,\n",
    "            source_project_dataset_table = table_name,\n",
    "            destination_cloud_storage_uris = [task[\"gcs_file_name\"]],\n",
    "            field_delimiter = csv_delimiter)\n",
    "        \n",
    "        # Generates statisctics by TFDV\n",
    "        task[\"tfdv_statisctics\"] = PythonOperator(\n",
    "            task_id = \"tfdv_statistics_for_\" + task_key,\n",
    "            python_callable = generate_tfdv_statistics,\n",
    "            op_kwargs={'gcs_file_name': task[\"gcs_file_name\"]})\n",
    "\n",
    "    joiner_1 = PythonOperator(\n",
    "        task_id = \"joiner_1\",\n",
    "        python_callable = joiner_func,\n",
    "        op_kwargs={ 'training_gcs_file_name': tasks[\"training\"][\"gcs_file_name\"],\n",
    "                    'eval_gcs_file_name': tasks[\"eval\"][\"gcs_file_name\"]})\n",
    "\n",
    "    # Model trainers\n",
    "#    trainer_1 = PythonOperator(\n",
    "#        task_id = \"trainer_1\",\n",
    "#        python_callable = model_trainer,\n",
    "#        op_kwargs={ 'training_gcs_file_name': tasks[\"training\"][\"gcs_file_name\"],\n",
    "#                    'eval_gcs_file_name': tasks[\"eval\"][\"gcs_file_name\"],\n",
    "#                    'model_file': f\"{job_experiment_root}/data/model1.joblib\"}\n",
    "#    )\n",
    "\n",
    "    submit_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    job_name = f\"training_job_{submit_time}\"\n",
    "    job_dir = f\"{job_experiment_root}/dmt_{submit_time}\"\n",
    "\n",
    "    # Template for string format ({variable}) and jinja template ({{variable}})\n",
    "    training_command_tmpl=\"\"\"gcloud ai-platform jobs submit training {job_name} \\\n",
    "        --region {region} \\\n",
    "        --scale-tier BASIC \\\n",
    "        --job-dir {job_dir} \\\n",
    "        --package-path /home/airflow/gcs/data/multi_model_trainer_dag/package/training/ \\\n",
    "        --module-name training.task \\\n",
    "        --master-image-uri {ml_image_uri} \\\n",
    "        --stream-logs \\\n",
    "        -- \\\n",
    "        --experiment_name {experiment_name} \\\n",
    "        --gcs_train_source {gcs_train_source} \\\n",
    "        --gcs_eval_source {gcs_eval_source} \\\n",
    "        --version_tag {version_tag} \\\n",
    "        --number_of_estimators {number_of_estimators} \\\n",
    "        --job_name {job_name}\"\"\"\n",
    "\n",
    "    training_tasks = []\n",
    "    for training_id in range(0, {{var['number_of_parallel_trainings']}}):\n",
    "        # Simulated training\n",
    "#        trainer = PythonOperator(\n",
    "#            task_id = f'trainer_{training_id}',\n",
    "#            python_callable = fake_model_tracking,\n",
    "#            templates_dict={'job_name': 'training_job_{{ \"{{ ts_nodash }}\" }}'+f'_{training_id}'})\n",
    "        \n",
    "        trainer = BashOperator(\n",
    "            task_id=f'trainer_{training_id}',\n",
    "            bash_command=training_command_tmpl.format(\n",
    "                 region = REGION,\n",
    "                 job_name = 'training_job_{{ \"{{ ts_nodash }}\" }}'+f'_{training_id}',\n",
    "                 job_dir = job_dir+f'_{training_id}',\n",
    "                 ml_image_uri = ML_IMAGE_URI,\n",
    "                 gcs_train_source = tasks[\"training\"][\"gcs_file_name\"],\n",
    "                 gcs_eval_source = tasks[\"eval\"][\"gcs_file_name\"],\n",
    "                 experiment_name = experiment_name,\n",
    "                 version_tag = f'trainer_{training_id}',\n",
    "                 # The only difference in trainings:\n",
    "                 number_of_estimators = random.randrange(60,200))\n",
    "        )\n",
    "\n",
    "        NATIVE_AIRFLOW=\"\"\"\n",
    "        trainer_1 = MLEngineStartTrainingJobOperator(\n",
    "            task_id=f'trainer_{training_id}',\n",
    "            project_id=PROJECT_ID,\n",
    "            job_id=f'trainer_{training_id}',\n",
    "            package_uris='gs://us-central1-mlops-50-af-e9fe149d-bucket/data/multi_model_trainer_dag/dist/trainer-0.1.tar.gz',\n",
    "            training_python_module='training.task',\n",
    "            master_type='CUSTOM',\n",
    "            master_config={\"imageUri\" : ML_IMAGE_URI},\n",
    "            training_args=[f'--jobDir={job_dir}',\n",
    "                           f'--experiment_name={experiment_name}',\n",
    "                           f'--gcs_train_source {tasks[\"training\"][\"gcs_file_name\"]}',\n",
    "                           f'--gcs_eval_source {tasks[\"eval\"][\"gcs_file_name\"]}',\n",
    "                           f'--version_tag trainer_{training_id}',\n",
    "                           f'--number_of_estimators {random.randrange(60,200)}',\n",
    "                           f'--job_name training_job_{{ \"{{ ts_nodash }}\" }}'+f'_{training_id}'\n",
    "                          ],\n",
    "            region=REGION,\n",
    "            scale_tier='BASIC',\n",
    "            runtime_version='2.3',\n",
    "            python_version='3.7',\n",
    "            mode=\"DRY_RUN\" # \"CLOUD\" or \"DRY_RUN\"\n",
    "        )\n",
    "        \"\"\"\n",
    "        training_tasks.append(trainer)\n",
    "    \n",
    "    # Select the best model of this run\n",
    "    model_blessing = PythonOperator(\n",
    "        task_id = \"model_blessing\",\n",
    "        python_callable = model_blessing,\n",
    "        templates_dict={'job_name': 'training_job_{{ \"{{ ts_nodash }}\" }}'})\n",
    "\n",
    "    # Exectute tasks\n",
    "    for task_key, task in tasks.items():\n",
    "        task[\"delete_table\"] >> task[\"split_table\"] >> task[\"extract_to_gcs\"] >> task[\"tfdv_statisctics\"]\n",
    "    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> joiner_1\n",
    "\n",
    "    # Brancing and merging training tasks\n",
    "    for trainer in training_tasks:\n",
    "        trainer.set_upstream(joiner_1)\n",
    "        model_blessing.set_upstream(trainer)\n",
    "\n",
    "#    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> trainer_1\n",
    "#    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> trainer_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DAG file to Cloud Composer\n",
    "!gcloud composer environments storage dags import \\\n",
    "  --environment {COMPOSER_NAME}  \\\n",
    "  --location {REGION} \\\n",
    "  --source multi_model_trainer_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
