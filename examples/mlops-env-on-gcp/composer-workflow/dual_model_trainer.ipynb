{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Jupyter magic template to create Python file with variable substitution. Will not working with string format inside the cell.\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Composer instance name: mlops-43-af\n",
      "Cloud Composer region: us-central1\n",
      "MLflow tracking server URI: http://127.0.0.1:80\n",
      "MLflow articfacts store root: gs://mlops-43-artifacts/experiments\n",
      "GCS root: gs://mlops-43-artifacts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr>You can check results of this test in MLflow and GCS folder:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4><a href=\"https://8b412f89dfc7171-dot-us-central2.pipelines.googleusercontent.com\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLflow UI</a></h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4><a href=\"https://console.cloud.google.com/storage/browser/mlops-43-artifacts/experiments\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open GCS folder</a></h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = \"chicago-taxi\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "MLFLOW_TRACKING_EXTERNAL_URI = os.environ[\"MLFLOW_TRACKING_EXTERNAL_URI\"]\n",
    "MLFLOW_EXPERIMENTS_URI = os.environ[\"MLFLOW_EXPERIMENTS_URI\"]\n",
    "training_artifacts_uri = MLFLOW_EXPERIMENTS_URI+experiment_name\n",
    "REGION=os.environ[\"MLOPS_REGION\"]\n",
    "ML_IMAGE_URI = os.environ[\"ML_IMAGE_URI\"]\n",
    "COMPOSER_NAME = os.environ[\"MLOPS_COMPOSER_NAME\"]\n",
    "MLFLOW_GCS_ROOT_URI = os.environ[\"MLFLOW_GCS_ROOT_URI\"]\n",
    "\n",
    "print(f\"Cloud Composer instance name: {COMPOSER_NAME}\")\n",
    "print(f\"Cloud Composer region: {REGION}\")\n",
    "print(f\"MLflow tracking server URI: {mlflow_tracking_uri}\")\n",
    "print(f\"MLflow articfacts store root: {MLFLOW_EXPERIMENTS_URI}\")\n",
    "print(f\"GCS root: {MLFLOW_GCS_ROOT_URI}\")\n",
    "\n",
    "experiment_path = MLFLOW_EXPERIMENTS_URI.replace(\"gs://\",\"\")\n",
    "display(HTML('<hr>You can check results of this test in MLflow and GCS folder:'))\n",
    "display(HTML(f'<h4><a href=\"{MLFLOW_TRACKING_EXTERNAL_URI}\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLflow UI</a></h4>'))\n",
    "display(HTML(f'<h4><a href=\"https://console.cloud.google.com/storage/browser/{experiment_path}\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open GCS folder</a></h4>'))\n",
    "\n",
    "!mkdir -p ./package/training\n",
    "!touch ./package/training/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./package/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./package/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['mlflow==1.8.0','PyMySQL==0.9.3']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Customer training setup.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate ./package/training/task.py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys, stat\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def train_model(args):\n",
    "    print(\"Taxi fare estimation model training step started...\")\n",
    "    with mlflow.start_run(nested=True):\n",
    "        #TODO 游때游때游때游때\n",
    "        mlflow.log_metric(\"score\", score)\n",
    "        mlflow.sklearn.log_model(lr, \"model\")\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Training arguments: \" + \" \".join(sys.argv[1:]))\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int)\n",
    "    parser.add_argument(\"--job-dir\", type=str)\n",
    "    parser.add_argument(\"--local_data\", type=str)\n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    # CLOUD_ML_JOB conatains other CAIP Training runtime parameters in JSON object\n",
    "    # job = os.environ[\"CLOUD_ML_JOB\"]\n",
    "    \n",
    "    # MLflow locally available\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:80\")\n",
    "    mlflow.set_experiment(\"{experiment_name}\")\n",
    "\n",
    "    print(\"Training main started\")\n",
    "    train_model(args)\n",
    "\n",
    "    # if --job-dir provided in 'ai-platform jobs submit' command you can upload any training result to that\n",
    "    # if args.job_dir:\n",
    "    # args.local_data, args.job_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: 'training_job_20201020_084356' will run in us-central1 region using image from:\n",
      " gcr.io/edgeml-demo/mlops-43-mlimage:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submit_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=f\"training_job_{submit_time}\"\n",
    "JOB_DIR=f\"{training_artifacts_uri}/training_{submit_time}\"\n",
    "print(f\"Training job name: '{JOB_NAME}' will run in {REGION} region using image from:\\n {ML_IMAGE_URI}\\n\")\n",
    "!gcloud composer environments storage data import \\\n",
    "    --environment {COMPOSER_NAME} \\\n",
    "    --location {REGION} \\\n",
    "    --source ./package \\\n",
    "    --destination dual_model_trainer_dag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate dual_model_trainer_dag.py\n",
    "# GCS folder where dataset CSV files are stored\n",
    "DATASET_GCS_FOLDER = \"{MLFLOW_GCS_ROOT_URI}/data\"\n",
    "REGION=\"{REGION}\"\n",
    "MLFLOW_EXPERIMENTS_URI=\"{MLFLOW_EXPERIMENTS_URI}\"\n",
    "\n",
    "JOB_DIR=\"{JOB_DIR}\"\n",
    "JOB_NAME=\"{JOB_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dual_model_trainer_dag.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a dual_model_trainer_dag.py\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "import tensorflow_data_validation as tfdv\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Working with Airflow 1.10.10 version!\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "\n",
    "default_args = dict(retries=1,start_date=airflow.utils.dates.days_ago(0))\n",
    "INTERVAL = \"@once\"\n",
    "START_DATE = datetime.datetime(2020, 9, 1)\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT\", \"edgeml-demo\")\n",
    "REGION = os.getenv(\"COMPOSER_LOCATION\", \"us-central\")\n",
    "\n",
    "# Postfixes for temporary BQ tables and output CSV files\n",
    "TRAINING_POSTFIX = \"_training\"\n",
    "EVAL_POSTFIX = \"_eval\"\n",
    "VALIDATION_POSTFIX = \"_validation\"\n",
    "\n",
    "BQ_DATASET = \"chicago_taxi_trips\"\n",
    "BQ_TABLE = \"taxi_trips\"\n",
    "BQ_QUERY = \"\"\"\n",
    "    SELECT unique_key, taxi_id, trip_start_timestamp, trip_end_timestamp, trip_seconds, trip_miles, pickup_census_tract, \n",
    "        dropoff_census_tract, pickup_community_area, dropoff_community_area, fare, tips, tolls, extras, trip_total, \n",
    "        payment_type, company, pickup_latitude, pickup_longitude, pickup_location, dropoff_latitude, dropoff_longitude, dropoff_location\n",
    "    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
    "    WHERE\n",
    "      dropoff_latitude IS NOT NULL and\n",
    "      dropoff_longitude IS NOT NULL and\n",
    "      dropoff_location  IS NOT NULL and\n",
    "      MOD(ABS(FARM_FINGERPRINT(unique_key)), 100) {}\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "\n",
    "command=f\"\"\"gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --scale-tier BASIC \\\n",
    "    --job-dir {JOB_DIR} \\\n",
    "    --package-path /home/airflow/gcs/data/dual_model_trainer_dag/package/training/ \\\n",
    "    --module-name training.task \\\n",
    "    --master-image-uri {JOB_DIR} \\\n",
    "    -- \\\n",
    "    --mlflowuri {MLFLOW_EXPERIMENTS_URI} \\\n",
    "    --epochs 2\"\"\"\n",
    "\n",
    "def generate_tfdv_statistics(gcs_file_name):\n",
    "    logging.info(\"Processing %s\", gcs_file_name)\n",
    "    train_stats = tfdv.generate_statistics_from_csv(gcs_file_name)\n",
    "    #tfdv.WriteStatisticsToTFRecord(output_path = gcs_file_name + \".tfrecord\")\n",
    "    return None\n",
    "\n",
    "with DAG(\"dual_model_trainer\",\n",
    "         description = \"Train evaluate and validate two models on taxi fare dataset. Select the best one and register it to Mlflow v0.02\",\n",
    "         schedule_interval = INTERVAL,\n",
    "         start_date = START_DATE,\n",
    "         catchup = False,\n",
    "         doc_md = __doc__\n",
    "         ) as dag:\n",
    "    \n",
    "    tasks = [{\n",
    "            \"postfix\" : \"training\",\n",
    "            \"dataset_range\" : \"between 0 and 80\"\n",
    "        },{\n",
    "            \"postfix\" : \"eval\",\n",
    "            \"dataset_range\" : \"between 80 and 95\"\n",
    "        },{\n",
    "            \"postfix\" : \"validation\",\n",
    "            \"dataset_range\" : \"between 95 and 100\"\n",
    "        }]\n",
    "    \n",
    "    # Define task list\n",
    "    for task in tasks:\n",
    "        logging.info(\"task: %s\", task)\n",
    "        postfix = task.get(\"postfix\")\n",
    "        # Note: fix table names causes race condition in case when DAG triggered before the previous finished.\n",
    "        table_name = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}_{postfix}\"\n",
    "        gcs_file_name = f\"{DATASET_GCS_FOLDER}/ds_{postfix}.csv\"\n",
    "        \n",
    "        # Deletes previous training temporary tables\n",
    "        task[\"delete_table\"] = BigQueryTableDeleteOperator(\n",
    "            task_id = \"delete_table_\" + postfix,\n",
    "            deletion_dataset_table = table_name,\n",
    "            ignore_if_missing = True)\n",
    "\n",
    "        # Splits and copy source BQ table to 'dataset_range' sized segments\n",
    "        task[\"split_table\"] = BigQueryOperator(\n",
    "            task_id = \"split_table_\" + postfix,\n",
    "            use_legacy_sql=False,\n",
    "            destination_dataset_table = table_name,\n",
    "            sql = BQ_QUERY.format(task[\"dataset_range\"]),\n",
    "            location = REGION)\n",
    "        \n",
    "        # Extract split tables to CSV files in GCS\n",
    "        task[\"extract_to_gcs\"] = BigQueryToCloudStorageOperator(\n",
    "            task_id = \"extract_to_gcs_\" + postfix,\n",
    "            source_project_dataset_table = table_name,\n",
    "            destination_cloud_storage_uris = [gcs_file_name],\n",
    "            field_delimiter = '|')\n",
    "        \n",
    "        # Generates statisctics by TFDV\n",
    "        task[\"tfdv_statisctics\"] = PythonOperator(\n",
    "            task_id = \"tfdv_statistics_for_\" + postfix,\n",
    "            python_callable = generate_tfdv_statistics,\n",
    "            provide_context = True,\n",
    "            op_kwargs={'gcs_file_name': gcs_file_name})\n",
    "\n",
    "    # Exectute tasks\n",
    "    for task in tasks:\n",
    "        task[\"delete_table\"] >> task[\"split_table\"] >> task[\"extract_to_gcs\"] >> task[\"tfdv_statisctics\"]\n",
    "    \n",
    "    # Train two models (two separate AI Platform Training Jobs) (PythonOperator)\n",
    "    #  Input: data in GCS\n",
    "    #  Output: model1.joblib model2.joblib\n",
    "    #  Note: eval metric (one eval split) is stored in MLflow\n",
    "\n",
    "    # Evaluate the previous model on the current  eval split\n",
    "    #  Input: experiment Id (fetch the last (registered) model)\n",
    "    #  Output: eval stored in MLflow for the previous model\n",
    "\n",
    "    # Validate the model (PythonOperator)\n",
    "    #  Input: Mflow metric\n",
    "    #  Output: which model (path) to register\n",
    "\n",
    "    # Register the model (PythonOperator) \n",
    "    #  Input: Path of the winning model\n",
    "    #  Output: Model in specific GCS location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage dags import \\\n",
    "  --environment {COMPOSER_NAME}  \\\n",
    "  --location {REGION} \\\n",
    "  --source dual_model_trainer_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
