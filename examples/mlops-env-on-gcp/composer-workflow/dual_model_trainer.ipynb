{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic jinja template to create Python file with variable substitution.\n",
    "# Dictonaries for substituted variables: env[] for OS environment vars and var[] for global variables\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "from jinja2 import Template\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    dirname = os.path.dirname(line)\n",
    "    if len(dirname)>0 and not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(Template(cell).render({'env' : os.environ, 'var' : globals()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"chicago-taxi\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "MLFLOW_TRACKING_EXTERNAL_URI = os.environ[\"MLFLOW_TRACKING_EXTERNAL_URI\"]\n",
    "\n",
    "REGION=os.environ[\"MLOPS_REGION\"]\n",
    "ML_IMAGE_URI = os.environ[\"ML_IMAGE_URI\"]\n",
    "COMPOSER_NAME = os.environ[\"MLOPS_COMPOSER_NAME\"]\n",
    "MLFLOW_GCS_ROOT_URI = os.environ[\"MLFLOW_GCS_ROOT_URI\"]\n",
    "\n",
    "print(f\"Cloud Composer instance name: {COMPOSER_NAME}\")\n",
    "print(f\"Cloud Composer region: {REGION}\")\n",
    "print(f\"MLflow tracking server URI: {mlflow_tracking_uri}\")\n",
    "print(f\"MLflow GCS root: {MLFLOW_GCS_ROOT_URI}\")\n",
    "\n",
    "experiment_path = MLFLOW_GCS_ROOT_URI.replace(\"gs://\",\"\")\n",
    "display(HTML('<hr>You can check results of this test in MLflow and GCS folder:'))\n",
    "display(HTML(f'<h4><a href=\"{MLFLOW_TRACKING_EXTERNAL_URI}\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLflow UI</a></h4>'))\n",
    "display(HTML(f'<h4><a href=\"https://console.cloud.google.com/storage/browser/{experiment_path}/experiments\" rel=\"noopener noreferrer\" target=\"_blank\">Click to open MLFlow GCS folder</a></h4>'))\n",
    "\n",
    "!mkdir -p ./package/training\n",
    "!touch ./package/training/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./package/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['mlflow==1.11.0','PyMySQL==0.9.3']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Customer training setup.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate ./package/training/task.py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys, stat\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def train_model(args):\n",
    "    print(\"Taxi fare estimation model training step started...\")\n",
    "    mlflow.set_experiment(args.experiment_name)\n",
    "    with mlflow.start_run(nested=True):\n",
    "        #TODO 游때游때游때游때\n",
    "        mlflow.log_metric(\"score\", score)\n",
    "        mlflow.sklearn.log_model(lr, \"model\")\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Training arguments: \" + \" \".join(sys.argv[1:]))\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int)\n",
    "    parser.add_argument(\"--job-dir\", type=str)\n",
    "    parser.add_argument(\"--local_data\", type=str)\n",
    "    parser.add_argument(\"--experiment_name\", type=str)\n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    # CLOUD_ML_JOB conatains other CAIP Training runtime parameters in JSON object\n",
    "    # job = os.environ[\"CLOUD_ML_JOB\"]\n",
    "    \n",
    "    # MLflow locally available\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:80\")\n",
    "\n",
    "\n",
    "    print(\"Training main started\")\n",
    "    train_model(args)\n",
    "\n",
    "    # if --job-dir provided in 'ai-platform jobs submit' command you can upload any training result to that\n",
    "    # if args.job_dir:\n",
    "    # args.local_data, args.job_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage data import \\\n",
    "    --environment {COMPOSER_NAME} \\\n",
    "    --location {REGION} \\\n",
    "    --source ./package \\\n",
    "    --destination dual_model_trainer_dag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate dual_model_trainer_dag.py\n",
    "import os\n",
    "import logging\n",
    "from datetime import (datetime, timedelta)\n",
    "#import tensorflow_data_validation as tfdv\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Working with Airflow 1.10.10 version!\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n",
    "from airflow.contrib.operators.mlengine_operator import MLEngineTrainingOperator\n",
    "\n",
    "experiment_name = \"{{ var['experiment_name'] }}\"\n",
    "ML_IMAGE_URI = \"{{ var['ML_IMAGE_URI'] }}\"\n",
    "job_experiment_root = f\"{{ env['MLFLOW_GCS_ROOT_URI'] }}/experiments/{experiment_name}\"\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT\")\n",
    "REGION = os.getenv(\"COMPOSER_LOCATION\")\n",
    "\n",
    "default_args = dict(retries=1,start_date=airflow.utils.dates.days_ago(0))\n",
    "\n",
    "# Postfixes for temporary BQ tables and output CSV files\n",
    "TRAINING_POSTFIX = \"_training\"\n",
    "EVAL_POSTFIX = \"_eval\"\n",
    "VALIDATION_POSTFIX = \"_validation\"\n",
    "\n",
    "BQ_DATASET = \"chicago_taxi_trips\"\n",
    "BQ_TABLE = \"taxi_trips\"\n",
    "BQ_QUERY = \"\"\"\n",
    "    SELECT unique_key, taxi_id, trip_start_timestamp, trip_end_timestamp, trip_seconds, trip_miles, pickup_census_tract, \n",
    "        dropoff_census_tract, pickup_community_area, dropoff_community_area, fare, tips, tolls, extras, trip_total, \n",
    "        payment_type, company, pickup_latitude, pickup_longitude, pickup_location, dropoff_latitude, dropoff_longitude, dropoff_location\n",
    "    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
    "    WHERE\n",
    "      dropoff_latitude IS NOT NULL and\n",
    "      dropoff_longitude IS NOT NULL and\n",
    "      dropoff_location  IS NOT NULL and\n",
    "      MOD(ABS(FARM_FINGERPRINT(unique_key)), 100) {}\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "\n",
    "training_command=\"\"\"gcloud ai-platform jobs submit training {job_name} \\\n",
    "    --region {region} \\\n",
    "    --scale-tier BASIC \\\n",
    "    --job-dir {job_dir} \\\n",
    "    --package-path /home/airflow/gcs/data/dual_model_trainer_dag/package/training/ \\\n",
    "    --module-name training.task \\\n",
    "    --master-image-uri {ml_image_uri} \\\n",
    "    -- \\\n",
    "    --experiment_name {experiment_name} \\\n",
    "    --epochs 2\"\"\"\n",
    "\n",
    "def generate_tfdv_statistics(gcs_file_name, **kwargs):\n",
    "    logging.info(\"Processing %s\", gcs_file_name)\n",
    "    # Currently skipped because of pip module versions in Airflow\n",
    "    #train_stats = tfdv.generate_statistics_from_csv(gcs_file_name)\n",
    "    #tfdv.WriteStatisticsToTFRecord(output_path = gcs_file_name + \".tfrecord\")\n",
    "    return None\n",
    "\n",
    "def joiner_1(training_gcs_file_name, eval_gcs_file_name, **kwargs):\n",
    "    logging.info(\"Joining %s, eval GCS files %s\", training_gcs_file_name, eval_gcs_file_name)\n",
    "    return None\n",
    "\n",
    "def model_trainer(training_gcs_file_name, eval_gcs_file_name, model_file, **kwargs):\n",
    "    logging.info(\"Training %s, eval GCS files %s\", training_gcs_file_name, eval_gcs_file_name)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "with DAG(\"dual_model_trainer\",\n",
    "         description = \"Train evaluate and validate two models on taxi fare dataset. Select the best one and register it to Mlflow v0.02\",\n",
    "         schedule_interval = None, # manual trigger\n",
    "         start_date = datetime(1969, 9, 1),\n",
    "         catchup = False,\n",
    "         doc_md = __doc__\n",
    "         ) as dag:\n",
    "\n",
    "    tasks = {\n",
    "        \"training\" : {\n",
    "            \"dataset_range\" : \"between 0 and 80\"\n",
    "            },\n",
    "        \"eval\":{\n",
    "            \"dataset_range\" : \"between 80 and 95\"\n",
    "            },\n",
    "        \"validation\": {\n",
    "            \"dataset_range\" : \"between 95 and 100\"\n",
    "        }}\n",
    "    \n",
    "    # Define task list for preparation\n",
    "    for task_key in tasks.keys():\n",
    "        # Note: fix table names causes race condition in case when DAG triggered before the previous finished.\n",
    "        table_name = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}_{task_key}\"\n",
    "        task = tasks[task_key]\n",
    "        task[\"gcs_file_name\"] = f\"{job_experiment_root}/data/ds_{task_key}.csv\"\n",
    "        \n",
    "        # Deletes previous training temporary tables\n",
    "        task[\"delete_table\"] = BigQueryTableDeleteOperator(\n",
    "            task_id = \"delete_table_\" + task_key,\n",
    "            deletion_dataset_table = table_name,\n",
    "            ignore_if_missing = True)\n",
    "\n",
    "        # Splits and copy source BQ table to 'dataset_range' sized segments\n",
    "        task[\"split_table\"] = BigQueryOperator(\n",
    "            task_id = \"split_table_\" + task_key,\n",
    "            use_legacy_sql=False,\n",
    "            destination_dataset_table = table_name,\n",
    "            sql = BQ_QUERY.format(task[\"dataset_range\"]),\n",
    "            location = REGION)\n",
    "        \n",
    "        # Extract split tables to CSV files in GCS\n",
    "        task[\"extract_to_gcs\"] = BigQueryToCloudStorageOperator(\n",
    "            task_id = \"extract_to_gcs_\" + task_key,\n",
    "            source_project_dataset_table = table_name,\n",
    "            destination_cloud_storage_uris = [task[\"gcs_file_name\"]],\n",
    "            field_delimiter = '|')\n",
    "        \n",
    "        # Generates statisctics by TFDV\n",
    "        task[\"tfdv_statisctics\"] = PythonOperator(\n",
    "            task_id = \"tfdv_statistics_for_\" + task_key,\n",
    "            python_callable = generate_tfdv_statistics,\n",
    "            provide_context = True,\n",
    "            op_kwargs={'gcs_file_name': task[\"gcs_file_name\"]})\n",
    "\n",
    "    \n",
    "    joiner_1 = PythonOperator(\n",
    "        task_id = \"joiner_1\",\n",
    "        python_callable = joiner_1,\n",
    "        provide_context = True,\n",
    "        op_kwargs={ 'training_gcs_file_name': tasks[\"training\"][\"gcs_file_name\"],\n",
    "                    'eval_gcs_file_name': tasks[\"eval\"][\"gcs_file_name\"]})\n",
    "\n",
    "    # Model trainers\n",
    "    \n",
    "#    trainer_1 = PythonOperator(\n",
    "#        task_id = \"trainer_1\",\n",
    "#        python_callable = model_trainer,\n",
    "#        provide_context = True,\n",
    "#        op_kwargs={ 'training_gcs_file_name': tasks[\"training\"][\"gcs_file_name\"],\n",
    "#                    'eval_gcs_file_name': tasks[\"eval\"][\"gcs_file_name\"],\n",
    "#                    'model_file': f\"{job_experiment_root}/data/model1.joblib\"}\n",
    "#    )\n",
    "#\n",
    "#    trainer_2 = PythonOperator(\n",
    "#        task_id = \"trainer_2\",\n",
    "#        python_callable = model_trainer,\n",
    "#        provide_context = True,\n",
    "#        op_kwargs={ 'training_gcs_file_name': tasks[\"training\"][\"gcs_file_name\"],\n",
    "#                    'eval_gcs_file_name': tasks[\"eval\"][\"gcs_file_name\"],\n",
    "#                    'model_file': f\"{job_experiment_root}/data/model2.joblib\"}\n",
    "#    )\n",
    "\n",
    "    submit_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    job_name = f\"training_job_{submit_time}\"\n",
    "    job_dir = f\"{job_experiment_root}/dmt_{submit_time}\"\n",
    "    print(f\"Training job: '{job_name}' will produce output to: {job_dir}\")\n",
    "\n",
    "    trainer_1 = BashOperator(\n",
    "        task_id=\"trainer_1\",\n",
    "        bash_command=training_command.format(region = REGION,\n",
    "                                             job_name = job_name+'_1',\n",
    "                                             job_dir = job_dir+'_1',\n",
    "                                             ml_image_uri = ML_IMAGE_URI,\n",
    "                                             experiment_name = experiment_name)\n",
    "    )\n",
    "    trainer_2 = BashOperator(\n",
    "        task_id=\"trainer_2\",\n",
    "        bash_command=training_command.format(region = REGION,\n",
    "                                             job_name = job_name+'_2',\n",
    "                                             job_dir = job_dir+'_2',\n",
    "                                             ml_image_uri = ML_IMAGE_URI,\n",
    "                                             experiment_name = experiment_name)\n",
    "    )\n",
    "\n",
    "    # Exectute tasks\n",
    "    for task_key, task in tasks.items():\n",
    "        task[\"delete_table\"] >> task[\"split_table\"] >> task[\"extract_to_gcs\"] >> task[\"tfdv_statisctics\"]\n",
    "    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> joiner_1\n",
    "    joiner_1 >> [trainer_1, trainer_2]\n",
    "#    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> trainer_1\n",
    "#    [tasks[\"training\"][\"tfdv_statisctics\"], tasks[\"eval\"][\"tfdv_statisctics\"]] >> trainer_2\n",
    "    \n",
    "    # Train two models (two separate AI Platform Training Jobs) (PythonOperator)\n",
    "    #  Input: data in GCS\n",
    "    #  Output: model1.joblib model2.joblib\n",
    "    #  Note: eval metric (one eval split) is stored in MLflow\n",
    "\n",
    "    # Evaluate the previous model on the current  eval split\n",
    "    #  Input: experiment Id (fetch the last (registered) model)\n",
    "    #  Output: eval stored in MLflow for the previous model\n",
    "\n",
    "    # Validate the model (PythonOperator)\n",
    "    #  Input: Mflow metric\n",
    "    #  Output: which model (path) to register\n",
    "\n",
    "    # Register the model (PythonOperator) \n",
    "    #  Input: Path of the winning model\n",
    "    #  Output: Model in specific GCS location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments storage dags import \\\n",
    "  --environment {COMPOSER_NAME}  \\\n",
    "  --location {REGION} \\\n",
    "  --source dual_model_trainer_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
