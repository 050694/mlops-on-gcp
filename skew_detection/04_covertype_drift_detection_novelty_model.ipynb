{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 04-covertype-model-based-skew-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zT-Pj-s1asEh",
        "Kg_Etw2KyEcz",
        "zmHpShFzyRqW",
        "rmtZqrlvyX5A",
        "UcUD6B5C1Wme",
        "6k_GBAWt1Vl1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLFeOqfFkTu",
        "colab_type": "text"
      },
      "source": [
        "# Detecting Training-Serving Data Skews using Novelty Detection Modeling\n",
        "\n",
        "This tutorial shows how to use novelty detection models to detect skews between data split (e.g. training and serving). Novelty detection models can identify whether an instance blongs to a population, or is considered as an outlier. \n",
        "\n",
        "The tutorial also shows that, while analyzing feature-by-feature can identify a distribution skew in a feature, it cannot identify instances as outliers, as their feature values might be in the expected range, but the combination of such feature values is odd.\n",
        "\n",
        "The tutorial consists of the following parts:\n",
        "\n",
        "* **Part 1: Produceing baseline statistics and reference schema**\n",
        " 1. Download training and serving  data splits\n",
        " 2. Compute baseline statistics and reference schema using training data \n",
        " 3. Validate the serving data against the reference schema and statistics\n",
        "\n",
        "* **Part 2: Generating mutated data points**\n",
        "  1. Generate mutated data with random featuer value combinations\n",
        "  2. Validate the mutated data against the reference schema and \n",
        "statistics\n",
        "\n",
        "* **Part 3: Detecting data skews with Elliptic Envelope**\n",
        "  1. Train Elliptic Envelope using the training data\n",
        "  2. Validate the normal and mutated  data against the model\n",
        "\n",
        "* **Part 4: Detecting skews in BigQuery request-reponse data**\n",
        "  1. Implement Apache Beam pipeline for model-based drift detection\n",
        "  2. Display drift detection output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-Pj-s1asEh",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YAsA6ZIKeHq",
        "colab_type": "text"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac7sDkawFX55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q apache-beam[interactive]\n",
        "!pip install -U -q tensorflow_data_validation\n",
        "!pip install -U -q pandas\n",
        "!pip install -U -q sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3qp8K3BKp4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOy7TgT5K8-Y",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXWABKNUFkDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tensorflow import io as tf_io\n",
        "import tensorflow_data_validation as tfdv\n",
        "import apache_beam as beam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "print(\"TFDV version: {}\".format(tfdv.__version__))\n",
        "print(\"Apache Beam version: {}\".format(beam.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK0oWyt7LD_3",
        "colab_type": "text"
      },
      "source": [
        "### Create a local workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWL4Ifwubtn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCS_DATA_LOCATION = 'gs://workshop-datasets/covertype/data_validation'\n",
        "WORKSPACE = './workspace'\n",
        "DATA_DIR = os.path.join(WORKSPACE, 'data')\n",
        "TRAIN_DATA = os.path.join(DATA_DIR, 'train.csv') \n",
        "EVAL_DATA = os.path.join(DATA_DIR, 'eval.csv') \n",
        "MODELS_DIR = os.path.join(WORKSPACE, 'models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe720dJd0-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf_io.gfile.exists(WORKSPACE):\n",
        "  print(\"Removing previous workspace artifacts...\")\n",
        "  tf_io.gfile.rmtree(WORKSPACE)\n",
        "\n",
        "print(\"Creating a new workspace...\")\n",
        "tf_io.gfile.makedirs(WORKSPACE)\n",
        "tf_io.gfile.makedirs(DATA_DIR)\n",
        "tf_io.gfile.makedirs(MODELS_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg_Etw2KyEcz",
        "colab_type": "text"
      },
      "source": [
        "# Part1: Produceing baseline statistics and reference schema\n",
        "\n",
        " 1. Download training and serving  data splits\n",
        " 2. Compute baseline statistics and reference schema using training data \n",
        " 3. Validate the serving data against the reference schema and statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KPT7KfudAUf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download Data Splits\n",
        "\n",
        "We use the [covertype](https://archive.ics.uci.edu/ml/datasets/covertype) from UCI Machine Learning Repository. The task is to Predict forest cover type from cartographic variables only. \n",
        "\n",
        "The dataset is preprocessed, split, and uploaded to the `gs://workshop-datasets/covertype` public GCS location. \n",
        "\n",
        "We use this version of the preprocessed dataset in this notebook. For more information, see [Cover Type Dataset](https://github.com/GoogleCloudPlatform/mlops-on-gcp/tree/master/datasets/covertype)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aCzZNfXcqVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://workshop-datasets/covertype/data_validation/training/dataset.csv {TRAIN_DATA}\n",
        "!gsutil cp gs://workshop-datasets/covertype/data_validation/evaluation/dataset.csv {EVAL_DATA}\n",
        "!wc -l {TRAIN_DATA}\n",
        "!wc -l {EVAL_DATA}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zCYn7e6dRWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = pd.read_csv(TRAIN_DATA).head()\n",
        "sample.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k5lNNFedkZI",
        "colab_type": "text"
      },
      "source": [
        "## 2. Create Schema and Statistics with TFDV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9K50OmEekWY",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Compute and visualize the statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxSh50yldTRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA\n",
        ")\n",
        "\n",
        "# tfdv.visualize_statistics(baseline_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBYwxbLLfO0p",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Generate and display schema "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-92Hhdtpfcsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_metadata.proto.v0 import schema_pb2, statistics_pb2\n",
        "\n",
        "# Infer schema\n",
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "# Set Soil_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Soil_Type', schema_pb2.IntDomain(\n",
        "    name='Soil_Type', is_categorical=True))\n",
        "\n",
        "# Set Cover_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Cover_Type', schema_pb2.IntDomain(\n",
        "    name='Cover_Type', is_categorical=True))\n",
        "\n",
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        schema=reference_schema,\n",
        "        sample_count=10000\n",
        "        )\n",
        "    )\n",
        "\n",
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "# Set Soil_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Soil_Type', schema_pb2.IntDomain(\n",
        "    name='Soil_Type', is_categorical=True))\n",
        "\n",
        "# Set Cover_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Cover_Type', schema_pb2.IntDomain(\n",
        "    name='Cover_Type', is_categorical=True))\n",
        "\n",
        "reference_schema.default_environment.append('TRAINING')\n",
        "reference_schema.default_environment.append('SERVING')\n",
        "\n",
        "# Specify that 'Cover_Type' feature is not in SERVING environment.\n",
        "tfdv.get_feature(reference_schema, 'Cover_Type').not_in_environment.append('SERVING')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAqrBG2LL1UK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.display_schema(\n",
        "    schema=reference_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Ny_e15i_b0",
        "colab_type": "text"
      },
      "source": [
        "## 3. Validate Serving data against the reference schema and stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6y6I6LehxwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serving_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=EVAL_DATA,\n",
        "    stats_options = tfdv.StatsOptions(\n",
        "        schema=reference_schema\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lro64VSnwmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anomalies = tfdv.validate_statistics(\n",
        "    serving_stats, \n",
        "    schema=reference_schema,\n",
        "    previous_statistics=baseline_stats,\n",
        "    environment='TRAINING'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL8a9wS-oIo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmHpShFzyRqW",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Generating mutated data points\n",
        "\n",
        "  1. Generate mutated data with random featuer value combinations\n",
        "  2. Validate the mutated data against the reference schema and \n",
        "statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f9_91krqigO",
        "colab_type": "text"
      },
      "source": [
        "## 1. Generate Mutated Serving Data\n",
        "We are going to generate a dataset with mutated data points, by shuffling each column values accross the rows, creating rows with random combination of feature values.\n",
        "\n",
        "This method makes sure that the values of each feature, independently, follows the distribution of the original serving data. However, the joint distribution is completely different, since we generate feature values independetly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEu4Eg1cpjOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serving_data = pd.read_csv(EVAL_DATA).drop('Cover_Type', axis=1)\n",
        "serving_data.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sox72_k1t9o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_values(dataframe):     \n",
        "  shuffeld_dataframe = dataframe.copy()\n",
        "  for column_name in dataframe.columns:\n",
        "    shuffeld_dataframe[column_name] = shuffeld_dataframe[column_name].sample(frac=1.0).reset_index(drop=True)\n",
        "\n",
        "  return shuffeld_dataframe\n",
        "\n",
        "mutated_serving_data = shuffle_values(serving_data)\n",
        "mutated_serving_data.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tcfc6BPWdz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MUTATED_DATA_FILE = os.path.join(DATA_DIR, 'mutated_data.csv')\n",
        "mutated_serving_data.to_csv(MUTATED_DATA_FILE, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC_AUo4UZAlJ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Validate the mutated serving data against the reference schema\n",
        "\n",
        "Notice that the individual feature distributions are the same as the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH0B1WqSVcaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mutated_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=MUTATED_DATA_FILE)\n",
        "\n",
        "sample_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA)\n",
        "\n",
        "tfdv.visualize_statistics(\n",
        "    sample_stats, mutated_stats, 'Original', 'Mutated')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m30Z6Oj3XTY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anomalies = tfdv.validate_statistics(\n",
        "    mutated_stats, \n",
        "    schema=reference_schema,\n",
        "    previous_statistics=baseline_stats,\n",
        "    environment = 'SERVING'\n",
        ")\n",
        "\n",
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmtZqrlvyX5A",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Detecting data skews with Elliptic Envelope\n",
        "\n",
        "  1. Train Elliptic Envelope using the training data\n",
        "  2. Validate the normal and mutated  data against the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ogDNC4ZVs5",
        "colab_type": "text"
      },
      "source": [
        "## 1. Train an Elliptic Envelope Model using Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LV9fC_ZzBp",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Define metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OAiQg_fZyNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_FEATURE_NAME = 'Cover_Type'\n",
        "\n",
        "CATEGORICAL_FEATURE_NAMES = [\n",
        "    'Soil_Type',\n",
        "    'Wilderness_Area'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj4K29vUofme",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0ghh6C188w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(TRAIN_DATA).drop(TARGET_FEATURE_NAME, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUjyeroWbhdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "encoders = dict()\n",
        "\n",
        "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "  encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "  encoder.fit(train_data[[feature_name]])\n",
        "  encoders[feature_name] = encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpIfUb8vczNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_frame):\n",
        "\n",
        "  if type(data_frame) != pd.DataFrame:\n",
        "    data_frame = pd.DataFrame(data_frame)\n",
        "  \n",
        "  data_frame = data_frame.reset_index()\n",
        "  for feature_name, encoder in encoders.items():\n",
        "    encoded_feature = pd.DataFrame(\n",
        "      encoder.transform(data_frame[[feature_name]]).toarray()\n",
        "    )\n",
        "    data_frame = data_frame.drop(feature_name, axis=1)\n",
        "    encoded_feature.columns = [feature_name+\"_\"+str(column) \n",
        "                               for column in encoded_feature.columns]\n",
        "    data_frame = data_frame.join(encoded_feature)\n",
        "  \n",
        "  return data_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMDdQ2coknt",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqcYr03tqEF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepared_training_data = prepare_data(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaGeGFL2mVlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "model = EllipticEnvelope(contamination=0.)\n",
        "\n",
        "print(\"Fitting...\")\n",
        "t0 = time.time()\n",
        "model.fit(prepared_training_data)\n",
        "t1 = time.time()\n",
        "print(\"Model is fitted in {} seconds.\".format(round(t1-t0)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PAREHDGaCsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import statistics\n",
        "\n",
        "training_distances = model.mahalanobis(prepared_training_data)\n",
        "model._mean = statistics.mean(training_distances)\n",
        "model._stdv = statistics.stdev(training_distances)\n",
        "print(\"training distance mean: {}\".format(round(model._mean, 5))) \n",
        "print(\"training distance stdv: {}\".format(round(model._stdv, 5)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLcclzLKfOpx",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use the Elliptic Envelope Model to Validate Serving Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N6mmbYETH7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_data(model, data_frame, stdv_units=2):\n",
        "\n",
        "  distances = model.mahalanobis(data_frame)\n",
        "  distance = statistics.mean(distances)\n",
        "  threshold = model._mean + (stdv_units * model._stdv)\n",
        "  ratio = len([v for v in distances if v >= threshold]) / len(data_frame.index)\n",
        "  \n",
        "  return distance, ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glmHgv5Gfb4c",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Validate normal serving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dRRzH6KgDKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stdv_units = 2\n",
        "prepared_serving_data = prepare_data(serving_data)\n",
        "results = validate_data(model, prepared_serving_data, stdv_units)\n",
        "score = round(results[1]*100, 2)\n",
        "print(\"There is {}% of the data points more than {} standard deviation units away from the mean of the training data\".format(score, stdv_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gugytaZhfcS7",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Validate mutated serving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_xWczW9pzHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepared_mutated_data = prepare_data(mutated_serving_data)\n",
        "results = validate_data(model, prepared_mutated_data, stdv_units)\n",
        "score = round(results[1]*100, 2)\n",
        "print(\"There is {}% of the data points more than {} standard deviation units away from the mean of the training data\".format(score, stdv_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BJmtt6ezA9u",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: Detecting skews in BigQuery request-reponse data\n",
        "\n",
        "  1. Implement Apache Beam pipeline for model-based drift detection\n",
        "  2. Run pipeline and display drift detection output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTcAHK_E1eHB",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8zOmNew8yd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcUD6B5C1Wme",
        "colab_type": "text"
      },
      "source": [
        "### Configure GCP environment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCfREVp91JoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID = \"sa-data-validation\"\n",
        "BUCKET = \"sa-data-validation\"\n",
        "BQ_DATASET_NAME = 'prediction_logs'\n",
        "BQ_TABLE_NAME = 'covertype_classifier_logs'  \n",
        "MODEL_NAME = 'covertype_classifier'\n",
        "MODEL_VERSION = 'v1'\n",
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k_GBAWt1Vl1",
        "colab_type": "text"
      },
      "source": [
        "### Authenticate your GCP account\n",
        "This is required if you run the notebook in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKrmsAaH1SlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  print(\"Colab user is authenticated.\")\n",
        "except: pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNt6gQiaziPg",
        "colab_type": "text"
      },
      "source": [
        "## 1. Implement Apache Beam pipeline for model-based drift detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDILrGNg5XDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def parse_batch_data(log_records):\n",
        "  data_dict = defaultdict(list)\n",
        "\n",
        "  for log_record in log_records:\n",
        "    raw_data = json.loads(log_record['raw_data'])\n",
        "    for raw_instance in raw_data['instances']:\n",
        "      for name, value in raw_instance.items():\n",
        "        data_dict[name].append(value[0])\n",
        "\n",
        "  return data_dict\n",
        "\n",
        "\n",
        "def score_data(data, model, stdv_units=2):\n",
        "  distances = model.mahalanobis(data)\n",
        "  threshold = model._mean + (stdv_units * model._stdv)\n",
        "  outlier_count = len([v for v in distances if v >= threshold])\n",
        "  records_count = len(data)\n",
        "  return {'outlier_count': outlier_count, 'records_count': records_count}\n",
        "\n",
        "\n",
        "def aggregate_scores(items):\n",
        "  outlier_count = 0 \n",
        "  records_count = 0\n",
        "  for item in items:\n",
        "    outlier_count += item['outlier_count']\n",
        "    records_count += item['records_count']\n",
        "  return {'outlier_count': outlier_count, 'records_count': records_count}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xakztiZY7SlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_query(bq_table_fullname, model_name, model_version, start_time, end_time):\n",
        "  query = \"\"\"\n",
        "  SELECT raw_data\n",
        "  FROM {}\n",
        "  WHERE model = '{}'\n",
        "  AND model_version = '{}'\n",
        "  \"\"\".format(bq_table_fullname, model_name, model_version, start_time, end_time)\n",
        "\n",
        "  return query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRke4Z62c4S",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Beam pipeline implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ClnVBlnzEkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_pipeline(args):\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "  query = get_query(\n",
        "      args.bq_table_fullname, args.model_name, \n",
        "      args.model_version, \n",
        "      args.start_time, \n",
        "      args.end_time\n",
        "  )\n",
        "\n",
        "  print(\"Starting the Beam pipeline...\")\n",
        "  with beam.Pipeline(options=options) as pipeline:\n",
        "    (\n",
        "        pipeline \n",
        "        | 'ReadBigQueryData' >> beam.io.Read(\n",
        "            beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
        "        | 'BatchRecords' >> beam.BatchElements(\n",
        "            min_batch_size=100, max_batch_size=1000)\n",
        "        | 'InstancesToBeamExamples' >> beam.Map(parse_batch_data)\n",
        "        | 'PrepareData' >> beam.Map(prepare_data)\n",
        "        | 'ScoreData' >> beam.Map(\n",
        "            lambda data: score_data(data, args.drift_model, stdv_units=1))\n",
        "        | 'CombineResults' >> beam.CombineGlobally(aggregate_scores)\n",
        "        | 'ComputeRatio' >> beam.Map(\n",
        "            lambda result: {\n",
        "                \"outlier_count\": result['outlier_count'], \n",
        "                \"records_count\": result['records_count'],\n",
        "                \"drift_ratio\": result['outlier_count'] / result['records_count']\n",
        "                })\n",
        "         | 'WriteOutput' >> beam.io.WriteToText(\n",
        "             file_path_prefix=args.output_file_path, num_shards=1, shard_name_template='')\n",
        "    )\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbntsBEo2cKW",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Pipeline parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_qNMilj2xUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "job_name = 'drift-detection-{}'.format(\n",
        "    datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
        "bq_table_fullname = \"{}.{}.{}\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
        "runner = 'InteractiveRunner'\n",
        "output_dir = os.path.join(WORKSPACE, 'output')\n",
        "output_path = os.path.join(output_dir, 'drift_output.json')\n",
        "start_time = '2020-06-05 00:00:00 UTC'\n",
        "end_time = '2020-06-06 23:59:59 UTC'\n",
        "\n",
        "args = {\n",
        "    'job_name': job_name,\n",
        "    'runner': runner,\n",
        "    'bq_table_fullname': bq_table_fullname,\n",
        "    'model_name': MODEL_NAME,\n",
        "    'model_version': MODEL_VERSION,\n",
        "    'start_time': start_time,\n",
        "    'end_time': end_time,\n",
        "    'output_file_path': output_path,\n",
        "    'project': PROJECT_ID,\n",
        "    'reference_schema': reference_schema,\n",
        "    'drift_model': model\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByWZEfpr2xO4",
        "colab_type": "text"
      },
      "source": [
        "## 2. Run pipeline and display drift detection output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJstUEVC2yw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r {output_dir}\n",
        "\n",
        "print(\"Running pipeline...\")\n",
        "%time run_pipeline(args)\n",
        "print(\"Pipeline is done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmjApTrazwqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls {output_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cKtvu4PIOfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dirft_results = json.loads(open(output_path).read()).items()\n",
        "for key, value in dirft_results:\n",
        "  if key == 'drift_ratio':\n",
        "    value = str(round(value * 100, 2)) +'%'\n",
        "  print(key,':', value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFQSTQY-J2JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}