{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-covertype-drift-detection-tfdv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcjbiI9dkOh",
        "colab_type": "text"
      },
      "source": [
        "# Drift Detection with TensorFlow Data Validation\n",
        "\n",
        "This tutorial shows how to use [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) (TFDV) to identify and analyze different data skews in request-response serving data logged by AI Platform Prediction in BigQuery.\n",
        "\n",
        "The tutorial has two parts:\n",
        "\n",
        "* **Part 1**: Detecting data skews\n",
        " 1. Download training data\n",
        " 2. Generate baseline statistics and reference schema using TFDV\n",
        " 3. Implement an Apache Beam pipeline to validate serving data logged in BigQuery. This pipeline stores the generated statistics and anomalies to disk. \n",
        "\n",
        "* **Part 2**: Analyzing statistics and anomalies\n",
        "  1. Load the generated serving statistics and anomalies from disk\n",
        "  2. Use TFDV to visualize and display the statistics and anomalies\n",
        "  3. Analyze how statistics change over time.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoQwXZ7jhA1R",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL_EyB1phEA3",
        "colab_type": "text"
      },
      "source": [
        "### Install packages and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8geaoXthnvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q tensorflow\n",
        "!pip install -U -q tensorflow_data_validation\n",
        "!pip install -U -q apache_beam\n",
        "!pip install -U -q pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7igWnNgh0AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDWGkvAh3ji",
        "colab_type": "text"
      },
      "source": [
        "### Configure GCP environment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIw9c6Qh0xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID = \"sa-data-validation\"\n",
        "BQ_DATASET_NAME = 'prediction_logs'\n",
        "BQ_TABLE_NAME = 'covertype_classifier_logs'  \n",
        "MODEL_NAME = 'covertype_classifier'\n",
        "MODEL_VERSION = 'v1'\n",
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-xkckCAiVXk",
        "colab_type": "text"
      },
      "source": [
        "### Authenticate your GCP account\n",
        "\n",
        "This is required if you run the notebook in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzB4OFtWiSY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  print(\"Colab user is authenticated.\")\n",
        "except: pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNZP0LzXiYFz",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmGsur3xiXdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "import apache_beam as beam\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "print(\"TF version: {}\".format(tf.__version__))\n",
        "print(\"TFDV version: {}\".format(tfdv.__version__))\n",
        "print(\"Beam version: {}\".format(beam.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ0ZEarvjKNx",
        "colab_type": "text"
      },
      "source": [
        "### Create a local workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAn8OIUJjKTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORKSPACE = './workspace'\n",
        "DATA_DIR = os.path.join(WORKSPACE, 'data')\n",
        "TRAIN_DATA = os.path.join(DATA_DIR, 'train.csv') \n",
        "ARTIFACTS_DIR = os.path.join(WORKSPACE, 'artifacts')\n",
        "\n",
        "if tf.io.gfile.exists(WORKSPACE):\n",
        "  print(\"Removing previous workspace artifacts...\")\n",
        "  tf.io.gfile.rmtree(WORKSPACE)\n",
        "\n",
        "print(\"Creating a new workspace...\")\n",
        "tf.io.gfile.makedirs(WORKSPACE)\n",
        "tf.io.gfile.makedirs(DATA_DIR)\n",
        "tf.io.gfile.makedirs(ARTIFACTS_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXgmrq8yMhk5",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Detecting Serving Data Skews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoaUj1yZj8Jg",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download data\n",
        "\n",
        "We use the [covertype](https://archive.ics.uci.edu/ml/datasets/covertype) from UCI Machine Learning Repository.\n",
        "\n",
        "The dataset is preprocessed, split, and uploaded to the `gs://workshop-datasets/covertype` public GCS location. We use this version of the preprocessed dataset in this notebook. For more information, see [Cover Type Dataset](https://github.com/GoogleCloudPlatform/mlops-on-gcp/tree/master/datasets/covertype).\n",
        "\n",
        "We use the training data split to generate reference schema and statistics from, in order to use for validating serving data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCGwr-dkN8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://workshop-datasets/covertype/data_validation/training/dataset.csv {TRAIN_DATA}\n",
        "!wc -l {TRAIN_DATA}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U82cqeblqpHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = pd.read_csv(TRAIN_DATA).head()\n",
        "sample.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1dYHVLqwdB",
        "colab_type": "text"
      },
      "source": [
        "## 2. Generate Baseline Statistics and Reference with TFDV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE-wWIywq9ZE",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Generate statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e8dLW_0qyu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA,\n",
        "    stats_options = tfdv.StatsOptions(\n",
        "        sample_count=1000\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oskOMHmZrJ5I",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Visualize statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5FoTGPCrIip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.visualize_statistics(baseline_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2jWSKatq4Vr",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Generate a schema "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "364Hnw6Azo7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_metadata.proto.v0 import schema_pb2, statistics_pb2\n",
        "\n",
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "tfdv.set_domain(reference_schema, 'Soil_Type', schema_pb2.IntDomain(\n",
        "    name='Soil_Type', is_categorical=True))\n",
        "\n",
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        schema=reference_schema,\n",
        "        sample_count=1000\n",
        "        )\n",
        "    )\n",
        "\n",
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "tfdv.get_feature(reference_schema, 'Soil_Type').type = 1\n",
        "\n",
        "\n",
        "reference_schema.default_environment.append('TRAINING')\n",
        "reference_schema.default_environment.append('SERVING')\n",
        "\n",
        "# Specify that 'Cover_Type' feature is not in SERVING environment.\n",
        "tfdv.get_feature(\n",
        "    reference_schema, 'Cover_Type').not_in_environment.append('SERVING')\n",
        "\n",
        "tfdv.display_schema(schema=reference_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-3Q5am5bUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_stats(stats):\n",
        "  # treat 'Soil_Type' (feature 11) as a categorical feature (type = 2)\n",
        "  for feature in stats.datasets[0].features:\n",
        "    if feature.path.step == ['Soil_Type']:\n",
        "      feature.type = 2\n",
        "      break\n",
        "  return stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w31P5YSy3OX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_stats(baseline_stats)\n",
        "\n",
        "tfdv.display_anomalies(\n",
        "    tfdv.validate_statistics(\n",
        "        baseline_stats, \n",
        "        reference_schema, \n",
        "        environment='TRAINING'\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPa4NMpBsS0C",
        "colab_type": "text"
      },
      "source": [
        "### 2.4. Store reference schema and statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izAJfFausS8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REFERENCE_SCHEMA_FILE = os.path.join(ARTIFACTS_DIR, 'reference_schema.pbtxt')\n",
        "\n",
        "tfdv.write_schema_text(\n",
        "    reference_schema,\n",
        "    REFERENCE_SCHEMA_FILE \n",
        "    \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o2j7D7gsTFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASELINE_STATS_FILE = os.path.join(ARTIFACTS_DIR, 'baseline_statistics.pbtxt')\n",
        "\n",
        "tfdv.utils.stats_util.write_stats_text(\n",
        "    baseline_stats, \n",
        "    BASELINE_STATS_FILE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJUiOf-oNLU",
        "colab_type": "text"
      },
      "source": [
        "## 3. Implementing Apache Beam Pipeline for Skew Detection\n",
        "The Beam pipeline will perform the following steps:\n",
        "1. Read the raw serving request-response data in the BigQuery logs table\n",
        "2. Parse the data to BeamExamples\n",
        "3. Generate statistics for the serving data\n",
        "4. Validate the serving statistics against the reference_schema\n",
        "5. Store the serving statistics and any detected anomalies for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CXEj6SVp27H",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Implement helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVk68lE9uUkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_query(bq_table, model_name, model_version, start_time, end_time):\n",
        "  query = \"\"\"\n",
        "    SELECT *\n",
        "    FROM `{}`\n",
        "    WHERE model = '{}' AND model_version = '{}'\n",
        "    AND time BETWEEN '{}' AND '{}';\n",
        "  \"\"\".format(bq_table, model_name, model_version, start_time, end_time)\n",
        "\n",
        "  return query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUXqtpeBwCA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_RAW_DATA_COLUMN = 'raw_data'\n",
        "_INSTANCES_KEY = 'instances'\n",
        "\n",
        "class JSONObjectCoder(beam.DoFn):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._example_size = beam.metrics.Metrics.counter(\n",
        "      tfdv.constants.METRICS_NAMESPACE, \"example_size\")\n",
        "      \n",
        "  def process(self, log_record):\n",
        "    \n",
        "    raw_data = json.loads(log_record[_RAW_DATA_COLUMN])\n",
        "    for instance in raw_data[_INSTANCES_KEY]:\n",
        "        for key, value in instance.items():\n",
        "            instance[key] = np.array(value)\n",
        "        yield instance\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Hl93fcLfmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_datetime(datetime_str):\n",
        "  return datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S').strftime('%Y%m%d%H%M%S')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKIfq7BAp_YP",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Implement skew detection pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0DSJSVaMlEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "_STATS_FILENAME = 'stats.pb'\n",
        "_ANOMALIES_FILENAME = 'anomalies.pbtxt'\n",
        "\n",
        "def run_pipeline(args):\n",
        "  \n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "  \n",
        "  query = generate_query(\n",
        "      args.request_response_log_table, args.model_name, args.model_version, \n",
        "      args.start_time, args.end_time)    \n",
        "\n",
        "  output_directory = os.path.join(\n",
        "      args.output_path, format_datetime(args.start_time)+\"-\"+format_datetime(args.end_time))\n",
        "  stats_output_path = os.path.join(output_directory, _STATS_FILENAME)\n",
        "  anomalies_output_path = os.path.join(output_directory, _ANOMALIES_FILENAME)\n",
        "  reference_schema = tfdv.load_schema_text(args.reference_schema_path)\n",
        "  baseline_stats = tfdv.load_statistics(args.baseline_stats_path)\n",
        "  \n",
        "  with beam.Pipeline(options=options) as pipeline:\n",
        "    \n",
        "    raw_examples = (pipeline\n",
        "                   | 'ReadBigQeuryData' >> beam.io.Read(beam.io.BigQuerySource(\n",
        "                       query=query, use_standard_sql=True)))\n",
        "    examples = (raw_examples\n",
        "                | 'JSONObjectInstancesToBeamExamples' >> beam.ParDo(\n",
        "                    JSONObjectCoder()))  \n",
        "    \n",
        "    stats = (examples\n",
        "             | 'BeamExamplesToArrow' >> tfdv.utils.batch_util.BatchExamplesToArrowRecordBatches() \n",
        "             | 'GenerateStatistics' >> tfdv.GenerateStatistics())\n",
        "        \n",
        "    _ = (stats       \n",
        "         | 'WriteStatsOutput' >> beam.io.WriteToTFRecord(\n",
        "             file_path_prefix=stats_output_path,\n",
        "             shard_name_template='',\n",
        "             coder=beam.coders.ProtoCoder(\n",
        "                 statistics_pb2.DatasetFeatureStatisticsList)))\n",
        "        \n",
        "    _ = (stats\n",
        "         | 'ValidateStatistics' >> beam.Map(\n",
        "             lambda new_stats: tfdv.validate_statistics(\n",
        "                 new_stats, schema=reference_schema, \n",
        "                 previous_statistics=baseline_stats, \n",
        "                 environment='SERVING'))\n",
        "         | 'WriteAnomaliesOutput' >> beam.io.textio.WriteToText(\n",
        "             file_path_prefix=anomalies_output_path,\n",
        "             shard_name_template='',\n",
        "             append_trailing_newlines=False))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3uoXDqYqDUP",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Set pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkb9k6dMlHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "request_response_log_table = \"{}.{}.{}\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME\n",
        ")\n",
        "\n",
        "start_time = '2020-05-23 10:30:00'\n",
        "end_time = '2020-05-23 11:30:00'\n",
        "\n",
        "output_dir = os.path.join(ARTIFACTS_DIR, 'outputs')\n",
        "\n",
        "args = {\n",
        "    'job_name': 'tfdv-skew-detection-{}'.format(datetime.utcnow().strftime('%Y%m%d-%H%M%S')),\n",
        "    'request_response_log_table': request_response_log_table,\n",
        "    'model_name': MODEL_NAME,\n",
        "    'model_version': MODEL_VERSION,\n",
        "    'start_time': start_time,\n",
        "    'end_time': end_time,\n",
        "    'reference_schema_path': REFERENCE_SCHEMA_FILE,\n",
        "    'baseline_stats_path': BASELINE_STATS_FILE,\n",
        "    'output_path': output_dir,\n",
        "    'project': PROJECT_ID,\n",
        "}\n",
        "\n",
        "args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Gn2vPSqKui",
        "colab_type": "text"
      },
      "source": [
        "### 3.4. Run pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CTETFFIIJh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r {output_dir}\n",
        "!mkdir {output_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3t4J8hOMTbr",
        "colab_type": "text"
      },
      "source": [
        "We will run the pipeline to create serving statistics and detect skews in the different days in the request-response BigQuery logs table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UObjxMm_r-H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"\"\"\n",
        "  SELECT DISTINCT FORMAT_TIMESTAMP(\"%Y-%m-%d\", time) AS date\n",
        "  FROM `{}`\n",
        "  WHERE model = '{}' AND model_version = '{}'\n",
        "  ORDER BY date\n",
        "\"\"\".format(request_response_log_table, MODEL_NAME, MODEL_VERSION)\n",
        "\n",
        "results = pd.io.gbq.read_gbq(\n",
        "    query, project_id=PROJECT_ID)\n",
        "dates = list(results.date)\n",
        "dates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qipMgmZdMlJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d in dates:\n",
        "  start_time = d + ' 00:00:00'\n",
        "  end_time = d + ' 23:59:59'\n",
        "  args['start_time'] = start_time\n",
        "  args['end_time'] = end_time\n",
        "  print(\"Running pipeline: {} - {}\".format(start_time, end_time))\n",
        "  run_pipeline(args)\n",
        "  print(\"Pipeline is done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0cK5GNvvcgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls {output_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcq7J6GpMlU9",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Analyzing Serving Data Statistics and Anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8e3dhmrfYAM",
        "colab_type": "text"
      },
      "source": [
        "Load Serving Statistics and Anomalies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCjE-F6JMpg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serving_stats = []\n",
        "serving_anomalies = []\n",
        "for directory in os.listdir(output_dir):\n",
        "  stats_path = os.path.join(output_dir, directory, _STATS_FILENAME)\n",
        "  stats = tfdv.load_statistics(stats_path)\n",
        "  serving_stats.append(stats)\n",
        "  print(\"Stats loaded: {}\".format(stats_path))\n",
        "\n",
        "  anomalies_path = os.path.join(output_dir, directory, _ANOMALIES_FILENAME)\n",
        "  anomalies = tfdv.load_anomalies_text(anomalies_path)\n",
        "  print(\"Anomalies loaded: {}\".format(anomalies_path))\n",
        "  serving_anomalies.append(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Meuxzarfg09",
        "colab_type": "text"
      },
      "source": [
        "## 1. Visualize Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0gtTgwnfg7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for stats in serving_stats:\n",
        "  tfdv.visualize_statistics(\n",
        "    baseline_stats, stats, 'baseline', 'current')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mesWXuttfgYs",
        "colab_type": "text"
      },
      "source": [
        "## 2. Display Anomalies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6kULTNZpD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for anomalies in serving_anomalies:\n",
        "  tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0cX1PbDAyBS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Analyze Statistics Change Over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBsYaI600bjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "feature_means = defaultdict(list)\n",
        "for stats in serving_stats:\n",
        "  for feature in stats.datasets[0].features:\n",
        "    if feature.type in [0, 1]:\n",
        "      mean = feature.num_stats.mean\n",
        "      feature_means[feature.path.step[0]].append(mean)\n",
        "\n",
        "feature_means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPwhPUECuqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "dataframe = pd.DataFrame(feature_means, index=dates)\n",
        "num_features = len(feature_means)\n",
        "ncolumns = 3\n",
        "nrows = int(num_features // ncolumns)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncolumns, figsize=(25, 30))\n",
        "for i, col in enumerate(dataframe.columns[:num_features]):\n",
        "  r = i // ncolumns\n",
        "  c = i % ncolumns\n",
        "  dataframe[col].plot.bar(ax=axes[r][c], title=col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa1qyi4kB4dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}