{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF Serving with AI Platform Prediction Custom Containers (Beta)\n",
    "\n",
    "This notebook demonstrates how to deploy a TensorFlow 2.x model using AI Platform Prediction Custom Containers (Alpha) and TensorFlow Serving.\n",
    "\n",
    "\n",
    "Although, this notebook uses the custom serving module developed in the `01-prepare-for-serving.ipynb` notebook, the discussed techniques can be applied to any TF 2.x model.\n",
    "\n",
    "For more information about the AI Platform Prediction Custom Containers feature refer to [TBD]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from google.auth.credentials import Credentials\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "from typing import List, Optional, Text, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "This notebook was tested on **AI Platform Notebooks** using the standard TF 2.2 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model store path\n",
    "\n",
    "Set the `SAVED_MODEL_PATH` to the GCS location of the `SavedModel` created in the `01-prepare-for-serving.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = 'gs://mlops-dev-workspace/models/resnet_serving'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a regional Artifact Registry\n",
    "\n",
    "\n",
    "A container image to be deployed to AI Platform Prediction must be stored in a regional [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registry/docs). Using an external registry like Docker Hub or a Container Registry is not supported. \n",
    "\n",
    "If you already have an existing Artifact Registry you can use,  skip to the **Push TF Serving container images to Artifact Registry** section.\n",
    "\n",
    "#### Create a regional Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "repository_name = 'aipp-images'\n",
    "region = 'us-central1'\n",
    "\n",
    "!gcloud beta artifacts repositories create {repository_name} \\\n",
    "--repository-format=docker \\\n",
    "--location={region}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing items under project mlops-dev-env, across all locations.\n",
      "\n",
      "Note: To perform actions on the Container Registry repositories listed below please use 'gcloud container images'.\n",
      "\n",
      "                                   ARTIFACT_REGISTRY\n",
      "REPOSITORY   FORMAT  DESCRIPTION  LOCATION     CREATE_TIME          UPDATE_TIME\n",
      "aipp-images  DOCKER               us-central1  2020-08-17T20:00:20  2020-08-17T20:34:36\n",
      "\n",
      "         CONTAINER_REGISTRY\n",
      "HOSTNAME           LOCATION\n",
      "gcr.io             us\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up authenticaton for Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\", \n",
      "    \"asia.gcr.io\": \"gcloud\", \n",
      "    \"staging-k8s.gcr.io\": \"gcloud\", \n",
      "    \"us.gcr.io\": \"gcloud\", \n",
      "    \"gcr.io\": \"gcloud\", \n",
      "    \"marketplace.gcr.io\": \"gcloud\", \n",
      "    \"eu.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "hostname = f'{region}-docker.pkg.dev'\n",
    "\n",
    "!gcloud beta auth configure-docker {hostname} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push TF Serving container images to the Artifact Registry\n",
    "\n",
    "If you have skipped the previous steps and are re-using the existing Artifact Registry set the `region` and `repository_name` variables in the below cell with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regions='your-region'\n",
    "#repository_name='your-registryname'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , project_id = google.auth.default()\n",
    "\n",
    "#cpu_image_name = f'{region}-docker.pkg.dev/{project_id}/{repository_name}/tensorflow_serving:latest-cpu'\n",
    "#gpu_image_name = f'{region}-docker.pkg.dev/{project_id}/{repository_name}/tensorflow_serving:latest-gpu'\n",
    "cpu_image_name = f'gcr.io/{project_id}/tensorflow_serving:latest-cpu'\n",
    "gpu_image_name = f'gcr.io/{project_id}/tensorflow_serving:latest-gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from tensorflow/serving\n",
      "Digest: sha256:a94b7e3b0e825350675e83b0c2f2fc28f34be358c34e4126a1d828de899ec44f\n",
      "Status: Image is up to date for tensorflow/serving:latest\n",
      "docker.io/tensorflow/serving:latest\n",
      "latest-gpu: Pulling from tensorflow/serving\n",
      "Digest: sha256:9f2154baa458bf7b523d5f3c9f545056ed14d75ceac00742d1903d37d80393e9\n",
      "Status: Image is up to date for tensorflow/serving:latest-gpu\n",
      "docker.io/tensorflow/serving:latest-gpu\n"
     ]
    }
   ],
   "source": [
    "!docker pull tensorflow/serving:latest\n",
    "!docker pull tensorflow/serving:latest-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag tensorflow/serving:latest {cpu_image_name}\n",
    "!docker tag tensorflow/serving:latest-gpu {gpu_image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/mlops-dev-env/tensorflow_serving]\n",
      "\n",
      "\u001b[1Bac716820: Preparing \n",
      "\u001b[1Bbd8c4bd3: Preparing \n",
      "\u001b[1Be785c230: Preparing \n",
      "\u001b[1Ba73fd165: Preparing \n",
      "\u001b[1Bf9a74649: Preparing \n",
      "\u001b[1Bda143c91: Preparing \n",
      "\u001b[1B287e1f04: Preparing \n",
      "\u001b[2B287e1f04: Layer already exists \u001b[8A\u001b[2K\u001b[3A\u001b[2Klatest-cpu: digest: sha256:a94b7e3b0e825350675e83b0c2f2fc28f34be358c34e4126a1d828de899ec44f size: 1989\n",
      "The push refers to repository [gcr.io/mlops-dev-env/tensorflow_serving]\n",
      "\n",
      "\u001b[1B41b4553f: Preparing \n",
      "\u001b[1B6ab262b7: Preparing \n",
      "\u001b[1Bfdb5f1f9: Preparing \n",
      "\u001b[1B64ade40f: Preparing \n",
      "\u001b[1B0889ee68: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[1Bb5a53aac: Preparing \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[1B7c529ced: Layer already exists \u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2Klatest-gpu: digest: sha256:9f2154baa458bf7b523d5f3c9f545056ed14d75ceac00742d1903d37d80393e9 size: 2835\n"
     ]
    }
   ],
   "source": [
    "!docker push {cpu_image_name}\n",
    "!docker push {gpu_image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing items under project mlops-dev-env, location us-central1, repository aipp-images.\n",
      "\n",
      "IMAGE                                                                    DIGEST                                                                   CREATE_TIME          UPDATE_TIME\n",
      "us-central1-docker.pkg.dev/mlops-dev-env/aipp-images/tensorflow_serving  sha256:9f2154baa458bf7b523d5f3c9f545056ed14d75ceac00742d1903d37d80393e9  2020-08-17T20:34:36  2020-08-17T20:34:36\n",
      "us-central1-docker.pkg.dev/mlops-dev-env/aipp-images/tensorflow_serving  sha256:a94b7e3b0e825350675e83b0c2f2fc28f34be358c34e4126a1d828de899ec44f  2020-08-17T20:32:43  2020-08-17T20:32:43\n"
     ]
    }
   ],
   "source": [
    "repository_id = f'{region}-docker.pkg.dev/{project_id}/{repository_name}'\n",
    "\n",
    "!gcloud beta artifacts docker images list {repository_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a model version\n",
    "\n",
    "We will use AI Platform Prediction REST API to create model and model version resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an authorized session \n",
    "\n",
    "The AI Platform Prediction REST API calls must be authorized through OAuth 2. We will use the `google.auth.transport.requests.AuthorizedSession` client to transparently handle OAuth authorization flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_endpoint = 'https://alpha-ml.googleapis.com'\n",
    "\n",
    "credentials, project_ = google.auth.default()\n",
    "authed_session = AuthorizedSession(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all models in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'projects/mlops-dev-env/models/ResNet101',\n",
       "   'regions': ['us-central1'],\n",
       "   'etag': 'S7FgvSfwfUY='}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 409,\n",
       "  'message': 'Field: model.name Error: A model with the same name already exists.',\n",
       "  'status': 'ALREADY_EXISTS',\n",
       "  'details': [{'@type': 'type.googleapis.com/google.rpc.BadRequest',\n",
       "    'fieldViolations': [{'field': 'model.name',\n",
       "      'description': 'A model with the same name already exists.'}]}]}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ResNet101'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/'\n",
    "\n",
    "request_body = {\n",
    "    \"name\": model_name\n",
    "}\n",
    "\n",
    "response = authed_session.post(url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model's info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/models/ResNet101',\n",
       " 'regions': ['us-central1'],\n",
       " 'etag': 'S7FgvSfwfUY='}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version\n",
    "\n",
    "When deploying a custom container to AI Platform Prediction you need to configure two groups of settings. The first group defines the configuration of the AI Platform Prediction service that hosts your container. For example, a node type, manual or autoscaling parameters, an accelerator configuration, etc. The second group are the settings specific to a given container. \n",
    "\n",
    "Refer to [TBD]() for a detailed discussion of the available service settings.\n",
    "\n",
    "There are three ways of passing configuration settings to a container:\n",
    "* the settings can be embedded in a custom container image\n",
    "* you can pass the settings as command line arguments, or \n",
    "* you can supply a configuration file. \n",
    "\n",
    "In the first method, the configuration settings are supplied  at the time the container container is built. The other two methods allow you to set the settings  at the deployment time. \n",
    "\n",
    "Some model servers commonly used in AI Platform Prediction custom containers, including TF Serving used in this notebook, also expose a management API that allows you to change configurations after the server has been deployed. Configuring the server through the management API is currently not supported due to the constraints of the REST interface exposed by AI Platform Prediction.\n",
    "\n",
    "\n",
    "Supplying configuration settings through a command line interface is straightforward. The AI Platform Prediction REST API utilizes JSON to encode requests and responses. You can provide the command line arguments as the `args` key of the JSON `container` object in the create model version request body.\n",
    "\n",
    "\n",
    "Passing a config file to a container hosted in AI Platform Prediction is a little bit trickier. The container runs in an isolated environment and does not have access to resources (including Cloud Storage) outside of this environment. To pass file based assets (including a config file) to the container you need to stage them in the GCS deployment location. The GCS deployment location - set through the `deployment_uri` field of the REST API request body - is copied to the isolated environment by the create model version request. The url to the location of the copy in the isolated environment is exposed through the `AIP_STORAGE_URI` environment variable. \n",
    "\n",
    "In the following example you will use both the command line arguments and the configuration file to configure the TF Serving model server. Most of the configurations will be passed as command line arguments. The [server side batching]()(https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration) parameters will be passed as a config file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the config file with batching settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batching_config = 'batching.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batching.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {batching_config}\n",
    "\n",
    "max_batch_size { value: 128 }\n",
    "batch_timeout_micros { value: 150000 }\n",
    "max_enqueued_batches { value: 16 }\n",
    "num_batch_threads { value: 8 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the batch config file to the staging location in GCS\n",
    "\n",
    "You are going to use the folder where the custom ResNet10 model was saved as the staging location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://batching.pbtxt [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  136.0 B/  136.0 B]                                                \n",
      "Operation completed over 1 objects/136.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {batching_config} {SAVED_MODEL_PATH}/{batching_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_batch_size { value: 128 }\n",
      "batch_timeout_micros { value: 150000 }\n",
      "max_enqueued_batches { value: 16 }\n",
      "num_batch_threads { value: 8 }\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat {SAVED_MODEL_PATH}/batching.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mlops-dev-workspace/models/resnet_serving/batching.pbtxt\n",
      "gs://mlops-dev-workspace/models/resnet_serving/1/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {SAVED_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/operations/create_ResNet101_batching_150-1597710897245',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.ml.v1.OperationMetadata',\n",
       "  'createTime': '2020-08-18T00:34:58Z',\n",
       "  'operationType': 'CREATE_VERSION',\n",
       "  'modelName': 'projects/mlops-dev-env/models/ResNet101',\n",
       "  'version': {'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       "   'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       "   'createTime': '2020-08-18T00:34:57Z',\n",
       "   'etag': 'GM7PM2uxJB0=',\n",
       "   'machineType': 'n1-standard-8',\n",
       "   'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       "   'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "    'args': ['--rest_api_port=8080',\n",
       "     '--model_name=ResNet101',\n",
       "     '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "     '--enable_batching',\n",
       "     '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       "   'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "    'health': '/v1/models/ResNet101'}}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_name = 'batching_150'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions'\n",
    "\n",
    "request_body = {\n",
    "    # Service settings\n",
    "    \"name\": version_name,\n",
    "    \"deployment_uri\": SAVED_MODEL_PATH,\n",
    "    \"machine_type\": 'n1-standard-8',\n",
    "    \"accelerator_config\": {\n",
    "        \"count\": 1,\n",
    "        \"type\": 'NVIDIA_TESLA_P4'},\n",
    "    \"routes\": {\n",
    "        \"predict\": f\"/v1/models/{model_name}:predict\",\n",
    "        \"health\": f\"/v1/models/{model_name}\"},\n",
    "    \n",
    "    # Container settings\n",
    "    \"container\": {\n",
    "        \"image\": gpu_image_name,\n",
    "        \"args\": [\n",
    "            \"--rest_api_port=8080\",\n",
    "            f\"--model_name={model_name}\",\n",
    "            \"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "            \"--enable_batching\",\n",
    "            \"--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt\"]}\n",
    "}\n",
    "            \n",
    "response = authed_session.post(url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the deployment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       " 'isDefault': True,\n",
       " 'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       " 'createTime': '2020-08-18T00:34:57Z',\n",
       " 'state': 'READY',\n",
       " 'etag': 'Kg+4YsdtKYY=',\n",
       " 'machineType': 'n1-standard-8',\n",
       " 'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       " 'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "  'args': ['--rest_api_port=8080',\n",
       "   '--model_name=ResNet101',\n",
       "   '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "   '--enable_batching',\n",
       "   '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       " 'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "  'health': '/v1/models/ResNet101'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions/{version_name}'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run inference by invoking the TF Serving `Predict` API.\n",
    "\n",
    "Refer to the [TF Serving REST API Reference](https://www.tensorflow.org/tfx/serving/api_rest) for more information about the API format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'test_images'\n",
    "raw_images = [tf.io.read_file(os.path.join(image_folder, image_path)).numpy()\n",
    "         for image_path in os.listdir(image_folder)]\n",
    "\n",
    "encoded_images = [{'b64': base64.b64encode(image).decode('utf-8')} for image in raw_images]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the `predict` endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'labels': ['military uniform',\n",
       "    'suit',\n",
       "    'Windsor tie',\n",
       "    'pickelhaube',\n",
       "    'bow tie'],\n",
       "   'probabilities': [0.940013826,\n",
       "    0.0485324822,\n",
       "    0.00640657172,\n",
       "    0.00201301626,\n",
       "    0.000604337547]},\n",
       "  {'labels': ['Egyptian cat', 'tiger cat', 'tabby', 'lynx', 'Siamese cat'],\n",
       "   'probabilities': [0.827052057,\n",
       "    0.131283119,\n",
       "    0.0410555713,\n",
       "    0.0005708182,\n",
       "    1.89249167e-05]}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions/{version_name}:predict'\n",
    "signature = 'serving_preprocess'\n",
    "\n",
    "request_body = {\n",
    "            'signature_name': signature,\n",
    "            'instances': encoded_images\n",
    "        }\n",
    "            \n",
    "response = authed_session.post(url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model version and model resources\n",
    "#### List model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'versions': [{'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       "   'isDefault': True,\n",
       "   'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       "   'createTime': '2020-08-18T00:34:57Z',\n",
       "   'lastUseTime': '2020-08-18T00:42:19Z',\n",
       "   'state': 'READY',\n",
       "   'etag': 'Kg+4YsdtKYY=',\n",
       "   'machineType': 'n1-standard-8',\n",
       "   'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       "   'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "    'args': ['--rest_api_port=8080',\n",
       "     '--model_name=ResNet101',\n",
       "     '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "     '--enable_batching',\n",
       "     '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       "   'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "    'health': '/v1/models/ResNet101'}}]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ResNet101'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/operations/delete_ResNet101_batching_150-1597711347712',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.ml.v1.OperationMetadata',\n",
       "  'createTime': '2020-08-18T00:42:27Z',\n",
       "  'operationType': 'DELETE_VERSION',\n",
       "  'modelName': 'projects/mlops-dev-env/models/ResNet101',\n",
       "  'version': {'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       "   'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       "   'createTime': '2020-08-18T00:34:57Z',\n",
       "   'state': 'READY',\n",
       "   'etag': 'Kg+4YsdtKYY=',\n",
       "   'machineType': 'n1-standard-8',\n",
       "   'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       "   'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "    'args': ['--rest_api_port=8080',\n",
       "     '--model_name=ResNet101',\n",
       "     '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "     '--enable_batching',\n",
       "     '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       "   'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "    'health': '/v1/models/ResNet101'}}}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_name = 'batching_150'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions/{version_name}'\n",
    "\n",
    "response = authed_session.delete(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}'\n",
    "\n",
    "response = authed_session.delete(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Walk through the `aipp_deploy.ipynb` notebook to learn how to deploy the custom serving module created in this notebook to **AI Platform Prediction** using TF Serving container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
