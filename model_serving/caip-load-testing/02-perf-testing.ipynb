{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform Prediction Load Testing using Locust\n",
    "\n",
    "This notebook demonstrates how to perform load testing of AI Platform Prediction using [Locust](https://locust.io). The notebook covers the following steps:\n",
    "1. Creating custom log-based metrics in Cloud Monitoring for the Locust logs\n",
    "2. Creating a Cloud Monitoring Dashboard to display the AI Platform and custom metrics\n",
    "3. Deploying Locust to GKE clustor\n",
    "4. Configuring Lucost test\n",
    "5. Runing the Lucost load test\n",
    "6. Retrieving and consolidating test results\n",
    "\n",
    "The diagram below depicts the load testing environment utilized in this example.\n",
    "\n",
    "![Test harness](images/locust-caipp.png)\n",
    "\n",
    "\n",
    "In the environment, Locust is run in a distributed mode on a GKE cluster. Locust's master and workers are deployed to the cluster as Kubernetes [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) using a custom docker image dervied from the baseline [locustio/locust](https://hub.docker.com/r/locustio/locust) image. The custom image incorporates the [locustfile](locust/locust-image/tasks.py) script and its dependencies.\n",
    "\n",
    "The script simulates calls to the `predict` method of the  AI Platform Prediction REST endpoint. The parameters of the method (project, model, model version, and signature) and test instances passed in the method's body are retrieved from a Cloud Storage location at the start of each test.\n",
    "\n",
    "In addition to simulating requests, the script logs test statistics managed by the Locust master to [Cloud Logging](https://cloud.google.com/logging). \n",
    "The log entries created by the script are used to define a set of [Log-based metrics](https://cloud.google.com/logging/docs/logs-based-metrics) that complement standard [AI Platform Prediction metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-ml). \n",
    "\n",
    "Load tests can be configured, started, and stoped using **Locust's** [web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface). The **Locust's** web interface is enabled on the Locust master and exposed through a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) configured as an external load balancer.\n",
    "\n",
    "The progress of the tests can be monitored using [Locust's web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface) and/or a Cloud Monitoring [dashboard](https://cloud.google.com/monitoring/dashboards). The advantage of a Cloud Monitoring dashboard is that it can combine AI Platform Prediction metrics with custom Locust log-based metrics. You can find an example dashboard template in the `dashboard_template` folder.\n",
    "\n",
    "After a test completes, the test's metrics are retrieved from Cloud Monitoring and consolidated into a Pandas dataframe to facilitate comprehensive post-mortem analysis.  The `04-analyze-test.ipynb` notebook demonstrates how to use Pandas and Matplotlib to analyze and interpret the test runs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This notebook was tested on AI Platform Notebooks using the standard TF 2.2 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U locust google-cloud-monitoring google-cloud-logging google-cloud-monitoring-dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPICallError \n",
    "\n",
    "from google.cloud import logging_v2\n",
    "from google.cloud.logging_v2 import MetricsServiceV2Client\n",
    "from google.cloud.logging_v2 import LoggingServiceV2Client\n",
    "\n",
    "from google.cloud.monitoring_dashboard.v1.types import Dashboard\n",
    "from google.cloud.monitoring_dashboard.v1 import DashboardsServiceClient\n",
    "from google.cloud.monitoring_v3 import MetricServiceClient\n",
    "from google.cloud.monitoring_v3.query import Query\n",
    "from google.cloud.monitoring_v3.types import TimeInterval\n",
    "\n",
    "from google.protobuf.json_format import ParseDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GCP environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-google-project-id]' # Set your project Id\n",
    "BUCKET = '[your-bucket-name]' # Set your bucket name Id\n",
    "REGION = '[your-region]'  # Set your region for deploying the model\n",
    "GKE_CLUSTER_NAME = '[your-gke-cluster-name]' # Set the GKE cluster name\n",
    "GKE_CLUSTER_ZONE = '[your-gke-cluster-zone]'\n",
    "MODEL_NAME = 'resnet_classifier'\n",
    "MODEL_VERSION = 'v1'\n",
    "IMAGES_FOLDER = 'test/test_data'\n",
    "GCS_LOCUST_TEST_CONFIG_DIR = 'gs://{}/locust-test'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Custom Log-based Metrics in Cloud Monitoring\n",
    "\n",
    "In this section of the notebook you will use the [Python Cloud Logging client library](https://googleapis.dev/python/logging/latest/v2.html) to create a set of custom log-based metrics. The metrics are based on the log entries generated by the example locustfile script. The script writes the log entries into the *Cloud Logging* log named `locust`.\n",
    "\n",
    "Each log entry includes a set of key value pairs encoded as the JSON payload type. The metrics are based on the subset of keys from the log entry.\n",
    "\n",
    "Key | Value\n",
    "----|------\n",
    "test_id | An ID of a test\n",
    "model | An AI Platform Prediction Model name\n",
    "model_version | An AI Platform Prediction Model version\n",
    "latency | A 95 percentile response time, which is calculated over a 10 sliding second window\n",
    "num_requests | A total number of requests since the test started\n",
    "num_failures | A total number of requests since the test started\n",
    "user_count | A number of simulated users \n",
    "rps | A current requests per second\n",
    "\n",
    "\n",
    "Refer to the [Cloud Logging API reference](https://googleapis.dev/python/logging/latest/v2.html) for more information about the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define a helper function that creates a custom log metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_locust_metric(\n",
    "    metric_name:str,\n",
    "    log_path:str, \n",
    "    value_field:str,  \n",
    "    bucket_bounds:List[int]):\n",
    "    \n",
    "    metric_path = logging_client.metric_path(project_id, metric_name)\n",
    "    log_entry_filter = 'resource.type=global AND logName={}'.format(log_path)\n",
    "    \n",
    "    metric_descriptor = {\n",
    "        'metric_kind': 'DELTA',\n",
    "        'value_type': 'DISTRIBUTION',\n",
    "        'labels': [\n",
    "            {\n",
    "                'key': 'test_id',\n",
    "                'value_type': 'STRING'\n",
    "            },\n",
    "            {\n",
    "                'key': 'signature',\n",
    "                'value_type': 'STRING'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    bucket_options = {\n",
    "        'explicit_buckets': {\n",
    "            'bounds': bucket_bounds\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    value_extractor = 'EXTRACT(jsonPayload.{})'.format(value_field)\n",
    "    label_extractors = {\n",
    "        'test_id': 'EXTRACT(jsonPayload.test_id)',\n",
    "        'signature': 'EXTRACT(jsonPayload.signature)'\n",
    "    }\n",
    "    \n",
    "    metric = logging_v2.types.LogMetric(\n",
    "        name=metric_name,\n",
    "        filter=log_entry_filter,\n",
    "        value_extractor=value_extractor,\n",
    "        bucket_options=bucket_options,\n",
    "        label_extractors=label_extractors,\n",
    "        metric_descriptor=metric_descriptor,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        logging_client.get_log_metric(metric_path)\n",
    "        print('Metric: {} already exists'.format(metric_path))\n",
    "    except:\n",
    "        logging_client.create_log_metric(parent, metric)\n",
    "        print('Created metric {}'.format(metric_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create a logging client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = 'locust'\n",
    "\n",
    "creds , project_id = google.auth.default()\n",
    "logging_client = MetricsServiceV2Client(credentials=creds)\n",
    "\n",
    "parent = logging_client.project_path(project_id)\n",
    "log_path = LoggingServiceV2Client.log_path(project_id, log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Create metrics to track Locust logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user count metric\n",
    "metric_name = 'locust_users'\n",
    "value_field = 'user_count'\n",
    "bucket_bounds = [1, 16, 32, 64, 128]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)\n",
    "\n",
    "# latency metric\n",
    "metric_name = 'locust_latency'\n",
    "value_field = 'latency'\n",
    "bucket_bounds = [1, 50, 100, 200, 500]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)\n",
    "\n",
    "# failure count metric\n",
    "metric_name = 'num_failures'\n",
    "value_field = 'num_failures'\n",
    "bucket_bounds = [1, 1000]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)\n",
    "\n",
    "# request count metric\n",
    "metric_name = 'num_requests'\n",
    "value_field = 'num_requests'\n",
    "bucket_bounds = [1, 1000]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. List metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = logging_client.list_log_metrics(parent)\n",
    "\n",
    "if not list(metrics):\n",
    "    print(\"There are not any log based metrics defined in the the project\")\n",
    "else:\n",
    "    for element in logging_client.list_log_metrics(parent):\n",
    "        print(element.metric_descriptor.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Cloud Monitoring dashboard\n",
    "\n",
    "The`dashboard_template` folder contains an example monitoring dashboard template that combines standard AI Platform Prediction metrics with log-based metrics defined in the previous steps. You can use [Python Client for Cloud Monitoring Dashboards API](https://googleapis.dev/python/monitoring-dashboards/latest/index.html) to create a dashboard based on the template.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load the dashboard template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_service_client = DashboardsServiceClient(credentials=creds)\n",
    "parent = 'projects/{}'.format(project_id)\n",
    "\n",
    "dashboard_template_file = 'monitoring-template.json'\n",
    "with open(dashboard_template_file) as f:\n",
    "    dashboard_template = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create the dashboard in Cloud Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_proto = Dashboard()\n",
    "dashboard_proto = ParseDict(dashboard_template, dashboard_proto)\n",
    "dashboard = dashboard_service_client.create_dashboard(parent, dashboard_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dashboard in dashboard_service_client.list_dashboards(parent):\n",
    "    print('Dashboard name: {}, Dashboard ID: {}'.format(dashboard.display_name, dashboard.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying Locust to a GKE cluster\n",
    "\n",
    "Before proceeding, you need access to a GKE cluster. The described deployment process can deploy Locust to any GKE cluster as long as there are enough compute resources to support your Locust configuration. The default configuration follows the Locust's best practices and requests one processor core and 4Gi of memory for the Locust master and one processor core and 2Gi of memory for each Locust worker. As you run your tests, it is important to monitor the the master and the workers for resource utilization and fine tune the allocated resources as required.\n",
    "\n",
    "The deployment process has been streamlined using [Kustomize](https://kustomize.io/). As described in the following steps, you can fine tune the baseline configuration by modifying the default `kustomization.yaml` and `patch.yaml` files in the `locust/manifests` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Install Kustomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\n",
    "!sudo mv kustomize /usr/local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Set credentials to access your GKE cluster\n",
    "\n",
    "Use, the `gcloud` command to set credentials to your GKE cluster. Make sure to update the `cluster_name` and `cluster_zone` variables with values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud container clusters get-credentials {GKE_CLUSTER_NAME} --zone {GKE_CLUSTER_ZONE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Build the Locust image\n",
    "\n",
    "The first step is to build a docker image that will be used to deploy Locust master and worker pods. The image is derived from the [baseline locust.io image](https://hub.docker.com/r/locustio/locust) and embeds the locustfile and the files's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = 'gcr.io/{}/locust'.format(project_id)\n",
    "\n",
    "!gcloud builds submit --tag {image_uri} locust/locust-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Update the manifests\n",
    "\n",
    "Before proceeding with deployment, you need to update the default manifests. The manifests are located in the `locust/manifests` folder. You will modify two files: `kustomization.yaml` and `patch.yaml`.\n",
    "\n",
    "#### - Set the name of the custom Locust image\n",
    "\n",
    "You need to update the `kustomization.yaml` file with a reference to the custom image your created in the previous step. \n",
    "\n",
    "Update the `newName` field in the `images` section of the `kustomization.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '22,26p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Set the number of worker pods \n",
    "\n",
    "The default configuration deploys 32 worker pods. If you want to change it, modify the `count` field in the `replicas` section of the `kustomization.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '26,30p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Set the GCS bucket for the test configuration and data files\n",
    "\n",
    "As described in more detail in the later section of the notebook, every time you start a test, the locustfile script attempts to retrieve a test configuration and test data files from a GCS location. You need to configure the name of the GCS bucket hosting the files and the name of the files in `kustomization.yaml`.\n",
    "\n",
    "Modify the `configMapGenerator` section of the file. Specifically, set the `LOCUST_TEST_BUCKET`, `LOCUST_TEST_CONFIG`, and `LOCUST_TEST_DATA` literals to the GCS bucket name, the test config file name, and the test data config file name respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '35,52p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Modify the node pool that hosts the Locust master and workers\n",
    "\n",
    "By default, master and worker pods are deployed to the `default-pool` node pool. If you want to change it (recommended), update the name of the node pool in the `patch.yaml` file. The name of the node pool is a value of the `values` field in the `matchExpressions` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 17 locust/manifests/patch.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Deploy Locust\n",
    "\n",
    "You are now ready to deploy Locust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kustomize build locust/manifests |kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure a Locust test\n",
    "\n",
    "At the start of each test, the locustfile script attempts to retrieve test data and a test configuration from a GCS location. Both the test data and the test configuration are formated as JSON. \n",
    "\n",
    "The test data is an array of JSON objects, where each object includes a list of instances and a model signature. If the array contains more than one object, Locust users will randomly pick a list of instances and an associated signature with each call to the `predict` method of the AI Platform Prediction endpoint.\n",
    "\n",
    "The test configuration is a JSON object with a project id, model name, model version, and a test id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Prepare test data\n",
    "\n",
    "In this example we are using the  **ResNet101** model prepared and deployed in the `01-prepare-and-deply.ipynb` notebook. We will prepare the instances for the `serving_preprocess` signature of the model using a couple of JPEG images from the `test/test_data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for image_name in os.listdir('test/test_data'):\n",
    "    with open(os.path.join(IMAGES_FOLDER, image_name), 'rb') as f:\n",
    "        images.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_instance = [{'b64': base64.b64encode(images[0]).decode('utf-8')}]\n",
    "two_instances = [{'b64': base64.b64encode(image).decode('utf-8')} for image in images] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "        {\n",
    "            'signature': 'serving_preprocess',\n",
    "            'instances': single_instance\n",
    "        },\n",
    "        {\n",
    "            'signature': 'serving_preprocess',\n",
    "            'instances': two_instances\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_local_file = 'test/test-payload.json'\n",
    "\n",
    "with open (test_data_local_file, 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "    \n",
    "!gsutil cp {test_data_local_file} {GCS_LOCUST_TEST_CONFIG_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prepare test config\n",
    "\n",
    "Make sure to update the below mapping with the values representing your environment. The `test_id` is an arbitrary value that is used to match the custom log-based metrics records with a given test run. Use a different value anytime you start a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "test_config = {\n",
    "    'test_id': 'test-{}'.format(datetime.now().strftime('%Y%m%d-%H%M%S')),\n",
    "    'project_id': PROJECT_ID,\n",
    "    'model': MODEL_NAME,\n",
    "    'version': MODEL_VERSION\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config_local_file = 'test-config.json'\n",
    "\n",
    "with open (test_config_local_file, 'w') as f:\n",
    "    json.dump(test_config, f)\n",
    "\n",
    "!gsutil cp {test_config_local_file} {GCS_LOCUST_TEST_CONFIG_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {GCS_LOCUST_TEST_CONFIG_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Locust load Tests\n",
    "\n",
    "Load tests can be configured, started, monitored and stopped using using Locust's [web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface). \n",
    "\n",
    "In our deployment, the web interface is exposed by an external load balancer. You can access the interface using the following URL:\n",
    "\n",
    "```\n",
    "http://[EXTERNAL-IP]:8089\n",
    "```\n",
    "\n",
    "where `[EXTERNAL-IP]` can be retrieved by the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service locust-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try using the following parameter configurations:\n",
    "1. Number of total users to simulate: 152\n",
    "2. Hatch rate: 1\n",
    "3. Host: ml.googleapis.com\n",
    "4. Number of users to increase by step: 8\n",
    "5. Step duration: 1m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieve and consolidate test results\n",
    "\n",
    "Locust's web interface along with a Cloud Monitoring dashboard provide a cursory view into performance of a tested AI Platform Prediction model version. A more thorough analysis can be performed by consolidating metrics collected during a test and using data analytics and visualization tools.\n",
    "\n",
    "In this section, you will retrieve the metrics captured in Cloud Monitoring and consolidate them into a single Pandas dataframe. The `04-analyze-test-results.ipynb` notebook demonstrates how to analyze the consolidated results using Pandas and Matplotlib.\n",
    "\n",
    "You will use the Python Cloud Monitoring client library. Refer to the [Cloud Monitoring API reference](https://googleapis.dev/python/monitoring/latest/gapic/v3/api.html) for more information about the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 List available AI Platform Prediction metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds , project_id = google.auth.default()\n",
    "client = MetricServiceClient(credentials=creds)\n",
    "\n",
    "project_path = client.project_path(project_id)\n",
    "filter = 'metric.type=starts_with(\"ml.googleapis.com/prediction\")'\n",
    "\n",
    "for descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n",
    "    print(descriptor.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. List custom log based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = 'metric.type=starts_with(\"logging.googleapis.com/user\")'\n",
    "\n",
    "for descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n",
    "    print(descriptor.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Retrieve test metrics\n",
    "\n",
    "Define a helper function that retrieves test metrics from Cloud Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_metrics(client, project_id, start_time, end_time, model, model_version, test_id, log_name):\n",
    "    \"\"\"\n",
    "    Retrieves test metrics from Cloud Monitoring.\n",
    "    \"\"\"\n",
    "    def _get_aipp_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves a specified AIPP metric.\n",
    "        \"\"\"\n",
    "        query = Query(client, project_id, metric_type=metric_type)\n",
    "        query = query.select_interval(end_time, start_time)\n",
    "        query = query.select_resources(model_id=model)\n",
    "        query = query.select_resources(version_id=model_version)\n",
    "        \n",
    "        if metric_name:\n",
    "            labels = ['metric'] + labels \n",
    "        df = query.as_dataframe(labels=labels)\n",
    "        \n",
    "        if not df.empty:\n",
    "            if metric_name:\n",
    "                df.columns.set_levels([metric_name], level=0, inplace=True)\n",
    "            df = df.set_index(df.index.round('T'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_locust_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves a specified custom log-based metric.\n",
    "        \"\"\"\n",
    "        query = Query(client, project_id, metric_type=metric_type)\n",
    "        query = query.select_interval(end_time, start_time)\n",
    "        query = query.select_metrics(log=log_name)\n",
    "        query = query.select_metrics(test_id=test_id)\n",
    "        \n",
    "        if metric_name:\n",
    "            labels = ['metric'] + labels \n",
    "        df = query.as_dataframe(labels=labels)\n",
    "        \n",
    "        if not df.empty:    \n",
    "            if metric_name:\n",
    "                df.columns.set_levels([metric_name], level=0, inplace=True)\n",
    "            df = df.apply(lambda row: [metric.mean for metric in row])\n",
    "            df = df.set_index(df.index.round('T'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Retrieve GPU duty cycle\n",
    "    metric_type = 'ml.googleapis.com/prediction/online/accelerator/duty_cycle'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'duty_cycle')\n",
    "    df = metric\n",
    "\n",
    "    # Retrieve CPU utilization\n",
    "    metric_type = 'ml.googleapis.com/prediction/online/cpu/utilization'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'cpu_utilization')\n",
    "    if not metric.empty:\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve prediction count\n",
    "    metric_type = 'ml.googleapis.com/prediction/prediction_count'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'prediction_count')\n",
    "    if not metric.empty:\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve responses per second\n",
    "    metric_type = 'ml.googleapis.com/prediction/response_count'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'response_rate')\n",
    "    if not metric.empty:\n",
    "        metric = (metric/60).round(2)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve backend latencies\n",
    "    metric_type = 'ml.googleapis.com/prediction/latencies'\n",
    "    metric = _get_aipp_metric(metric_type, ['latency_type', 'replica_id', 'signature'])\n",
    "    if not metric.empty:\n",
    "        metric = metric.apply(lambda row: [round(latency.mean/1000,1) for latency in row])\n",
    "        metric.columns.set_names(['metric', 'replica_id', 'signature'], inplace=True)\n",
    "        level_values = ['Latency: ' + value for value in metric.columns.get_level_values(level=0)]\n",
    "        metric.columns.set_levels(level_values, level=0, inplace=True)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust latency\n",
    "    metric_type = 'logging.googleapis.com/user/locust_latency'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Latency: client')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round(2).replace([0], np.nan)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust user count\n",
    "    metric_type = 'logging.googleapis.com/user/locust_users'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'User count')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust num_failures\n",
    "    metric_type = 'logging.googleapis.com/user/num_failures'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of failures')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust num_failures\n",
    "    metric_type = 'logging.googleapis.com/user/num_requests'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of requests')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the below variables with the values used to configure the test whose metrics you want to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ResNet1'\n",
    "model_version = 'batching_100'\n",
    "log_name = 'locust'\n",
    "test_id = 'test-2-2020-08-13'\n",
    "test_start_time = datetime.datetime.fromisoformat('2020-08-13T12:50:00-07:00')\n",
    "test_end_time = datetime.datetime.fromisoformat('2020-08-13T14:20:00-07:00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = retrieve_metrics(client, project_id, test_start_time, test_end_time, model, model_version, test_id, log_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved dataframe uses [hierarchical indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) for column names. The reason is that some metrics contain multiple time series. For example, the GPU `duty_cycle` metric includes a time series of measures per each GPU used in the deployment (denoted as `replica_id`). The top level of the column index is a metric name. The second level is a `replica_id`. The third level is a `signature` of a model.\n",
    "\n",
    "All metrics are aligned on the same timeline. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Serialize the metrics dataframe\n",
    "\n",
    "The consolidated metrics can be saved for a later analysis by saving the dataframe in the Python `pickle` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'test_results/{}.gzip'.format(test_id)\n",
    "\n",
    "df.to_pickle(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The `03-analyze-tests.ipynb` notebook demonstrates how to use Pandas and Matplotlib to perform a detailed analysis of the load testing runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Locust deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kustomize build locust/manifests | kubectl delete -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the log based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds , project_id = google.auth.default()\n",
    "\n",
    "logging_client = MetricsServiceV2Client(credentials=creds)\n",
    "parent = logging_client.project_path(project_id)\n",
    "\n",
    "for element in logging_client.list_log_metrics(parent):\n",
    "    metric_path = logging_client.metric_path(project_id, element.name)\n",
    "    logging_client.delete_log_metric(metric_path)\n",
    "    print(\"Deleted metric: \", metric_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dasboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_service_client = DashboardsServiceClient(credentials=creds)\n",
    "parent = 'projects/{}'.format(project_id)\n",
    "\n",
    "dashboard_service_client.delete_dashboard(parent, dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
