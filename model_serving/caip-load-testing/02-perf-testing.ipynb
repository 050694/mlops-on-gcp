{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform Prediction Load Testing using Locust\n",
    "\n",
    "This notebook demonstrates how to perform load testing of AI Platform Prediction using [Locust](https://locust.io). \n",
    "\n",
    "\n",
    "### Load testing environment\n",
    "\n",
    "The diagram below depicts the load testing environment utilized in this example.\n",
    "\n",
    "![Test harness](images/locust-caipp.png)\n",
    "\n",
    "\n",
    "In the environment, Locust is run in a distributed mode on a GKE cluster. Locust's master and workers are deployed to the cluster as Kubernetes [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) using a custom docker image dervied from the baseline [locustio/locust](https://hub.docker.com/r/locustio/locust) image. The custom image incorporates the [locustfile](locust/locust-image/tasks.py) script and its dependencies.\n",
    "\n",
    "The script simulates calls to the `predict` method of the  AI Platform Prediction REST endpoint. The parameters of the method (project, model, model version, and signature) and test instances passed in the method's body are retrieved from a Cloud Storage location at the start of each test.\n",
    "\n",
    "In addition to simulating requests, the script logs test statistics managed by the Locust master to [Cloud Logging](https://cloud.google.com/logging). \n",
    "The log entries created by the script are used to define a set of [Log-based metrics](https://cloud.google.com/logging/docs/logs-based-metrics) that complement standard [AI Platform Prediction metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-ml). \n",
    "\n",
    "Load tests can be configured, started, and stoped using **Locust's** [web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface). The **Locust's** web interface is enabled on the Locust master and exposed through a Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) configured as an external load balancer.\n",
    "\n",
    "The progress of the tests can be monitored using [Locust's web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface) and/or a Cloud Monitoring [dashboard](https://cloud.google.com/monitoring/dashboards). The advantage of a Cloud Monitoring dashboard is that it can combine AI Platform Prediction metrics with custom Locust log-based metrics. You can find an example dashboard template in the `dashboard_template` folder.\n",
    "\n",
    "After a test completes, the test's metrics are retrieved from Cloud Monitoring and consolidated into a Pandas dataframe to facilitate comprehensive post-mortem analysis.  The `04-analyze-test.ipynb` notebook demonstrates how to use Pandas and Matplotlib to analyze and interpret the test runs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U locust google-cloud-monitoring google-cloud-logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to restart the kernel to use the updated packages!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPICallError \n",
    "\n",
    "from google.cloud import logging_v2\n",
    "from google.cloud.logging_v2 import MetricsServiceV2Client\n",
    "from google.cloud.logging_v2 import LoggingServiceV2Client\n",
    "\n",
    "from google.cloud.monitoring_dashboard.v1.types import Dashboard\n",
    "from google.cloud.monitoring_dashboard.v1 import DashboardsServiceClient\n",
    "from google.cloud.monitoring_v3 import MetricServiceClient\n",
    "from google.cloud.monitoring_v3.query import Query\n",
    "from google.cloud.monitoring_v3.types import TimeInterval\n",
    "\n",
    "from google.protobuf.json_format import ParseDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring and deploying the test environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating log based metrics\n",
    "\n",
    "In this section of the notebook you will use the [Python Cloud Logging client library](https://googleapis.dev/python/logging/latest/v2.html) to create a set of custom log-based metrics. The metrics are based on the log entries generated by the example locustfile script. The script writes the log entries into the *Cloud Logging* log named `locust`.\n",
    "\n",
    "Each log entry includes a set of key value pairs encoded as the JSON payload type. The metrics are based on the subset of keys from the log entry.\n",
    "\n",
    "Key | Value\n",
    "----|------\n",
    "test_id | An ID of a test\n",
    "model | An AI Platform Prediction Model name\n",
    "model_version | An AI Platform Prediction Model version\n",
    "latency | A 95 percentile response time, which is calculated over a 10 sliding second window\n",
    "num_requests | A total number of requests since the test started\n",
    "num_failures | A total number of requests since the test started\n",
    "user_count | A number of simulated users \n",
    "rps | A current requests per second\n",
    "\n",
    "\n",
    "Refer to the [Cloud Logging API reference](https://googleapis.dev/python/logging/latest/v2.html) for more information about the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that creates a custom log metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_locust_metric(\n",
    "    metric_name:str,\n",
    "    log_path:str, \n",
    "    value_field:str,  \n",
    "    bucket_bounds:List[int]):\n",
    "    \n",
    "    metric_path = logging_client.metric_path(project_id, metric_name)\n",
    "    log_entry_filter = 'resource.type=global AND logName={}'.format(log_path)\n",
    "    \n",
    "    metric_descriptor = {\n",
    "        'metric_kind': 'DELTA',\n",
    "        'value_type': 'DISTRIBUTION',\n",
    "        'labels': [\n",
    "            {\n",
    "                'key': 'test_id',\n",
    "                'value_type': 'STRING'\n",
    "            },\n",
    "            {\n",
    "                'key': 'signature',\n",
    "                'value_type': 'STRING'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    bucket_options = {\n",
    "        'explicit_buckets': {\n",
    "            'bounds': bucket_bounds\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    value_extractor = 'EXTRACT(jsonPayload.{})'.format(value_field)\n",
    "    label_extractors = {\n",
    "        'test_id': 'EXTRACT(jsonPayload.test_id)',\n",
    "        'signature': 'EXTRACT(jsonPayload.signature)'\n",
    "    }\n",
    "    \n",
    "    metric = logging_v2.types.LogMetric(\n",
    "        name=metric_name,\n",
    "        filter=log_entry_filter,\n",
    "        value_extractor=value_extractor,\n",
    "        bucket_options=bucket_options,\n",
    "        label_extractors=label_extractors,\n",
    "        metric_descriptor=metric_descriptor,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        logging_client.get_log_metric(metric_path)\n",
    "        print('Metric: {} already exists'.format(metric_path))\n",
    "    except:\n",
    "        logging_client.create_log_metric(parent, metric)\n",
    "        print('Created metric {}'.format(metric_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a logging client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = 'locust'\n",
    "\n",
    "creds , project_id = google.auth.default()\n",
    "logging_client = MetricsServiceV2Client(credentials=creds)\n",
    "\n",
    "parent = logging_client.project_path(project_id)\n",
    "log_path = LoggingServiceV2Client.log_path(project_id, log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a metric to track Locust users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metric projects/mlops-dev-env/metrics/locust_users\n"
     ]
    }
   ],
   "source": [
    "metric_name = 'locust_users'\n",
    "value_field = 'user_count'\n",
    "bucket_bounds = [1, 16, 32, 64, 128]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a metric to track response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metric projects/mlops-dev-env/metrics/locust_latency\n"
     ]
    }
   ],
   "source": [
    "metric_name = 'locust_latency'\n",
    "value_field = 'latency'\n",
    "bucket_bounds = [1, 50, 100, 200, 500]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a metric to track total failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metric projects/mlops-dev-env/metrics/num_failures\n"
     ]
    }
   ],
   "source": [
    "metric_name = 'num_failures'\n",
    "value_field = 'num_failures'\n",
    "bucket_bounds = [1, 1000]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a metric to track total requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metric projects/mlops-dev-env/metrics/num_requests\n"
     ]
    }
   ],
   "source": [
    "metric_name = 'num_requests'\n",
    "value_field = 'num_requests'\n",
    "bucket_bounds = [1, 1000]\n",
    "\n",
    "create_locust_metric(metric_name, log_path, value_field, bucket_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/mlops-dev-env/metricDescriptors/logging.googleapis.com/user/locust_latency\n",
      "projects/mlops-dev-env/metricDescriptors/logging.googleapis.com/user/locust_users\n",
      "projects/mlops-dev-env/metricDescriptors/logging.googleapis.com/user/num_failures\n",
      "projects/mlops-dev-env/metricDescriptors/logging.googleapis.com/user/num_requests\n"
     ]
    }
   ],
   "source": [
    "metrics = logging_client.list_log_metrics(parent)\n",
    "\n",
    "if not list(metrics):\n",
    "    print(\"There are not any log based metrics defined in the the project\")\n",
    "else:\n",
    "    for element in logging_client.list_log_metrics(parent):\n",
    "        print(element.metric_descriptor.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Cloud Monitoring dashboard\n",
    "\n",
    "The`dashboard_template` folder contains an example monitoring dashboard template that combines standard AI Platform Prediction metrics with log-based metrics defined in the previous steps. You can use [Python Client for Cloud Monitoring Dashboards API](https://googleapis.dev/python/monitoring-dashboards/latest/index.html) to create a dashboard based on the template.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dashboard template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_service_client = DashboardsServiceClient(credentials=creds)\n",
    "parent = 'projects/{}'.format(project_id)\n",
    "\n",
    "dashboard_template_file = 'dashboard_template/aipp-monitoring.json'\n",
    "with open(dashboard_template_file) as f:\n",
    "    dashboard_template = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a dashboard protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_proto = Dashboard()\n",
    "dashboard_proto = ParseDict(dashboard_template, dashboard_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dashboard in Cloud Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = dashboard_service_client.create_dashboard(parent, dashboard_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List custom dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard name: test, Dashboard ID: projects/881178567352/dashboards/0d899081-d606-417d-9b0d-772ebc737dd2\n",
      "Dashboard name: AI Platform Prediction and Locust, Dashboard ID: projects/881178567352/dashboards/ab98f253-4067-4e43-b303-0df7a5ed5064\n"
     ]
    }
   ],
   "source": [
    "for dashboard in dashboard_service_client.list_dashboards(parent):\n",
    "    print('Dashboard name: {}, Dashboard ID: {}'.format(dashboard.display_name, dashboard.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Locust to a GKE cluster\n",
    "\n",
    "Before proceeding, you need access to a GKE cluster. The described deployment process can deploy Locust to any GKE cluster as long as there are enough compute resources to support your Locust configuration. The default configuration follows the Locust's best practices and requests one processor core and 4Gi of memory for the Locust master and one processor core and 2Gi of memory for each Locust worker. As you run your tests, it is important to monitor the the master and the workers for resource utilization and fine tune the allocated resources as required.\n",
    "\n",
    "The deployment process has been streamlined using [Kustomize](https://kustomize.io/). As described in the following steps, you can fine tune the baseline configuration by modifying the default `kustomization.yaml` and `patch.yaml` files in the `locust/manifests` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Kustomize\n",
    "\n",
    "The configuration files depend on the latest version of Kustomize. \n",
    "\n",
    "##### Download Kustomize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Version:kustomize/v3.8.1 GitCommit:0b359d0ef0272e6545eda0e99aacd63aef99c4d0 BuildDate:2020-07-16T00:58:46Z GoOs:linux GoArch:amd64}\n",
      "kustomize installed to current directory.\n"
     ]
    }
   ],
   "source": [
    "!curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move Kustomize to a folder on your path\n",
    "The following command moves the Kustomize executable to `/usr/local/bin`. Modify the command if you prefer to move it to some other location on your `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mv kustomize /usr/local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set credentials to access your GKE cluster\n",
    "\n",
    "Use, the `gcloud` command to set credentials to your GKE cluster. Make sure to update the `cluster_name` and `cluster_zone` variables with values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for locust.\n"
     ]
    }
   ],
   "source": [
    "cluster_name = 'locust'\n",
    "cluster_zone = 'us-central1-a'\n",
    "\n",
    "!gcloud container clusters get-credentials {cluster_name} --zone {cluster_zone}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Locust image\n",
    "\n",
    "The first step is to build a docker image that will be used to deploy Locust master and worker pods. The image is derived from the [baseline locust.io image](https://hub.docker.com/r/locustio/locust) and embeds the locustfile and the files's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM locustio/locust\n",
      "WORKDIR /tasks\n",
      "COPY tasks.py .\n",
      "RUN pip install -U google-auth google-cloud-storage google-cloud-logging python-dotenv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tail -n 5 locust/locust-image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 14 file(s) totalling 32.1 KiB before compression.\n",
      "Uploading tarball of [locust/locust-image] to [gs://mlops-dev-env_cloudbuild/source/1597770175.85-244e139dcf97467c9a4ac301c79254ca.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-dev-env/builds/4aa54269-18fd-4af9-99fd-0574cfd18a50].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/4aa54269-18fd-4af9-99fd-0574cfd18a50?project=881178567352].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4aa54269-18fd-4af9-99fd-0574cfd18a50\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-dev-env_cloudbuild/source/1597770175.85-244e139dcf97467c9a4ac301c79254ca.tgz#1597770176395643\n",
      "Copying gs://mlops-dev-env_cloudbuild/source/1597770175.85-244e139dcf97467c9a4ac301c79254ca.tgz#1597770176395643...\n",
      "/ [1 files][  8.1 KiB/  8.1 KiB]                                                \n",
      "Operation completed over 1 objects/8.1 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "Sending build context to Docker daemon  49.15kB\n",
      "Step 1/4 : FROM locustio/locust\n",
      "latest: Pulling from locustio/locust\n",
      "e9afc4f90ab0: Already exists\n",
      "989e6b19a265: Already exists\n",
      "af14b6c2f878: Already exists\n",
      "5573c4b30949: Already exists\n",
      "11a88e764313: Already exists\n",
      "ee776f0e36af: Already exists\n",
      "513c90a1afc3: Pulling fs layer\n",
      "df9b9e95bdb9: Pulling fs layer\n",
      "86c9edb54464: Pulling fs layer\n",
      "f01e1951e973: Pulling fs layer\n",
      "37461fc021ff: Pulling fs layer\n",
      "b34eea2eeb50: Pulling fs layer\n",
      "f01e1951e973: Waiting\n",
      "37461fc021ff: Waiting\n",
      "b34eea2eeb50: Waiting\n",
      "df9b9e95bdb9: Verifying Checksum\n",
      "df9b9e95bdb9: Download complete\n",
      "86c9edb54464: Verifying Checksum\n",
      "86c9edb54464: Download complete\n",
      "513c90a1afc3: Verifying Checksum\n",
      "513c90a1afc3: Download complete\n",
      "f01e1951e973: Verifying Checksum\n",
      "f01e1951e973: Download complete\n",
      "b34eea2eeb50: Verifying Checksum\n",
      "b34eea2eeb50: Download complete\n",
      "37461fc021ff: Verifying Checksum\n",
      "37461fc021ff: Download complete\n",
      "513c90a1afc3: Pull complete\n",
      "df9b9e95bdb9: Pull complete\n",
      "86c9edb54464: Pull complete\n",
      "f01e1951e973: Pull complete\n",
      "37461fc021ff: Pull complete\n",
      "b34eea2eeb50: Pull complete\n",
      "Digest: sha256:f1b8cdd8a8393c25fac305a1bc3cbfe72e5f3fa8de4d0d267acc71bb956468b5\n",
      "Status: Downloaded newer image for locustio/locust:latest\n",
      " ---> 32fcddf9b74c\n",
      "Step 2/4 : WORKDIR /tasks\n",
      " ---> Running in bbb306bc4abb\n",
      "Removing intermediate container bbb306bc4abb\n",
      " ---> 77ee99c659e1\n",
      "Step 3/4 : COPY tasks.py .\n",
      " ---> 2087c1533572\n",
      "Step 4/4 : RUN pip install -U google-auth google-cloud-storage google-cloud-logging python-dotenv\n",
      " ---> Running in 4479a6dc870a\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-auth\n",
      "  Downloading google_auth-1.20.1-py2.py3-none-any.whl (91 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-1.30.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting google-cloud-logging\n",
      "  Downloading google_cloud_logging-1.15.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.14.0-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.8/site-packages (from google-auth) (47.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.8/site-packages (from google-auth) (1.15.0)\n",
      "Collecting google-resumable-media<2.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-0.7.1-py2.py3-none-any.whl (42 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.2.0\n",
      "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-api-core[grpc]<2.0.0dev,>=1.15.0\n",
      "  Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"\n",
      "  Downloading google_crc32c-0.1.0-cp38-cp38-manylinux2010_x86_64.whl (39 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.13.0-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging) (2.24.0)\n",
      "Collecting grpcio<2.0dev,>=1.29.0; extra == \"grpc\"\n",
      "  Downloading grpcio-1.31.0-cp38-cp38-manylinux2014_x86_64.whl (3.4 MB)\n",
      "Collecting cffi>=1.0.0\n",
      "  Downloading cffi-1.14.2-cp38-cp38-manylinux1_x86_64.whl (410 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging) (2.10)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Installing collected packages: pyasn1, pyasn1-modules, rsa, cachetools, google-auth, pycparser, cffi, google-crc32c, google-resumable-media, protobuf, pytz, googleapis-common-protos, grpcio, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-logging, python-dotenv\n",
      "\u001b[91m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/locust/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\u001b[0m\u001b[91m  WARNING: The script dotenv is installed in '/home/locust/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\u001b[0mSuccessfully installed cachetools-4.1.1 cffi-1.14.2 google-api-core-1.22.1 google-auth-1.20.1 google-cloud-core-1.4.1 google-cloud-logging-1.15.1 google-cloud-storage-1.30.0 google-crc32c-0.1.0 google-resumable-media-0.7.1 googleapis-common-protos-1.52.0 grpcio-1.31.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 python-dotenv-0.14.0 pytz-2020.1 rsa-4.6\n",
      "\u001b[91mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 4479a6dc870a\n",
      " ---> b462e703053f\n",
      "Successfully built b462e703053f\n",
      "Successfully tagged gcr.io/mlops-dev-env/locust:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-dev-env/locust\n",
      "The push refers to repository [gcr.io/mlops-dev-env/locust]\n",
      "3b76d6c7c6d0: Preparing\n",
      "779cdc2f08c6: Preparing\n",
      "26896e3a44b4: Preparing\n",
      "7f171c642c11: Preparing\n",
      "e3647d9fc52e: Preparing\n",
      "fc0d2c5619b5: Preparing\n",
      "ccbefb30278f: Preparing\n",
      "7a8a38bf5538: Preparing\n",
      "0d77d4546954: Preparing\n",
      "98d95bdfa037: Preparing\n",
      "da9418a2e1b1: Preparing\n",
      "2e5b4ca91984: Preparing\n",
      "527ade4639e0: Preparing\n",
      "c2c789d2d3c5: Preparing\n",
      "8803ef42039d: Preparing\n",
      "fc0d2c5619b5: Waiting\n",
      "ccbefb30278f: Waiting\n",
      "7a8a38bf5538: Waiting\n",
      "0d77d4546954: Waiting\n",
      "98d95bdfa037: Waiting\n",
      "da9418a2e1b1: Waiting\n",
      "2e5b4ca91984: Waiting\n",
      "527ade4639e0: Waiting\n",
      "c2c789d2d3c5: Waiting\n",
      "8803ef42039d: Waiting\n",
      "e3647d9fc52e: Layer already exists\n",
      "7f171c642c11: Layer already exists\n",
      "fc0d2c5619b5: Layer already exists\n",
      "ccbefb30278f: Layer already exists\n",
      "0d77d4546954: Layer already exists\n",
      "7a8a38bf5538: Layer already exists\n",
      "98d95bdfa037: Layer already exists\n",
      "da9418a2e1b1: Layer already exists\n",
      "2e5b4ca91984: Layer already exists\n",
      "527ade4639e0: Layer already exists\n",
      "c2c789d2d3c5: Layer already exists\n",
      "8803ef42039d: Layer already exists\n",
      "26896e3a44b4: Pushed\n",
      "779cdc2f08c6: Pushed\n",
      "3b76d6c7c6d0: Pushed\n",
      "latest: digest: sha256:3466d5a31f84d65fda9c0fdfe8e0a14c54e50525843a51589ab382792d8c85eb size: 3475\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                 STATUS\n",
      "4aa54269-18fd-4af9-99fd-0574cfd18a50  2020-08-18T17:02:56+00:00  34S       gs://mlops-dev-env_cloudbuild/source/1597770175.85-244e139dcf97467c9a4ac301c79254ca.tgz  gcr.io/mlops-dev-env/locust (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "image_uri = 'gcr.io/{}/locust'.format(project_id)\n",
    "\n",
    "!gcloud builds submit --tag {image_uri} locust/locust-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the manifests\n",
    "\n",
    "Before proceeding with deployment, you need to update the default manifests. The manifests are located in the `locust/manifests` folder. You will modify two files: `kustomization.yaml` and `patch.yaml`.\n",
    "\n",
    "##### Set the name of the custom Locust image\n",
    "\n",
    "You need to update the `kustomization.yaml` file with a reference to the custom image your created in the previous step. \n",
    "\n",
    "Update the `newName` field in the `images` section of the `kustomization.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "images:\n",
      "- name: locustio/locust\n",
      "  newName: gcr.io/mlops-dev-env/locust:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sed -n '22,26p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the number of worker pods \n",
    "\n",
    "The default configuration deploys 32 worker pods. If you want to change it, modify the `count` field in the `replicas` section of the `kustomization.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "replicas:\n",
      "- name: locust-worker\n",
      "  count: 32\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!sed -n '26,30p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the GCS bucket for the test configuration and data files\n",
    "\n",
    "As described in more detail in the later section of the notebook, every time you start a test, the locustfile script attempts to retrieve a test configuration and test data files from a GCS location. You need to configure the name of the GCS bucket hosting the files and the name of the files in `kustomization.yaml`.\n",
    "\n",
    "Modify the `configMapGenerator` section of the file. Specifically, set the `LOCUST_TEST_BUCKET`, `LOCUST_TEST_CONFIG`, and `LOCUST_TEST_DATA` literals to the GCS bucket name, the test config file name, and the test data config file name respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "configMapGenerator:\n",
      "- name: test-config-locations\n",
      "  literals:\n",
      "    - LOCUST_TEST_BUCKET=mlops-dev-workspace\n",
      "    - LOCUST_TEST_CONFIG=test-config/test-config.json\n",
      "    - LOCUST_TEST_DATA=test-config/test-data.json\n",
      "  options:\n",
      "    disableNameSuffixHash: true"
     ]
    }
   ],
   "source": [
    "!sed -n '35,52p' locust/manifests/kustomization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the node pool that hosts the Locust master and workers\n",
    "\n",
    "By default, master and worker pods are deployed to the `default-pool` node pool. If you want to change it (recommended), update the name of the node pool in the `patch.yaml` file. The name of the node pool is a value of the `values` field in the `matchExpressions` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: not-important\n",
      "spec:\n",
      "  template:\n",
      "    spec:\n",
      "      affinity:\n",
      "        nodeAffinity:\n",
      "          requiredDuringSchedulingIgnoredDuringExecution:\n",
      "            nodeSelectorTerms:\n",
      "            - matchExpressions:\n",
      "              - key: cloud.google.com/gke-nodepool\n",
      "                operator: In\n",
      "                values:\n",
      "                - locust     \n"
     ]
    }
   ],
   "source": [
    "!tail -n 17 locust/manifests/patch.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Locust\n",
    "\n",
    "You are now ready to deploy Locust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/test-config-locations created\n",
      "service/locust-master created\n",
      "deployment.apps/locust-master created\n",
      "deployment.apps/locust-worker created\n"
     ]
    }
   ],
   "source": [
    "!kustomize build locust/manifests |kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running load tests\n",
    "\n",
    "Load tests can be configured, started, monitored and stopped using using Locust's [web interface](https://docs.locust.io/en/stable/quickstart.html#locust-s-web-interface). \n",
    "\n",
    "In our deployment, the web interface is exposed by an external load balancer. You can access the interface using the following URL:\n",
    "\n",
    "```\n",
    "http://[EXTERNAL-IP]:8089\n",
    "```\n",
    "\n",
    "where `[EXTERNAL-IP]` can be retrieved by the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)                                        AGE\n",
      "locust-master   LoadBalancer   10.0.6.116   <pending>     8089:32679/TCP,5557:31663/TCP,5558:30154/TCP   6s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service locust-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a Locust test\n",
    "\n",
    "At the start of each test, the locustfile script attempts to retrieve test data and a test configuration from a GCS location. Both the test data and the test configuration are formated as JSON. \n",
    "\n",
    "The test data is an array of JSON objects, where each object includes a list of instances and a model signature. If the array contains more than one object, Locust users will randomly pick a list of instances and an associated signature with each call to the `predict` method of the AI Platform Prediction endpoint.\n",
    "\n",
    "The test configuration is a JSON object with a project id, model name, model version, and a test id.\n",
    "\n",
    "#### Specify the GCS bucket for the test data and test configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config_bucket = 'mlops-dev-workspace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test data\n",
    "\n",
    "In this example we are using the  **ResNet101** model developed in the `01-prepare-for-serving.ipynb` notebook and deployed to AI Platform Prediction in the `02-deploy-to-aipp.ipynb` notebook. We will prepare the instances for the `serving_preprocess` signature of the model using a couple of JPEG images from the `test_images` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'test_images'\n",
    "images = []\n",
    "for image_name in os.listdir(image_folder):\n",
    "    with open(os.path.join(image_folder, image_name), 'rb') as f:\n",
    "        images.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_instance = [{'b64': base64.b64encode(images[0]).decode('utf-8')}]\n",
    "two_instances = [{'b64': base64.b64encode(image).decode('utf-8')} for image in images] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "        {\n",
    "            'signature': 'serving_preprocess',\n",
    "            'instances': single_instance\n",
    "        },\n",
    "        {\n",
    "            'signature': 'serving_preprocess',\n",
    "            'instances': two_instances\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://test-data.json [Content-Type=application/json]...\n",
      "/ [1 files][242.8 KiB/242.8 KiB]                                                \n",
      "Operation completed over 1 objects/242.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "test_data_local_file = 'test-data.json'\n",
    "test_data_gcs_file = 'test-config/test-data.json'\n",
    "\n",
    "with open (test_data_local_file, 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "    \n",
    "!gsutil cp {test_data_local_file} gs://{test_config_bucket}/{test_data_gcs_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test config\n",
    "\n",
    "Make sure to update the below mapping with the values representing your environment. The `test_id` is an arbitrary value that is used to match the custom log-based metrics records with a given test run. Use a different value anytime you start a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    'test_id': 'test-3-2020-08-13',\n",
    "    'project_id': 'mlops-dev-env',\n",
    "    'model': 'ResNet101',\n",
    "    'version': 'batching_150'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://test-config.json [Content-Type=application/json]...\n",
      "/ [1 files][  112.0 B/  112.0 B]                                                \n",
      "Operation completed over 1 objects/112.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "test_config_local_file = 'test-config.json'\n",
    "test_config_gcs_file = 'test-config/test-config.json'\n",
    "\n",
    "with open (test_config_local_file, 'w') as f:\n",
    "    json.dump(test_config, f)\n",
    "\n",
    "!gsutil cp {test_config_local_file} gs://{test_config_bucket}/{test_config_gcs_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double check the test data and config in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"test_id\": \"test-3-2020-08-13\", \"project_id\": \"mlops-dev-env\", \"model\": \"ResNet101\", \"version\": \"batching_150\"}"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://{test_config_bucket}/{test_config_gcs_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"signature\": \"serving_preprocess\", \"instances\": [{\"b64\": \"/9j/4AAQSkZJRgABAQEAYABgAAD//gBGRmlsZSBzb3VyY2U6IGh0dHA6Ly9jb21tb25zLndpa2ltZWRpYS5vcmcvd2l"
     ]
    }
   ],
   "source": [
    "!gsutil cat -r 0-150 gs://{test_config_bucket}/{test_data_gcs_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run and monitor tests\n",
    "\n",
    "You are now ready to run the tests. Use the Locust web UI to start and monitor the tests. To see the consolidated view of AI Platform Prediction performance metrics and Locust client metrics use the Cloud Monitoring dashboard created in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and consolidating test results\n",
    "\n",
    "Locust's web interface along with a Cloud Monitoring dashboard provide a cursory view into performance of a tested AI Platform Prediction model version. A more thorough analysis can be performed by consolidating metrics collected during a test and using data analytics and visualization tools.\n",
    "\n",
    "In this section, you will retrieve the metrics captured in Cloud Monitoring and consolidate them into a single Pandas dataframe. The `04-analyze-test-results.ipynb` notebook demonstrates how to analyze the consolidated results using Pandas and Matplotlib.\n",
    "\n",
    "You will use the Python Cloud Monitoring client library. Refer to the [Cloud Monitoring API reference](https://googleapis.dev/python/monitoring/latest/gapic/v3/api.html) for more information about the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available AI Platform Prediction metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.googleapis.com/prediction/error_count\n",
      "ml.googleapis.com/prediction/latencies\n",
      "ml.googleapis.com/prediction/online/accelerator/duty_cycle\n",
      "ml.googleapis.com/prediction/online/accelerator/memory/bytes_used\n",
      "ml.googleapis.com/prediction/online/cpu/utilization\n",
      "ml.googleapis.com/prediction/online/memory/bytes_used\n",
      "ml.googleapis.com/prediction/online/network/bytes_received\n",
      "ml.googleapis.com/prediction/online/network/bytes_sent\n",
      "ml.googleapis.com/prediction/online/replicas\n",
      "ml.googleapis.com/prediction/online/target_replicas\n",
      "ml.googleapis.com/prediction/prediction_count\n",
      "ml.googleapis.com/prediction/response_count\n"
     ]
    }
   ],
   "source": [
    "creds , project_id = google.auth.default()\n",
    "client = MetricServiceClient(credentials=creds)\n",
    "\n",
    "project_path = client.project_path(project_id)\n",
    "filter = 'metric.type=starts_with(\"ml.googleapis.com/prediction\")'\n",
    "\n",
    "for descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n",
    "    print(descriptor.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List custom log based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging.googleapis.com/user/locust_latency\n",
      "logging.googleapis.com/user/locust_users\n",
      "logging.googleapis.com/user/num_failures\n",
      "logging.googleapis.com/user/num_requests\n"
     ]
    }
   ],
   "source": [
    "filter = 'metric.type=starts_with(\"logging.googleapis.com/user\")'\n",
    "\n",
    "for descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n",
    "    print(descriptor.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve test metrics\n",
    "\n",
    "#### Define a helper function that retrieves test metrics from Cloud Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_metrics(client, project_id, start_time, end_time, model, model_version, test_id, log_name):\n",
    "    \"\"\"\n",
    "    Retrieves test metrics from Cloud Monitoring.\n",
    "    \"\"\"\n",
    "    def _get_aipp_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves a specified AIPP metric.\n",
    "        \"\"\"\n",
    "        query = Query(client, project_id, metric_type=metric_type)\n",
    "        query = query.select_interval(end_time, start_time)\n",
    "        query = query.select_resources(model_id=model)\n",
    "        query = query.select_resources(version_id=model_version)\n",
    "        \n",
    "        if metric_name:\n",
    "            labels = ['metric'] + labels \n",
    "        df = query.as_dataframe(labels=labels)\n",
    "        \n",
    "        if not df.empty:\n",
    "            if metric_name:\n",
    "                df.columns.set_levels([metric_name], level=0, inplace=True)\n",
    "            df = df.set_index(df.index.round('T'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_locust_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves a specified custom log-based metric.\n",
    "        \"\"\"\n",
    "        query = Query(client, project_id, metric_type=metric_type)\n",
    "        query = query.select_interval(end_time, start_time)\n",
    "        query = query.select_metrics(log=log_name)\n",
    "        query = query.select_metrics(test_id=test_id)\n",
    "        \n",
    "        if metric_name:\n",
    "            labels = ['metric'] + labels \n",
    "        df = query.as_dataframe(labels=labels)\n",
    "        \n",
    "        if not df.empty:    \n",
    "            if metric_name:\n",
    "                df.columns.set_levels([metric_name], level=0, inplace=True)\n",
    "            df = df.apply(lambda row: [metric.mean for metric in row])\n",
    "            df = df.set_index(df.index.round('T'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Retrieve GPU duty cycle\n",
    "    metric_type = 'ml.googleapis.com/prediction/online/accelerator/duty_cycle'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'duty_cycle')\n",
    "    df = metric\n",
    "\n",
    "    # Retrieve CPU utilization\n",
    "    metric_type = 'ml.googleapis.com/prediction/online/cpu/utilization'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'cpu_utilization')\n",
    "    if not metric.empty:\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve prediction count\n",
    "    metric_type = 'ml.googleapis.com/prediction/prediction_count'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'prediction_count')\n",
    "    if not metric.empty:\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve responses per second\n",
    "    metric_type = 'ml.googleapis.com/prediction/response_count'\n",
    "    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'response_rate')\n",
    "    if not metric.empty:\n",
    "        metric = (metric/60).round(2)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve backend latencies\n",
    "    metric_type = 'ml.googleapis.com/prediction/latencies'\n",
    "    metric = _get_aipp_metric(metric_type, ['latency_type', 'replica_id', 'signature'])\n",
    "    if not metric.empty:\n",
    "        metric = metric.apply(lambda row: [round(latency.mean/1000,1) for latency in row])\n",
    "        metric.columns.set_names(['metric', 'replica_id', 'signature'], inplace=True)\n",
    "        level_values = ['Latency: ' + value for value in metric.columns.get_level_values(level=0)]\n",
    "        metric.columns.set_levels(level_values, level=0, inplace=True)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust latency\n",
    "    metric_type = 'logging.googleapis.com/user/locust_latency'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Latency: client')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round(2).replace([0], np.nan)\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust user count\n",
    "    metric_type = 'logging.googleapis.com/user/locust_users'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'User count')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust num_failures\n",
    "    metric_type = 'logging.googleapis.com/user/num_failures'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of failures')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "    \n",
    "    # Retrieve Locust num_failures\n",
    "    metric_type = 'logging.googleapis.com/user/num_requests'\n",
    "    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of requests')\n",
    "    if not metric.empty:\n",
    "        metric = metric.round()\n",
    "        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve metrics for a specific test and time period.\n",
    "\n",
    "Update the below variables with the values used to configure the test whose metrics you want to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ResNet1'\n",
    "model_version = 'batching_100'\n",
    "log_name = 'locust'\n",
    "test_id = 'test-2-2020-08-13'\n",
    "test_start_time = datetime.datetime.fromisoformat('2020-08-13T12:50:00-07:00')\n",
    "test_end_time = datetime.datetime.fromisoformat('2020-08-13T14:20:00-07:00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>duty_cycle</th>\n",
       "      <th>cpu_utilization</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>Latency: api server</th>\n",
       "      <th>Latency: model</th>\n",
       "      <th>Latency: network</th>\n",
       "      <th>Latency: overhead</th>\n",
       "      <th>Latency: total</th>\n",
       "      <th>Latency: client</th>\n",
       "      <th>User count</th>\n",
       "      <th>Num of failures</th>\n",
       "      <th>Num of requests</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>replica_id</th>\n",
       "      <th>resnb3624f-batc48cbf2-7dd9b997c4-dzxr7</th>\n",
       "      <th>resnb3624f-batc48cbf2-7dd9b997c4-dzxr7</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-13 19:51:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 19:52:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 19:53:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.099228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 19:54:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.098050</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>32.9</td>\n",
       "      <td>255.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>47.9</td>\n",
       "      <td>303.5</td>\n",
       "      <td>1207.50</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 19:55:00</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.152498</td>\n",
       "      <td>318.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>24.4</td>\n",
       "      <td>119.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>34.9</td>\n",
       "      <td>154.3</td>\n",
       "      <td>241.82</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 21:16:00</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.544746</td>\n",
       "      <td>5490.0</td>\n",
       "      <td>90.80</td>\n",
       "      <td>1.7</td>\n",
       "      <td>149.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>160.9</td>\n",
       "      <td>235.00</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 21:17:00</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.614392</td>\n",
       "      <td>5457.0</td>\n",
       "      <td>91.32</td>\n",
       "      <td>1.7</td>\n",
       "      <td>149.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.9</td>\n",
       "      <td>160.5</td>\n",
       "      <td>237.50</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 21:18:00</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.684038</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>90.40</td>\n",
       "      <td>1.7</td>\n",
       "      <td>148.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.1</td>\n",
       "      <td>160.1</td>\n",
       "      <td>235.00</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264758.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 21:19:00</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.645338</td>\n",
       "      <td>5456.0</td>\n",
       "      <td>90.95</td>\n",
       "      <td>1.8</td>\n",
       "      <td>148.2</td>\n",
       "      <td>10.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>160.2</td>\n",
       "      <td>238.75</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-13 21:20:00</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.606638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.36</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275605.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "metric                                          duty_cycle  \\\n",
       "replica_id          resnb3624f-batc48cbf2-7dd9b997c4-dzxr7   \n",
       "signature                                                    \n",
       "2020-08-13 19:51:00                                   0.00   \n",
       "2020-08-13 19:52:00                                   0.00   \n",
       "2020-08-13 19:53:00                                   0.00   \n",
       "2020-08-13 19:54:00                                   0.00   \n",
       "2020-08-13 19:55:00                                   0.08   \n",
       "...                                                    ...   \n",
       "2020-08-13 21:16:00                                   0.56   \n",
       "2020-08-13 21:17:00                                   0.57   \n",
       "2020-08-13 21:18:00                                   0.57   \n",
       "2020-08-13 21:19:00                                   0.56   \n",
       "2020-08-13 21:20:00                                   0.56   \n",
       "\n",
       "metric                                     cpu_utilization prediction_count  \\\n",
       "replica_id          resnb3624f-batc48cbf2-7dd9b997c4-dzxr7                    \n",
       "signature                                                                     \n",
       "2020-08-13 19:51:00                               0.100362              NaN   \n",
       "2020-08-13 19:52:00                               0.100406              NaN   \n",
       "2020-08-13 19:53:00                               0.099228              0.0   \n",
       "2020-08-13 19:54:00                               0.098050            218.0   \n",
       "2020-08-13 19:55:00                               0.152498            318.0   \n",
       "...                                                    ...              ...   \n",
       "2020-08-13 21:16:00                               0.544746           5490.0   \n",
       "2020-08-13 21:17:00                               0.614392           5457.0   \n",
       "2020-08-13 21:18:00                               0.684038           5424.0   \n",
       "2020-08-13 21:19:00                               0.645338           5456.0   \n",
       "2020-08-13 21:20:00                               0.606638              NaN   \n",
       "\n",
       "metric              response_rate Latency: api server Latency: model  \\\n",
       "replica_id                                                             \n",
       "signature                                                              \n",
       "2020-08-13 19:51:00           NaN                 NaN            NaN   \n",
       "2020-08-13 19:52:00           NaN                 NaN            NaN   \n",
       "2020-08-13 19:53:00           NaN                 0.0            0.0   \n",
       "2020-08-13 19:54:00          1.07                32.9          255.6   \n",
       "2020-08-13 19:55:00          4.93                24.4          119.4   \n",
       "...                           ...                 ...            ...   \n",
       "2020-08-13 21:16:00         90.80                 1.7          149.7   \n",
       "2020-08-13 21:17:00         91.32                 1.7          149.7   \n",
       "2020-08-13 21:18:00         90.40                 1.7          148.9   \n",
       "2020-08-13 21:19:00         90.95                 1.8          148.2   \n",
       "2020-08-13 21:20:00         90.98                 NaN            NaN   \n",
       "\n",
       "metric              Latency: network Latency: overhead Latency: total  \\\n",
       "replica_id                                                              \n",
       "signature                                                               \n",
       "2020-08-13 19:51:00              NaN               NaN            NaN   \n",
       "2020-08-13 19:52:00              NaN               NaN            NaN   \n",
       "2020-08-13 19:53:00              0.0               0.0            0.0   \n",
       "2020-08-13 19:54:00             15.0              47.9          303.5   \n",
       "2020-08-13 19:55:00             10.5              34.9          154.3   \n",
       "...                              ...               ...            ...   \n",
       "2020-08-13 21:16:00              9.4              11.2          160.9   \n",
       "2020-08-13 21:17:00              9.2              10.9          160.5   \n",
       "2020-08-13 21:18:00              9.4              11.1          160.1   \n",
       "2020-08-13 21:19:00             10.3              12.0          160.2   \n",
       "2020-08-13 21:20:00              NaN               NaN            NaN   \n",
       "\n",
       "metric              Latency: client User count Num of failures Num of requests  \n",
       "replica_id                                                                      \n",
       "signature                                                                       \n",
       "2020-08-13 19:51:00             NaN        NaN             NaN             NaN  \n",
       "2020-08-13 19:52:00             NaN        NaN             NaN             NaN  \n",
       "2020-08-13 19:53:00             NaN        NaN             NaN             NaN  \n",
       "2020-08-13 19:54:00         1207.50        8.0             0.0           184.0  \n",
       "2020-08-13 19:55:00          241.82        8.0             0.0           416.0  \n",
       "...                             ...        ...             ...             ...  \n",
       "2020-08-13 21:16:00          235.00      152.0             0.0        253670.0  \n",
       "2020-08-13 21:17:00          237.50      152.0             0.0        259212.0  \n",
       "2020-08-13 21:18:00          235.00      152.0             0.0        264758.0  \n",
       "2020-08-13 21:19:00          238.75      152.0             0.0        270986.0  \n",
       "2020-08-13 21:20:00          236.36      152.0             0.0        275605.0  \n",
       "\n",
       "[90 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = retrieve_metrics(client, project_id, test_start_time, test_end_time, model, model_version, test_id, log_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved dataframe uses [hierarchical indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) for column names. The reason is that some metrics contain multiple time series. For example, the GPU `duty_cycle` metric includes a time series of measures per each GPU used in the deployment (denoted as `replica_id`). The top level of the column index is a metric name. The second level is a `replica_id`. The third level is a `signature` of a model.\n",
    "\n",
    "All metrics are aligned on the same timeline. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize the metrics dataframe\n",
    "\n",
    "The consolidated metrics can be saved for a later analysis by saving the dataframe in the Python `pickle` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'test_results/{}.gzip'.format(test_id)\n",
    "\n",
    "df.to_pickle(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The `04-analyze-tests.ipynb` notebook demonstrates how to use Pandas and Matplotlib to perform a detailed analysis of the load testing runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Locust deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap \"test-config-locations\" deleted\n",
      "service \"locust-master\" deleted\n",
      "deployment.apps \"locust-master\" deleted\n",
      "deployment.apps \"locust-worker\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kustomize build locust/manifests | kubectl delete -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the log based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted metric:  projects/mlops-dev-env/metrics/locust_latency\n",
      "Deleted metric:  projects/mlops-dev-env/metrics/locust_users\n",
      "Deleted metric:  projects/mlops-dev-env/metrics/num_failures\n",
      "Deleted metric:  projects/mlops-dev-env/metrics/num_requests\n"
     ]
    }
   ],
   "source": [
    "creds , project_id = google.auth.default()\n",
    "\n",
    "logging_client = MetricsServiceV2Client(credentials=creds)\n",
    "parent = logging_client.project_path(project_id)\n",
    "\n",
    "for element in logging_client.list_log_metrics(parent):\n",
    "    metric_path = logging_client.metric_path(project_id, element.name)\n",
    "    logging_client.delete_log_metric(metric_path)\n",
    "    print(\"Deleted metric: \", metric_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dasboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dashboard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-86b6a63405e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'projects/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdashboard_service_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_dashboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdashboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dashboard' is not defined"
     ]
    }
   ],
   "source": [
    "dashboard_service_client = DashboardsServiceClient(credentials=creds)\n",
    "parent = 'projects/{}'.format(project_id)\n",
    "\n",
    "dashboard_service_client.delete_dashboard(parent, dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
